# -*- coding: utf-8 -*-

#
# Title: Ultrasoundå½±åƒç»„å­¦æ•°æ®æœºå™¨å­¦ä¹ åˆ†ç±»åˆ†æ
# Author: Li Yang
# Description:
# Refer:
# Date: 2025-05-22
#

"""
Ultrasoundå½±åƒç»„å­¦æ•°æ®æœºå™¨å­¦ä¹ åˆ†ç±»åˆ†æ
åˆ†æå†…å®¹ï¼š
1. ä½¿ç”¨RFEè¿›è¡Œç‰¹å¾é€‰æ‹©
2. 15ç§æœºå™¨å­¦ä¹ åˆ†ç±»å™¨åˆ†æ
3. 5æŠ˜äº¤å‰éªŒè¯è¯„ä¼°
4. ROCæ›²çº¿å’Œæ€§èƒ½æŒ‡æ ‡
5. SHAPå¯è§£é‡Šæ€§åˆ†æ
6. å†³ç­–æ›²çº¿åˆ†æï¼ˆDCAï¼‰
7.rfeè¾¹ç¼˜ç‰¹å¾+rfeè¶…å£°ç‰¹å¾è¿›è¡Œæœºå™¨å­¦ä¹ 
8.rfeçš„ä¸¤ç±»ç‰¹å¾å€¼æ•°å¯»ä¼˜(ä»¥å†³ç­–æ ‘ä¸ºåŸºå‡†çš„AUC/ACCçš„æ ‡å‡†ï¼‰
9.æ–°å¢æ•°æ®å¹³è¡¡çš„æ–¹æ³•
10.åŠ å…¥å››å¤§æŒ‡å—çš„å¯¹æ¯”
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import RFE
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_curve, auc,
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, ConfusionMatrixDisplay
)
from sklearn.multiclass import OneVsRestClassifier

# æœºå™¨å­¦ä¹ æ¨¡å‹
from sklearn.ensemble import (
    RandomForestClassifier, GradientBoostingClassifier,
    AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier
)
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from xgboost import XGBClassifier

import shap
import warnings
import os
from pathlib import Path
import gc
from tqdm import tqdm
from itertools import cycle

# æ·»åŠ sklearnçš„cloneå‡½æ•°å¯¼å…¥
from sklearn.base import clone

from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours
from imblearn.combine import SMOTEENN, SMOTETomek
from collections import Counter

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤ºå’Œè­¦å‘Šè¿‡æ»¤
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore')
sns.set_style("whitegrid")


class BreastCancerClassifier:
    def __init__(self, data_path, balance_strategy='smote'):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.data_path = data_path
        self.output_dir = Path("tk03_result")
        self.output_dir.mkdir(exist_ok=True)

        # æ–°å¢ï¼šæ•°æ®å¹³è¡¡ç­–ç•¥
        self.balance_strategy = balance_strategy

        # ç¦»æ•£å±æ€§åˆ—è¡¨
        self.discrete_cols = [
            'Echo', 'Component', 'Echotexture', 'Margin',
            'Calcification', 'Halo', 'Shadow', 'CDFI', 'A/T ratio', 'Nodule-capsule',
            'Thyroid-tracheal'
        ]

        # è¾¹ç¼˜ç‰¹å¾åç§°åˆ—è¡¨
        self.edge_cols = [
            'Perimeter', 'Area', 'Circularity', 'Compactness', 'Convex_hull_area', 'Solidity',
            'Ellipticity', 'Eccentricity', 'Boundary_irregularity', 'Shape_index', 'Edge_variance',
            'Glcm_contrast', 'Glcm_energy', 'Glcm_entropy', 'Glcm_homogeneity', 'Edge_mean_intensity',
            'Edge_gradient_magnitude', 'Edge_gradient_direction_consistency', 'Edge_sharpness',
            'Edge_smoothness', 'Mean_curvature', 'Curvature_std', 'Curvature_entropy', 'Max_curvature',
            'Curvature_percentage', 'Wavelet_energy', 'Edge_rmsd', 'Gabor_energy_1', 'Gabor_energy_2',
            'Gabor_energy_3', 'Gabor_energy_4', 'Gabor_energy_5', 'Gabor_energy_6', 'Gabor_energy_7',
            'Gabor_energy_8', 'Gabor_std_1', 'Gabor_std_2', 'Gabor_std_3', 'Gabor_std_4', 'Gabor_std_5',
            'Gabor_std_6', 'Gabor_std_7', 'Gabor_std_8', 'Lbp_bin_1', 'Lbp_bin_2', 'Lbp_bin_3', 'Lbp_bin_4',
            'Lbp_bin_5', 'Lbp_bin_6', 'Lbp_bin_7', 'Lbp_bin_8', 'Lbp_bin_9', 'Lbp_bin_10',
            'Glrlm_0_sre', 'Glrlm_0_lre', 'Glrlm_0_glu', 'Glrlm_0_rlu', 'Glrlm_0_rp',
            'Glrlm_45_sre', 'Glrlm_45_lre', 'Glrlm_45_glu', 'Glrlm_45_rlu', 'Glrlm_45_rp',
            'Glrlm_90_sre', 'Glrlm_90_lre', 'Glrlm_90_glu', 'Glrlm_90_rlu', 'Glrlm_90_rp',
            'Glrlm_135_sre', 'Glrlm_135_lre', 'Glrlm_135_glu', 'Glrlm_135_rlu', 'Glrlm_135_rp',
            'Fourier_descriptor_1', 'Fourier_descriptor_2', 'Fourier_descriptor_3', 'Fourier_descriptor_4',
            'Fourier_descriptor_5', 'Fourier_descriptor_6', 'Fourier_descriptor_7', 'Fourier_descriptor_8',
            'Fourier_descriptor_9', 'Fourier_descriptor_10', 'Total_power', 'Peak_power', 'Peak_freq',
            'Median_freq', 'Spectral_entropy', 'Freq_std', 'Fractal_dim', 'Box_dim', 'Higuchi_fd',
            'Multifractal_q_-2', 'Multifractal_q_-1', 'Multifractal_q_0', 'Multifractal_q_1',
            'Multifractal_q_2', 'Lacunarity_scale_2', 'Lacunarity_scale_4', 'Lacunarity_scale_8',
            'Lacunarity_scale_16', 'Lacunarity_scale_32', 'Boundary_background_contrast',
            'Boundary_gradient_ratio', 'Boundary_inside_std_ratio', 'Boundary_intensity_change_rate',
            'Boundary_sharpness', 'Wavelet_energy_ratio', 'Multiscale_entropy', 'Scale_space_curvature',
            'Edge_roughness_spectrum', 'Scale_space_persistence'
        ]

        # æ–°å¢ï¼šå½±åƒå­¦æŒ‡æ ‡è¯Šæ–­é˜ˆå€¼è®¾ç½®ï¼ˆå¯è°ƒèŠ‚ï¼‰
        # self.imaging_thresholds = {
        #     'C-TIRADS': {'benign_max': 1, 'name': 'C-TIRADS'},
        #     'ACR-TIRADS': {'benign_max': 2, 'name': 'ACR-TIRADS'},  # æ–°å¢ACR-TIRADS
        #     'Kwak TI-RADS': {'benign_max': 1, 'name': 'Kwak TI-RADS'},  # æ–°å¢Kwak TI-RADS
        #     'ATA': {'benign_max': 3, 'name': 'ATA'}
        # }
        self.imaging_thresholds = {
            'C-TIRADS': {'benign_max': 1, 'name': 'C-TIRADS'},
            'ACR-TIRADS': {'benign_max': 2, 'name': 'ACR-TIRADS'},  # æ–°å¢ACR-TIRADS
            'Kwak TI-RADS': {'benign_max': 1, 'name': 'Kwak TI-RADS'},  # æ–°å¢Kwak TI-RADS
            'ATA': {'benign_max': 3, 'name': 'ATA'}
        }

        # åŠ è½½æ•°æ®
        self.load_data()

        # åˆå§‹åŒ–æ¨¡å‹å­—å…¸
        self.init_models()

        # åˆå§‹åŒ–ç»“æœå­˜å‚¨
        self.results = {}
        self.cv_results = {}

    def apply_data_balancing(self, X, y, strategy=None):
        """
        åº”ç”¨æ•°æ®å¹³è¡¡ç­–ç•¥

        Parameters:
        -----------
        X : array-like
            ç‰¹å¾æ•°æ®
        y : array-like
            æ ‡ç­¾æ•°æ®
        strategy : str, optional
            å¹³è¡¡ç­–ç•¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨self.balance_strategy

        Returns:
        --------
        X_balanced, y_balanced : å¹³è¡¡åçš„æ•°æ®
        """
        if strategy is None:
            strategy = self.balance_strategy

        if strategy == 'none':
            print("No data balancing applied.")
            return X, y

        print(f"Original class distribution: {Counter(y)}")

        try:
            if strategy == 'smote':
                sampler = SMOTE(random_state=42, k_neighbors=min(5, np.min(np.bincount(y)) - 1))
            elif strategy == 'adasyn':
                sampler = ADASYN(random_state=42, n_neighbors=min(5, np.min(np.bincount(y)) - 1))
            elif strategy == 'borderline':
                sampler = BorderlineSMOTE(random_state=42, k_neighbors=min(5, np.min(np.bincount(y)) - 1))
            elif strategy == 'random_over':
                sampler = RandomOverSampler(random_state=42)
            elif strategy == 'random_under':
                sampler = RandomUnderSampler(random_state=42)
            elif strategy == 'tomek':
                sampler = TomekLinks()
            elif strategy == 'enn':
                sampler = EditedNearestNeighbours()
            elif strategy == 'smote_tomek':
                sampler = SMOTETomek(random_state=42)
            elif strategy == 'smote_enn':
                sampler = SMOTEENN(random_state=42)
            else:
                raise ValueError(f"Unknown balancing strategy: {strategy}")

            X_balanced, y_balanced = sampler.fit_resample(X, y)
            print(f"After {strategy} balancing: {Counter(y_balanced)}")

            # ä¿å­˜å¹³è¡¡ä¿¡æ¯
            self.balancing_info = {
                'strategy': strategy,
                'original_distribution': Counter(y),
                'balanced_distribution': Counter(y_balanced),
                'original_samples': len(y),
                'balanced_samples': len(y_balanced)
            }

            return X_balanced, y_balanced

        except Exception as e:
            print(f"Error applying {strategy} balancing: {e}")
            print("Falling back to random oversampling...")

            # å¤‡ç”¨æ–¹æ¡ˆï¼šéšæœºè¿‡é‡‡æ ·
            sampler = RandomOverSampler(random_state=42)
            X_balanced, y_balanced = sampler.fit_resample(X, y)
            print(f"After random oversampling: {Counter(y_balanced)}")

            self.balancing_info = {
                'strategy': 'random_over (fallback)',
                'original_distribution': Counter(y),
                'balanced_distribution': Counter(y_balanced),
                'original_samples': len(y),
                'balanced_samples': len(y_balanced)
            }

            return X_balanced, y_balanced



    def load_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        try:
            print("Loading data...")
            self.df = pd.read_excel(self.data_path)

            # ä¿å­˜åŸå§‹æ•°æ®è¡Œæ•°
            self.original_row_count = len(self.df)

            # å…ˆåˆ é™¤éç‰¹å¾åˆ—ï¼ˆä¿ç•™å½±åƒå­¦æŒ‡æ ‡ï¼‰
            columns_to_drop = [
                'ç›®å½•å', 'å¹´ä»½æ–‡ä»¶å¤¹', 'å›¾åƒæ–‡ä»¶å', 'æ©ç æ–‡ä»¶å', 'nodule',
                'cell_position', 'cell_classify', 'sex', 'age', 'BRAF', 'KRAS', 'NRAS', 'HRAS', 'TERT'
                # æ³¨æ„ï¼šC-TIRADS, ATA, ACR-TIRADS, Kwak TI-RADS ä¸åˆ é™¤
            ]

            # ä¿å­˜å½±åƒå­¦æŒ‡æ ‡æ•°æ®ï¼ˆä¿å­˜åŸå§‹ç´¢å¼•ï¼‰
            self.imaging_indicators = {}
            self.original_indices = self.df.index.copy()  # ä¿å­˜åŸå§‹ç´¢å¼•

            for indicator in ['C-TIRADS', 'ATA', 'ACR-TIRADS', 'Kwak TI-RADS']:
                if indicator in self.df.columns:
                    self.imaging_indicators[indicator] = self.df[indicator].copy()
                    print(f"Found imaging indicator: {indicator}")
                else:
                    print(f"Warning: {indicator} not found in dataset")

            # ä½¿ç”¨drop()æ–¹æ³•åˆ é™¤åˆ—
            self.df = self.df.drop(columns=columns_to_drop, errors='ignore')

            # æ£€æŸ¥Labelåˆ—
            if 'Label' not in self.df.columns:
                raise ValueError("Label column not found in the dataset")

            # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
            self.y = self.df['Label'].values

            # åˆ†ç¦»è¿ç»­å’Œç¦»æ•£å˜é‡
            self.continuous_cols = [col for col in self.df.columns
                                    if col not in self.discrete_cols + ['Label'] + list(self.imaging_indicators.keys())
                                    and (self.df[col].dtype in ['int64', 'float64'])]

            print(f"Continuous variables: {len(self.continuous_cols)}")
            print(f"Discrete variables: {len(self.discrete_cols)}")
            print(f"Imaging indicators: {list(self.imaging_indicators.keys())}")
            print(f"Class distribution: {np.bincount(self.y)}")

        except Exception as e:
            print(f"Error loading data: {e}")
            raise

    def compress_column_name(self, col_name, max_length=20):
        """å‹ç¼©åˆ—åé•¿åº¦"""
        if len(col_name) <= max_length:
            return col_name

        # ç§»é™¤å¸¸è§å‰ç¼€
        prefixes = ['original_', 'boxmean_', 'additivegaussiannoise_',
                    'binomialblurimage_', 'curvatureflow_', 'boxsigmaimage_',
                    'log_', 'wavelet_', 'normalize_', 'laplaciansharpening_',
                    'discretegaussian_', 'mean_', 'specklenoise_',
                    'recursivegaussian_', 'shotnoise_', 'ROI_DETAILS_']

        compressed = col_name
        for prefix in prefixes:
            if compressed.startswith(prefix):
                compressed = compressed[len(prefix):]
                break

        # è¿›ä¸€æ­¥å‹ç¼©
        if len(compressed) > max_length:
            start_len = max_length // 2 - 2
            end_len = max_length - start_len - 3
            compressed = compressed[:start_len] + "..." + compressed[-end_len:]

        return compressed

    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†ï¼ˆåŒ…å«æ•°æ®å¹³è¡¡ï¼‰"""
        print("Preprocessing data...")

        # å¤„ç†è¿ç»­å˜é‡
        X_continuous = self.df[self.continuous_cols].copy()

        # ç§»é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„åˆ—
        missing_threshold = 0.5
        valid_continuous_cols = []
        for col in self.continuous_cols:
            missing_ratio = X_continuous[col].isnull().sum() / len(X_continuous)
            if missing_ratio < missing_threshold:
                valid_continuous_cols.append(col)

        print(f"Valid continuous columns after missing value filter: {len(valid_continuous_cols)}")
        X_continuous = X_continuous[valid_continuous_cols]

        # å¡«å……ç¼ºå¤±å€¼
        X_continuous = X_continuous.fillna(X_continuous.median())

        # å¤„ç†ç¦»æ•£å˜é‡
        X_discrete = self.df[self.discrete_cols].copy()

        # å¯¹ç¦»æ•£å˜é‡è¿›è¡Œæ ‡ç­¾ç¼–ç 
        le_dict = {}
        for col in self.discrete_cols:
            if col in X_discrete.columns:
                le = LabelEncoder()
                X_discrete[col] = X_discrete[col].fillna('Unknown')
                X_discrete[col] = le.fit_transform(X_discrete[col].astype(str))
                le_dict[col] = le

        # åˆå¹¶ç‰¹å¾
        self.X = pd.concat([X_continuous, X_discrete], axis=1)
        self.feature_names = list(self.X.columns)

        # æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(self.X)

        print(f"Feature matrix shape before balancing: {X_scaled.shape}")
        print(f"Class distribution before balancing: {Counter(self.y)}")

        # è®°å½•æ•°æ®é¢„å¤„ç†å‰åçš„ç´¢å¼•å¯¹åº”å…³ç³»
        self.processed_indices = self.df.index.copy()

        # åº”ç”¨æ•°æ®å¹³è¡¡
        X_balanced, y_balanced = self.apply_data_balancing(X_scaled, self.y)

        # æ›´æ–°æ•°æ®
        self.X_scaled = X_balanced
        self.y = y_balanced

        print(f"Final feature matrix shape: {self.X_scaled.shape}")
        print(f"Final class distribution: {Counter(self.y)}")

        return self.X_scaled, self.y

    def analyze_imaging_indicators_performance(self):
        """åˆ†æä¼ ç»Ÿå½±åƒå­¦æŒ‡æ ‡çš„è¯Šæ–­æ•ˆèƒ½"""
        print("Analyzing traditional imaging indicators performance...")

        if not hasattr(self, 'imaging_indicators') or not self.imaging_indicators:
            print("No imaging indicators available for analysis")
            return

        # ä½¿ç”¨åŸå§‹æ ‡ç­¾æ•°æ®ï¼ˆåœ¨æ•°æ®å¹³è¡¡ä¹‹å‰çš„ï¼‰
        if hasattr(self, 'processed_indices'):
            # è·å–å¤„ç†å‰çš„åŸå§‹æ ‡ç­¾
            original_labels_df = pd.read_excel(self.data_path)
            original_labels = original_labels_df['Label'].values
        else:
            print("Warning: Using balanced labels for imaging analysis")
            original_labels = self.y

        imaging_results = {}

        for indicator, data in self.imaging_indicators.items():
            if indicator not in self.imaging_thresholds:
                continue

            threshold_info = self.imaging_thresholds[indicator]
            benign_max = threshold_info['benign_max']
            indicator_name = threshold_info['name']

            # ç¡®ä¿æ•°æ®é•¿åº¦åŒ¹é…
            if len(data) != len(original_labels):
                print(
                    f"Warning: {indicator} data length ({len(data)}) doesn't match labels length ({len(original_labels)})")
                # å–è¾ƒçŸ­çš„é•¿åº¦
                min_length = min(len(data), len(original_labels))
                data = data.iloc[:min_length] if hasattr(data, 'iloc') else data[:min_length]
                labels_to_use = original_labels[:min_length]
            else:
                labels_to_use = original_labels

            # å¤„ç†ç¼ºå¤±å€¼
            valid_mask = ~pd.isna(data)
            valid_data = data[valid_mask]
            valid_labels = labels_to_use[valid_mask]

            if len(valid_data) == 0:
                print(f"No valid data for {indicator_name}")
                continue

            # æ ¹æ®é˜ˆå€¼è¿›è¡Œé¢„æµ‹
            if indicator == 'ATA':
                # ATA: 1-3ä¸ºè‰¯æ€§(0)ï¼Œå…¶ä»–ä¸ºæ¶æ€§(1)
                predicted_labels = np.where(
                    (valid_data >= 0) & (valid_data <= benign_max), 0, 1
                )
            else:
                # å…¶ä»–æŒ‡æ ‡: 0åˆ°thresholdä¸ºè‰¯æ€§(0)ï¼Œå…¶ä»–ä¸ºæ¶æ€§(1)
                predicted_labels = np.where(valid_data <= benign_max, 0, 1)

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            accuracy = accuracy_score(valid_labels, predicted_labels)
            precision = precision_score(valid_labels, predicted_labels, average='weighted', zero_division=0)
            recall = recall_score(valid_labels, predicted_labels, average='weighted', zero_division=0)
            f1 = f1_score(valid_labels, predicted_labels, average='weighted', zero_division=0)
            specificity = self.calculate_specificity(valid_labels, predicted_labels, 2)

            # è®¡ç®—æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(valid_labels, predicted_labels)

            # è®¡ç®—æ•æ„Ÿæ€§ã€ç‰¹å¼‚æ€§ã€PPVã€NPV
            if cm.shape == (2, 2):
                tn, fp, fn, tp = cm.ravel()
                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
                specificity_cm = tn / (tn + fp) if (tn + fp) > 0 else 0
                ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
                npv = tn / (tn + fn) if (tn + fn) > 0 else 0
            else:
                sensitivity = specificity_cm = ppv = npv = 0

            imaging_results[indicator] = {
                'name': indicator_name,
                'threshold': benign_max,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'specificity': specificity,
                'sensitivity': sensitivity,
                'specificity_cm': specificity_cm,
                'ppv': ppv,
                'npv': npv,
                'confusion_matrix': cm,
                'predicted_labels': predicted_labels,
                'true_labels': valid_labels,
                'valid_count': len(valid_data),
                'original_data': valid_data
            }

            print(
                f"{indicator_name} (threshold â‰¤ {benign_max}): Accuracy = {accuracy:.4f}, F1 = {f1:.4f}, Valid samples = {len(valid_data)}")

        self.imaging_results = imaging_results
        return imaging_results

    # def evaluate_edge_features_only_performance(self):
    #     """è¯„ä¼°ä»…ä½¿ç”¨è¾¹ç¼˜ç‰¹å¾çš„æ¨¡å‹æ€§èƒ½"""
    #     print("Evaluating edge features only performance...")
    #
    #     # ä½¿ç”¨é¢„å¤„ç†åä½†æœªå¹³è¡¡çš„æ•°æ®è¿›è¡Œè¾¹ç¼˜ç‰¹å¾åˆ†æ
    #     # é‡æ–°åŠ è½½å’Œé¢„å¤„ç†æ•°æ®ä»¥è·å¾—æœªå¹³è¡¡çš„ç‰ˆæœ¬
    #     try:
    #         # ä¸´æ—¶ä¿å­˜å½“å‰çš„å¹³è¡¡æ•°æ®
    #         current_X_scaled = self.X_scaled.copy()
    #         current_y = self.y.copy()
    #
    #         # é‡æ–°é¢„å¤„ç†è·å¾—æœªå¹³è¡¡çš„æ•°æ®
    #         temp_df = pd.read_excel(self.data_path)
    #
    #         # åˆ é™¤éç‰¹å¾åˆ—
    #         columns_to_drop = [
    #             'ç›®å½•å', 'å¹´ä»½æ–‡ä»¶å¤¹', 'å›¾åƒæ–‡ä»¶å', 'æ©ç æ–‡ä»¶å', 'nodule',
    #             'cell_position', 'cell_classify', 'sex', 'age', 'BRAF', 'KRAS', 'NRAS', 'HRAS', 'TERT',
    #             'C-TIRADS', 'ATA', 'ACR-TIRADS', 'Kwak TI-RADS'
    #         ]
    #         temp_df = temp_df.drop(columns=columns_to_drop, errors='ignore')
    #
    #         temp_y = temp_df['Label'].values
    #         temp_continuous_cols = [col for col in temp_df.columns
    #                                 if col not in self.discrete_cols + ['Label']
    #                                 and (temp_df[col].dtype in ['int64', 'float64'])]
    #
    #         # å¤„ç†è¿ç»­å˜é‡
    #         X_temp_continuous = temp_df[temp_continuous_cols].copy()
    #         X_temp_continuous = X_temp_continuous.fillna(X_temp_continuous.median())
    #
    #         # å¤„ç†ç¦»æ•£å˜é‡
    #         X_temp_discrete = temp_df[self.discrete_cols].copy()
    #         for col in self.discrete_cols:
    #             if col in X_temp_discrete.columns:
    #                 le = LabelEncoder()
    #                 X_temp_discrete[col] = X_temp_discrete[col].fillna('Unknown')
    #                 X_temp_discrete[col] = le.fit_transform(X_temp_discrete[col].astype(str))
    #
    #         # åˆå¹¶ç‰¹å¾
    #         X_temp = pd.concat([X_temp_continuous, X_temp_discrete], axis=1)
    #         temp_feature_names = list(X_temp.columns)
    #
    #         # æ ‡å‡†åŒ–
    #         temp_scaler = StandardScaler()
    #         X_temp_scaled = temp_scaler.fit_transform(X_temp)
    #
    #         # æå–è¾¹ç¼˜ç‰¹å¾
    #         edge_indices = [i for i, col in enumerate(temp_feature_names) if col in self.edge_cols]
    #         if not edge_indices:
    #             print("No edge features available")
    #             return {}
    #
    #         X_edge_only = X_temp_scaled[:, edge_indices]
    #         edge_feature_names = [temp_feature_names[i] for i in edge_indices]
    #
    #         print(f"Using {len(edge_feature_names)} edge features")
    #
    #         # ç‰¹å¾é€‰æ‹© - ä½¿ç”¨RFEé€‰æ‹©æœ€é‡è¦çš„è¾¹ç¼˜ç‰¹å¾
    #         rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
    #         rfe = RFE(estimator=rf_selector, n_features_to_select=min(20, len(edge_feature_names)))
    #         X_edge_selected = rfe.fit_transform(X_edge_only, temp_y)
    #         selected_edge_features = [edge_feature_names[i] for i in range(len(edge_feature_names)) if rfe.support_[i]]
    #
    #         print(f"Selected {len(selected_edge_features)} edge features using RFE")
    #
    #         # è¯„ä¼°å‰5ä¸ªæœ€ä½³æ¨¡å‹ä»…ä½¿ç”¨è¾¹ç¼˜ç‰¹å¾çš„æ€§èƒ½
    #         top_models = sorted(self.results.items(),
    #                             key=lambda x: x[1]['test_metrics']['Accuracy'],
    #                             reverse=True)[:5]
    #
    #         edge_only_results = {}
    #
    #         for name, _ in top_models:
    #             try:
    #                 model = clone(self.models[name])
    #
    #                 # è®­ç»ƒæµ‹è¯•åˆ†å‰²
    #                 X_train, X_test, y_train, y_test = train_test_split(
    #                     X_edge_selected, temp_y, test_size=0.2, random_state=42, stratify=temp_y
    #                 )
    #
    #                 # è®­ç»ƒæ¨¡å‹
    #                 model.fit(X_train, y_train)
    #
    #                 # é¢„æµ‹
    #                 y_pred = model.predict(X_test)
    #                 y_proba = model.predict_proba(X_test) if hasattr(model, "predict_proba") else None
    #
    #                 # è®¡ç®—æŒ‡æ ‡
    #                 metrics = self.calculate_metrics(y_test, y_pred, y_proba, 'test')
    #
    #                 edge_only_results[name] = {
    #                     'metrics': metrics,
    #                     'selected_features': selected_edge_features,
    #                     'n_features': len(selected_edge_features)
    #                 }
    #
    #                 print(f"  {name} (edge only): Accuracy = {metrics['Accuracy']:.4f}")
    #
    #             except Exception as e:
    #                 print(f"  Error evaluating {name} with edge features only: {e}")
    #                 continue
    #
    #         # æ¢å¤åŸå§‹çš„å¹³è¡¡æ•°æ®
    #         self.X_scaled = current_X_scaled
    #         self.y = current_y
    #
    #         self.edge_only_results = edge_only_results
    #         return edge_only_results
    #
    #     except Exception as e:
    #         print(f"Error in edge features analysis: {e}")
    #         # æ¢å¤åŸå§‹æ•°æ®
    #         if 'current_X_scaled' in locals():
    #             self.X_scaled = current_X_scaled
    #             self.y = current_y
    #         return {}

    def plot_diagnostic_performance_comparison(self):
        """ç»˜åˆ¶è¯Šæ–­æ•ˆèƒ½å¯¹æ¯”å›¾"""
        print("Plotting diagnostic performance comparison...")

        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        axes = axes.ravel()

        # æ”¶é›†æ‰€æœ‰æ–¹æ³•çš„æ€§èƒ½æ•°æ®
        all_methods = {}

        # 1. æ·»åŠ å½±åƒå­¦æŒ‡æ ‡ç»“æœ
        if hasattr(self, 'imaging_results'):
            for indicator, result in self.imaging_results.items():
                all_methods[result['name']] = {
                    'Accuracy': result['accuracy'],
                    'Precision': result['precision'],
                    'Recall': result['recall'],
                    'F1-Score': result['f1_score'],
                    'Specificity': result['specificity'],
                    'Sensitivity': result['sensitivity'],
                    'PPV': result['ppv'],
                    'NPV': result['npv']
                }

        # 2. æ·»åŠ æœ€ä½³æœºå™¨å­¦ä¹ æ¨¡å‹ç»“æœï¼ˆä½¿ç”¨å…·ä½“æ¨¡å‹åç§°ï¼‰
        if hasattr(self, 'results') and self.results:
            best_ml_model = max(self.results.items(), key=lambda x: x[1]['test_metrics']['Accuracy'])
            best_model_name = best_ml_model[0]  # è·å–å…·ä½“æ¨¡å‹åç§°
            ml_metrics = best_ml_model[1]['test_metrics']

            # è®¡ç®—é¢å¤–æŒ‡æ ‡
            y_test = best_ml_model[1]['y_test']
            y_pred = best_ml_model[1]['y_test_pred']
            cm = confusion_matrix(y_test, y_pred)

            if cm.shape == (2, 2):
                tn, fp, fn, tp = cm.ravel()
                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
                ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
                npv = tn / (tn + fn) if (tn + fn) > 0 else 0
            else:
                sensitivity = ppv = npv = 0

            all_methods[best_model_name] = {  # ä½¿ç”¨å…·ä½“æ¨¡å‹åç§°
                'Accuracy': ml_metrics['Accuracy'],
                'Precision': ml_metrics['Precision'],
                'Recall': ml_metrics['Recall'],
                'F1-Score': ml_metrics['F1-Score'],
                'Specificity': ml_metrics['Specificity'],
                'Sensitivity': sensitivity,
                'PPV': ppv,
                'NPV': npv
            }

        # åˆ é™¤è¾¹ç¼˜ç‰¹å¾ç›¸å…³ä»£ç å—

        # ç»˜åˆ¶å„ç§æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        metrics_to_plot = ['Accuracy', 'Sensitivity', 'Specificity', 'PPV', 'NPV', 'F1-Score']
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']

        for idx, metric in enumerate(metrics_to_plot):
            ax = axes[idx]

            methods = list(all_methods.keys())
            values = [all_methods[method].get(metric, 0) for method in methods]

            bars = ax.bar(range(len(methods)), values,
                          color=colors[:len(methods)], alpha=0.8, edgecolor='black')

            ax.set_xlabel('Diagnostic Methods', fontsize=12)
            ax.set_ylabel(metric, fontsize=12)
            ax.set_title(f'{metric} Comparison', fontsize=14, weight='bold')
            ax.set_xticks(range(len(methods)))
            ax.set_xticklabels(methods, rotation=45, ha='right', fontsize=10)
            ax.set_ylim([0, 1])
            ax.grid(True, alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width() / 2., height + 0.02,
                        f'{value:.3f}', ha='center', va='bottom', fontsize=9, weight='bold')

        plt.suptitle('Diagnostic Performance Comparison\n(Traditional Imaging vs Machine Learning)',
                     fontsize=16, weight='bold')
        plt.tight_layout()
        plt.savefig(self.output_dir / "diagnostic_performance_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

        # ç»˜åˆ¶é›·è¾¾å›¾å¯¹æ¯”
        self.plot_radar_comparison(all_methods)

    def plot_radar_comparison(self, all_methods):
        """ç»˜åˆ¶é›·è¾¾å›¾å¯¹æ¯”ä¸åŒè¯Šæ–­æ–¹æ³•"""
        print("Plotting radar comparison...")

        # é€‰æ‹©ä¸»è¦æŒ‡æ ‡
        radar_metrics = ['Accuracy', 'Sensitivity', 'Specificity', 'PPV', 'NPV', 'F1-Score']

        fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))

        # è§’åº¦è®¾ç½®
        angles = np.linspace(0, 2 * np.pi, len(radar_metrics), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆ

        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink']

        for idx, (method, metrics) in enumerate(all_methods.items()):
            values = [metrics.get(metric, 0) for metric in radar_metrics]
            values += values[:1]  # é—­åˆ

            ax.plot(angles, values, 'o-', linewidth=2, label=method,
                    color=colors[idx % len(colors)])
            ax.fill(angles, values, alpha=0.25, color=colors[idx % len(colors)])

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(radar_metrics)
        ax.set_ylim(0, 1)
        ax.set_title('Diagnostic Performance Radar Chart', fontsize=16, weight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)

        plt.tight_layout()
        plt.savefig(self.output_dir / "diagnostic_radar_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_confusion_matrices_comparison(self):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µå¯¹æ¯”"""
        print("Plotting confusion matrices comparison...")

        methods_with_cm = {}

        # æ”¶é›†å½±åƒå­¦æŒ‡æ ‡çš„æ··æ·†çŸ©é˜µ
        if hasattr(self, 'imaging_results'):
            for indicator, result in self.imaging_results.items():
                methods_with_cm[result['name']] = result['confusion_matrix']

        # æ”¶é›†MLæ¨¡å‹çš„æ··æ·†çŸ©é˜µï¼ˆä½¿ç”¨å…·ä½“æ¨¡å‹åç§°ï¼‰
        if hasattr(self, 'results') and self.results:
            best_ml_model = max(self.results.items(), key=lambda x: x[1]['test_metrics']['Accuracy'])
            best_model_name = best_ml_model[0]  # ä½¿ç”¨å…·ä½“æ¨¡å‹åç§°
            methods_with_cm[best_model_name] = best_ml_model[1]['confusion_matrix']

        if not methods_with_cm:
            print("No confusion matrices available for comparison")
            return

        n_methods = len(methods_with_cm)
        n_cols = min(3, n_methods)
        n_rows = (n_methods + n_cols - 1) // n_cols

        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))
        if n_methods == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = axes if n_cols > 1 else [axes]
        else:
            axes = axes.ravel()

        for idx, (method, cm) in enumerate(methods_with_cm.items()):
            ax = axes[idx]

            # è®¡ç®—ç™¾åˆ†æ¯”
            cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

            # ç»˜åˆ¶çƒ­å›¾
            sns.heatmap(cm_percent, annot=False, fmt='.2%', cmap='Blues',
                        xticklabels=['Benign', 'Malignant'],
                        yticklabels=['Benign', 'Malignant'],
                        ax=ax, vmin=0, vmax=1)

            # æ·»åŠ æ•°å€¼å’Œç™¾åˆ†æ¯”æ ‡ç­¾
            for i in range(cm.shape[0]):
                for j in range(cm.shape[1]):
                    count = cm[i, j]
                    percent = cm_percent[i, j] * 100
                    text = f"{count}\n({percent:.1f}%)"
                    ax.text(j + 0.5, i + 0.5, text, ha='center', va='center',
                            color='white' if cm_percent[i, j] > 0.5 else 'black', fontsize=10)

            ax.set_title(f'{method}', fontsize=12, weight='bold')
            ax.set_xlabel('Predicted')
            ax.set_ylabel('Actual')

        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_methods, len(axes)):
            axes[idx].set_visible(False)

        plt.suptitle('Confusion Matrices Comparison', fontsize=16, weight='bold')
        plt.tight_layout()
        plt.savefig(self.output_dir / "confusion_matrices_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

    def save_diagnostic_comparison_results(self):
        """ä¿å­˜è¯Šæ–­å¯¹æ¯”ç»“æœ"""
        print("Saving diagnostic comparison results...")

        # åˆ›å»ºç»¼åˆå¯¹æ¯”è¡¨
        comparison_data = []

        # æ·»åŠ å½±åƒå­¦æŒ‡æ ‡
        if hasattr(self, 'imaging_results'):
            for indicator, result in self.imaging_results.items():
                comparison_data.append({
                    'Method': result['name'],
                    'Type': 'Traditional Imaging',
                    'Threshold': f"â‰¤ {result['threshold']}",
                    'Accuracy': result['accuracy'],
                    'Sensitivity': result['sensitivity'],
                    'Specificity': result['specificity_cm'],
                    'Precision': result['precision'],
                    'Recall': result['recall'],
                    'F1-Score': result['f1_score'],
                    'PPV': result['ppv'],
                    'NPV': result['npv'],
                    'Sample_Size': result['valid_count']
                })

        # æ·»åŠ MLæ¨¡å‹ç»“æœï¼ˆä½¿ç”¨å…·ä½“æ¨¡å‹åç§°ï¼‰
        if hasattr(self, 'results') and self.results:
            best_ml_model = max(self.results.items(), key=lambda x: x[1]['test_metrics']['Accuracy'])
            best_model_name = best_ml_model[0]  # ä½¿ç”¨å…·ä½“æ¨¡å‹åç§°
            ml_metrics = best_ml_model[1]['test_metrics']

            # è®¡ç®—é¢å¤–æŒ‡æ ‡
            y_test = best_ml_model[1]['y_test']
            y_pred = best_ml_model[1]['y_test_pred']
            cm = confusion_matrix(y_test, y_pred)

            if cm.shape == (2, 2):
                tn, fp, fn, tp = cm.ravel()
                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
                specificity_cm = tn / (tn + fp) if (tn + fp) > 0 else 0
                ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
                npv = tn / (tn + fn) if (tn + fn) > 0 else 0
            else:
                sensitivity = specificity_cm = ppv = npv = 0

            comparison_data.append({
                'Method': best_model_name,  # ä½¿ç”¨å…·ä½“æ¨¡å‹åç§°
                'Type': 'Machine Learning',
                'Threshold': f"{len(self.selected_features)} features",
                'Accuracy': ml_metrics['Accuracy'],
                'Sensitivity': sensitivity,
                'Specificity': specificity_cm,
                'Precision': ml_metrics['Precision'],
                'Recall': ml_metrics['Recall'],
                'F1-Score': ml_metrics['F1-Score'],
                'PPV': ppv,
                'NPV': npv,
                'Sample_Size': len(y_test)
            })

        # åˆ é™¤è¾¹ç¼˜ç‰¹å¾ç›¸å…³ä»£ç å—

        # ä¿å­˜åˆ°Excel
        with pd.ExcelWriter(self.output_dir / "comprehensive_diagnostic_comparison.xlsx",
                            engine='openpyxl') as writer:

            # æ€»ä½“å¯¹æ¯”
            comparison_df = pd.DataFrame(comparison_data)
            comparison_df = comparison_df.sort_values('Accuracy', ascending=False)
            comparison_df.to_excel(writer, sheet_name='Overall_Comparison', index=False)

            # å½±åƒå­¦æŒ‡æ ‡è¯¦æƒ…
            if hasattr(self, 'imaging_results'):
                imaging_details = []
                for indicator, result in self.imaging_results.items():
                    imaging_details.append({
                        'Indicator': result['name'],
                        'Threshold_Rule': f"Benign if â‰¤ {result['threshold']}",
                        'Valid_Samples': result['valid_count'],
                        'True_Positives': int(result['confusion_matrix'][1, 1]) if result['confusion_matrix'].shape == (
                        2, 2) else 0,
                        'False_Positives': int(result['confusion_matrix'][0, 1]) if result[
                                                                                        'confusion_matrix'].shape == (
                                                                                    2, 2) else 0,
                        'True_Negatives': int(result['confusion_matrix'][0, 0]) if result['confusion_matrix'].shape == (
                        2, 2) else 0,
                        'False_Negatives': int(result['confusion_matrix'][1, 0]) if result[
                                                                                        'confusion_matrix'].shape == (
                                                                                    2, 2) else 0,
                        'Accuracy': result['accuracy'],
                        'Sensitivity': result['sensitivity'],
                        'Specificity': result['specificity_cm'],
                        'PPV': result['ppv'],
                        'NPV': result['npv']
                    })

                imaging_df = pd.DataFrame(imaging_details)
                imaging_df.to_excel(writer, sheet_name='Imaging_Details', index=False)

            # åˆ é™¤è¾¹ç¼˜ç‰¹å¾è¯¦æƒ…éƒ¨åˆ†

        print("Diagnostic comparison results saved successfully!")

        return comparison_data

    def search_best_rfe_combination_by_accuracy(self, max_edge_features=20, max_us_features=11):
        """
        è‡ªåŠ¨æœç´¢æœ€ä¼˜RFEç»„åˆï¼ˆè¶…å£°+è¾¹ç¼˜ï¼‰ï¼Œä»¥å‡†ç¡®æ€§ä¸ºæ ‡å‡†ï¼Œç»˜åˆ¶å‡†ç¡®æ€§çƒ­å›¾å¹¶ä¿å­˜ç»“æœã€‚
        """
        print("ğŸ” Searching best RFE combination by Accuracy...")

        best_accuracy = 0
        best_edge_num = 0
        best_us_num = 0
        best_features = []
        best_model = None
        accuracy_matrix = np.zeros((max_edge_features, max_us_features))  # è¡Œ: edge, åˆ—: us

        # å‡†ç¡®æ€§è¯„åˆ†æ¨¡å‹å’Œäº¤å‰éªŒè¯å™¨
        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

        all_results = []

        for edge_num in range(1, max_edge_features + 1):
            for us_num in range(1, max_us_features + 1):
                print(f"Evaluating: edge={edge_num}, ultrasound={us_num}")
                try:
                    self.dual_rfe_feature_selection(n_edge_features=edge_num, n_ultrasound_features=us_num)

                    scores = cross_val_score(
                        model, self.X_selected, self.y, scoring='accuracy', cv=cv, n_jobs=-1
                    )
                    mean_accuracy = np.mean(scores)
                    accuracy_matrix[edge_num - 1, us_num - 1] = mean_accuracy

                    all_results.append({
                        'Edge Features': edge_num,
                        'Ultrasound Features': us_num,
                        'Accuracy': mean_accuracy,
                        'Accuracy_Std': np.std(scores)
                    })

                    if mean_accuracy > best_accuracy:
                        best_accuracy = mean_accuracy
                        best_edge_num = edge_num
                        best_us_num = us_num
                        best_features = self.selected_features.copy()
                        best_model = clone(model)

                except Exception as e:
                    print(f"  Skipped due to error: {e}")
                    accuracy_matrix[edge_num - 1, us_num - 1] = np.nan
                    continue

        # è®­ç»ƒæœ€ä¼˜æ¨¡å‹
        print("\nâœ… Best RFE Combination Found (by Accuracy):")
        print(f"  Ultrasound: {best_us_num}, Edge: {best_edge_num}, Accuracy: {best_accuracy:.4f}")

        self.dual_rfe_feature_selection(n_edge_features=best_edge_num, n_ultrasound_features=best_us_num)
        best_model.fit(self.X_selected, self.y)

        # ä¿å­˜
        self.best_accuracy_model = best_model
        self.best_accuracy = best_accuracy
        self.best_feature_combo_accuracy = (best_us_num, best_edge_num)
        self.best_selected_features_accuracy = best_features
        self.accuracy_matrix = accuracy_matrix
        self.accuracy_result_table = pd.DataFrame(all_results)

        # ç»˜å›¾
        self.plot_accuracy_heatmap(accuracy_matrix)

        # å¯¼å‡ºç»“æœ
        self.save_accuracy_combination_results()

        return best_model, best_features, best_accuracy

    def plot_auc_heatmap(self, auc_matrix):
        """ç»˜åˆ¶AUCå€¼çƒ­åŠ›å›¾"""
        print("ğŸ“Š Plotting AUC heatmap...")

        plt.figure(figsize=(10, 8))
        sns.heatmap(auc_matrix, annot=True, fmt=".3f", cmap="YlGnBu",
                    xticklabels=range(1, auc_matrix.shape[1] + 1),
                    yticklabels=range(1, auc_matrix.shape[0] + 1),
                    cbar_kws={'label': 'Mean AUC'})

        plt.xlabel("Ultrasound Feature Count")
        plt.ylabel("Edge Feature Count")
        plt.title("AUC Heatmap (Edge Ã— Ultrasound Feature RFE)")
        plt.tight_layout()
        plt.savefig(self.output_dir / "auc_heatmap.png", dpi=300)
        plt.close()
        print("âœ… AUC heatmap saved.")

    def save_auc_combination_results(self):
        """ä¿å­˜æ¯ä¸ªç‰¹å¾ç»„åˆçš„AUCç»“æœä¸ºExcel"""
        print("ğŸ’¾ Saving AUC combination results to Excel...")

        result_path = self.output_dir / "rfe_auc_combination_results.xlsx"
        best_edge, best_us = self.best_feature_combo

        # åˆ›å»ºDataFrame
        df = self.auc_result_table.copy()
        df = df.sort_values(by="AUC", ascending=False)

        # å†™å…¥Excel
        with pd.ExcelWriter(result_path, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='All Combinations', index=False)
            pd.DataFrame({
                'Best Ultrasound Features': [best_us],
                'Best Edge Features': [best_edge],
                'Best AUC': [self.best_auc]
            }).to_excel(writer, sheet_name='Best Combination', index=False)

        print(f"âœ… AUC results saved to: {result_path}")

    def plot_accuracy_heatmap(self, accuracy_matrix):
        """ç»˜åˆ¶å‡†ç¡®æ€§å€¼çƒ­åŠ›å›¾"""
        print("ğŸ“Š Plotting Accuracy heatmap...")

        plt.figure(figsize=(12, 10))

        # åˆ›å»ºçƒ­åŠ›å›¾
        sns.heatmap(accuracy_matrix, annot=True, fmt=".4f", cmap="YlOrRd",
                    xticklabels=range(1, accuracy_matrix.shape[1] + 1),
                    yticklabels=range(1, accuracy_matrix.shape[0] + 1),
                    cbar_kws={'label': 'Mean Accuracy'},
                    vmin=np.nanmin(accuracy_matrix), vmax=np.nanmax(accuracy_matrix))

        plt.xlabel("Ultrasound Feature Count", fontsize=14)
        plt.ylabel("Edge Feature Count", fontsize=14)
        plt.title("Accuracy Heatmap (Edge Ã— Ultrasound Feature RFE)\nBased on 5-Fold Cross-Validation",
                  fontsize=16, weight='bold')

        # æ ‡è®°æœ€ä½³ç»„åˆ
        best_idx = np.unravel_index(np.nanargmax(accuracy_matrix), accuracy_matrix.shape)
        plt.scatter(best_idx[1] + 0.5, best_idx[0] + 0.5,
                    s=200, c='red', marker='*', edgecolors='white', linewidth=2,
                    label=f'Best: Edge={best_idx[0] + 1}, US={best_idx[1] + 1}')
        plt.legend()

        plt.tight_layout()
        plt.savefig(self.output_dir / "accuracy_heatmap_rfe_optimization.png", dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… Accuracy heatmap saved.")

    def search_best_combination_by_accuracy(self, max_edge_features=20, max_us_features=11):
        """
        è‡ªåŠ¨æœç´¢åœ¨RFEè¾¹ç¼˜ç‰¹å¾(<=20) + è¶…å£°ç‰¹å¾(<=11)ç»„åˆä¸‹Accuracyæœ€ä¼˜çš„ç‰¹å¾ç»„åˆã€‚
        ä½¿ç”¨Extra Treesä½œä¸ºåŸºå‡†è¯„ä¼°å™¨ã€‚
        """
        print("ğŸ” Searching best RFE combination by Accuracy using Extra Trees...")

        best_acc = 0
        best_edge_num = 0
        best_us_num = 0
        best_features = []
        best_model = None
        acc_matrix = np.zeros((max_edge_features, max_us_features))  # è¡Œ: edge, åˆ—: us

        # Using Extra Trees instead of Random Forest
        model = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

        all_results = []

        for edge_num in range(1, max_edge_features + 1):
            for us_num in range(1, max_us_features + 1):
                print(f"Evaluating: edge={edge_num}, ultrasound={us_num}")
                try:
                    self.dual_rfe_feature_selection(n_edge_features=edge_num, n_ultrasound_features=us_num)

                    scores = cross_val_score(
                        model, self.X_selected, self.y, scoring='accuracy', cv=cv, n_jobs=-1
                    )
                    mean_acc = np.mean(scores)
                    acc_matrix[edge_num - 1, us_num - 1] = mean_acc

                    all_results.append({
                        'Edge Features': edge_num,
                        'Ultrasound Features': us_num,
                        'Accuracy': mean_acc
                    })

                    if mean_acc > best_acc:
                        best_acc = mean_acc
                        best_edge_num = edge_num
                        best_us_num = us_num
                        best_features = self.selected_features.copy()
                        best_model = clone(model)

                except Exception as e:
                    print(f"  Skipped due to error: {e}")
                    acc_matrix[edge_num - 1, us_num - 1] = np.nan
                    continue

        # è®­ç»ƒæœ€ä¼˜æ¨¡å‹
        print("\nâœ… Best Accuracy Combination Found:")
        print(f"  Ultrasound: {best_us_num}, Edge: {best_edge_num}, Accuracy: {best_acc:.4f}")

        self.dual_rfe_feature_selection(n_edge_features=best_edge_num, n_ultrasound_features=best_us_num)
        best_model.fit(self.X_selected, self.y)

        self.best_acc_model = best_model
        self.best_acc = best_acc
        self.best_feature_combo_acc = (best_us_num, best_edge_num)
        self.best_selected_features = best_features
        self.acc_matrix = acc_matrix
        self.acc_result_table = pd.DataFrame(all_results)

        self.plot_accuracy_heatmap(acc_matrix)
        self.save_accuracy_combination_results()

        return best_model, best_features, best_acc

    def save_accuracy_combination_results(self):
        """ä¿å­˜å‡†ç¡®æ€§ç»„åˆç»“æœä¸ºExcel"""
        print("ğŸ’¾ Saving Accuracy combination results to Excel...")

        result_path = self.output_dir / "rfe_accuracy_optimization_results.xlsx"
        best_us, best_edge = self.best_feature_combo_accuracy

        # åˆ›å»ºDataFrame
        df = self.accuracy_result_table.copy()
        df = df.sort_values(by="Accuracy", ascending=False)

        # æ·»åŠ æ’ååˆ—
        df.insert(0, 'Rank', range(1, len(df) + 1))

        # å†™å…¥Excel
        with pd.ExcelWriter(result_path, engine='openpyxl') as writer:
            # æ‰€æœ‰ç»„åˆç»“æœ
            df.to_excel(writer, sheet_name='All Combinations', index=False)

            # æœ€ä½³ç»„åˆä¿¡æ¯
            best_info_df = pd.DataFrame({
                'Metric': ['Best Ultrasound Features', 'Best Edge Features', 'Best Accuracy', 'Best Accuracy Std'],
                'Value': [best_us, best_edge, self.best_accuracy,
                          df[df['Accuracy'] == self.best_accuracy]['Accuracy_Std'].iloc[0]]
            })
            best_info_df.to_excel(writer, sheet_name='Best Combination', index=False)

            # å‰10åç»„åˆ
            top10_df = df.head(10).copy()
            top10_df.to_excel(writer, sheet_name='Top 10 Combinations', index=False)

            # ç»Ÿè®¡åˆ†æ
            stats_df = pd.DataFrame({
                'Statistic': ['Mean Accuracy', 'Std Accuracy', 'Min Accuracy', 'Max Accuracy',
                              'Median Accuracy', '95th Percentile', '5th Percentile'],
                'Value': [df['Accuracy'].mean(), df['Accuracy'].std(), df['Accuracy'].min(),
                          df['Accuracy'].max(), df['Accuracy'].median(),
                          df['Accuracy'].quantile(0.95), df['Accuracy'].quantile(0.05)]
            })
            stats_df.to_excel(writer, sheet_name='Statistics', index=False)

        print(f"âœ… Accuracy optimization results saved to: {result_path}")

    def plot_rfe_accuracy_detailed_analysis(self):
        """ç»˜åˆ¶RFEå‡†ç¡®æ€§ä¼˜åŒ–çš„è¯¦ç»†åˆ†æå›¾"""
        print("ğŸ“Š Plotting detailed RFE accuracy analysis...")

        if not hasattr(self, 'accuracy_result_table'):
            print("No accuracy optimization results available")
            return

        df = self.accuracy_result_table.copy()

        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        axes = axes.ravel()

        # 1. ç‰¹å¾æ•°é‡vså‡†ç¡®æ€§æ•£ç‚¹å›¾
        ax1 = axes[0]
        df['Total_Features'] = df['Edge Features'] + df['Ultrasound Features']

        scatter = ax1.scatter(df['Total_Features'], df['Accuracy'],
                              c=df['Edge Features'], cmap='viridis',
                              s=50, alpha=0.7, edgecolors='black')

        # æ ‡è®°æœ€ä½³ç‚¹
        best_row = df.loc[df['Accuracy'].idxmax()]
        ax1.scatter(best_row['Total_Features'], best_row['Accuracy'],
                    s=200, c='red', marker='*', edgecolors='white', linewidth=2,
                    label=f'Best: {best_row["Accuracy"]:.4f}')

        ax1.set_xlabel('Total Features', fontsize=12)
        ax1.set_ylabel('Accuracy', fontsize=12)
        ax1.set_title('Total Features vs Accuracy', fontsize=14, weight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        cbar1 = plt.colorbar(scatter, ax=ax1)
        cbar1.set_label('Edge Features Count')

        # 2. è¾¹ç¼˜ç‰¹å¾æ•°é‡åˆ†å¸ƒ
        ax2 = axes[1]
        edge_counts = df['Edge Features'].value_counts().sort_index()
        bars = ax2.bar(edge_counts.index, edge_counts.values,
                       color='lightblue', alpha=0.7, edgecolor='black')

        ax2.set_xlabel('Edge Features Count', fontsize=12)
        ax2.set_ylabel('Frequency', fontsize=12)
        ax2.set_title('Edge Features Count Distribution', fontsize=14, weight='bold')
        ax2.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width() / 2., height + 0.5,
                     f'{int(height)}', ha='center', va='bottom', fontsize=10)

        # 3. è¶…å£°ç‰¹å¾æ•°é‡åˆ†å¸ƒ
        ax3 = axes[2]
        us_counts = df['Ultrasound Features'].value_counts().sort_index()
        bars = ax3.bar(us_counts.index, us_counts.values,
                       color='lightcoral', alpha=0.7, edgecolor='black')

        ax3.set_xlabel('Ultrasound Features Count', fontsize=12)
        ax3.set_ylabel('Frequency', fontsize=12)
        ax3.set_title('Ultrasound Features Count Distribution', fontsize=14, weight='bold')
        ax3.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width() / 2., height + 0.5,
                     f'{int(height)}', ha='center', va='bottom', fontsize=10)

        # 4. å‡†ç¡®æ€§åˆ†å¸ƒç›´æ–¹å›¾
        ax4 = axes[3]
        n_bins = min(30, len(df) // 3)
        counts, bins, patches = ax4.hist(df['Accuracy'], bins=n_bins,
                                         color='lightgreen', alpha=0.7, edgecolor='black')

        # æ ‡è®°æœ€ä½³å‡†ç¡®æ€§
        ax4.axvline(df['Accuracy'].max(), color='red', linestyle='--', linewidth=2,
                    label=f'Best: {df["Accuracy"].max():.4f}')

        # æ ‡è®°å¹³å‡å‡†ç¡®æ€§
        ax4.axvline(df['Accuracy'].mean(), color='blue', linestyle='--', linewidth=2,
                    label=f'Mean: {df["Accuracy"].mean():.4f}')

        ax4.set_xlabel('Accuracy', fontsize=12)
        ax4.set_ylabel('Frequency', fontsize=12)
        ax4.set_title('Accuracy Distribution', fontsize=14, weight='bold')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        # 5. å‰10åç»„åˆ
        ax5 = axes[4]
        top10 = df.nlargest(10, 'Accuracy').reset_index(drop=True)

        x_pos = range(len(top10))
        bars = ax5.bar(x_pos, top10['Accuracy'],
                       color=plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(top10))),
                       alpha=0.8, edgecolor='black')

        ax5.set_xlabel('Rank', fontsize=12)
        ax5.set_ylabel('Accuracy', fontsize=12)
        ax5.set_title('Top 10 Feature Combinations', fontsize=14, weight='bold')
        ax5.set_xticks(x_pos)
        ax5.set_xticklabels([f'#{i + 1}' for i in x_pos])
        ax5.grid(True, alpha=0.3)

        # æ·»åŠ ç‰¹å¾ç»„åˆæ ‡ç­¾
        for i, (bar, row) in enumerate(zip(bars, top10.itertuples())):
            height = bar.get_height()
            ax5.text(bar.get_x() + bar.get_width() / 2., height + 0.001,
                     f'E:{row._2}\nU:{row._3}', ha='center', va='bottom',
                     fontsize=8, weight='bold')

        # 6. ç‰¹å¾ç»„åˆæ•ˆç‡åˆ†æ
        ax6 = axes[5]

        # è®¡ç®—æ•ˆç‡ï¼šå‡†ç¡®æ€§/æ€»ç‰¹å¾æ•°
        df['Efficiency'] = df['Accuracy'] / df['Total_Features']

        scatter = ax6.scatter(df['Total_Features'], df['Efficiency'],
                              c=df['Accuracy'], cmap='viridis',
                              s=50, alpha=0.7, edgecolors='black')

        # æ ‡è®°æœ€é«˜æ•ˆç‡ç‚¹
        best_eff_row = df.loc[df['Efficiency'].idxmax()]
        ax6.scatter(best_eff_row['Total_Features'], best_eff_row['Efficiency'],
                    s=200, c='red', marker='*', edgecolors='white', linewidth=2,
                    label=f'Most Efficient: {best_eff_row["Efficiency"]:.4f}')

        ax6.set_xlabel('Total Features', fontsize=12)
        ax6.set_ylabel('Efficiency (Accuracy/Features)', fontsize=12)
        ax6.set_title('Feature Combination Efficiency', fontsize=14, weight='bold')
        ax6.legend()
        ax6.grid(True, alpha=0.3)

        cbar2 = plt.colorbar(scatter, ax=ax6)
        cbar2.set_label('Accuracy')

        plt.suptitle('RFE Accuracy Optimization - Detailed Analysis',
                     fontsize=18, weight='bold', y=0.98)
        plt.tight_layout()
        plt.savefig(self.output_dir / "rfe_accuracy_detailed_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

        print("âœ… Detailed RFE accuracy analysis plot saved.")

    def dual_rfe_feature_selection(self, n_edge_features=20, n_ultrasound_features=10):
        """
        åˆ†åˆ«å¯¹è¾¹ç¼˜ç‰¹å¾å’Œè¶…å£°ç‰¹å¾åšRFEï¼Œæœ€ç»ˆå°†äºŒè€…åˆå¹¶ã€‚
        åœ¨å¹³è¡¡æ•°æ®ä¸Šè¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚
        """
        print(
            f"Running RFE on balanced data - edge features ({n_edge_features}) and ultrasound features ({n_ultrasound_features})...")

        # 1. æå–è¶…å£°ç‰¹å¾ï¼ˆåˆ†ç±»å˜é‡ï¼‰
        us_indices = [i for i, col in enumerate(self.feature_names) if col in self.discrete_cols]
        X_us = self.X_scaled[:, us_indices]
        us_names = np.array(self.feature_names)[us_indices]

        # 2. æå–è¾¹ç¼˜ç‰¹å¾ï¼ˆè¿ç»­å˜é‡ä¸­åœ¨ edge_cols ä¸­çš„ï¼‰
        edge_indices = [i for i, col in enumerate(self.feature_names) if col in self.edge_cols]
        X_edge = self.X_scaled[:, edge_indices]
        edge_names = np.array(self.feature_names)[edge_indices]

        # 3. å¯¹è¶…å£°ç‰¹å¾è¿›è¡Œ RFE
        if n_ultrasound_features > 0 and X_us.shape[1] >= n_ultrasound_features:
            base_us = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')
            rfe_us = RFE(estimator=base_us, n_features_to_select=n_ultrasound_features)
            rfe_us.fit(X_us, self.y)
            selected_us_names = us_names[rfe_us.support_].tolist()
        else:
            selected_us_names = us_names.tolist()

        # 4. å¯¹è¾¹ç¼˜ç‰¹å¾è¿›è¡Œ RFE
        if n_edge_features > 0 and X_edge.shape[1] >= n_edge_features:
            base_edge = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')
            rfe_edge = RFE(estimator=base_edge, n_features_to_select=n_edge_features)
            rfe_edge.fit(X_edge, self.y)
            selected_edge_names = edge_names[rfe_edge.support_].tolist()
        else:
            selected_edge_names = edge_names.tolist()

        # 5. åˆå¹¶ç‰¹å¾å
        combined_selected_names = selected_us_names + selected_edge_names
        self.selected_features = combined_selected_names

        # è·å–åˆå¹¶åçš„ç´¢å¼•
        combined_indices = [self.feature_names.index(name) for name in combined_selected_names]
        self.X_selected = self.X_scaled[:, combined_indices]

        print(f"Selected ultrasound features: {selected_us_names}")
        print(f"Selected edge features: {selected_edge_names}")
        print(f"Combined total features: {len(combined_selected_names)}")
        print(f"Data shape for modeling: {self.X_selected.shape}")

        # å¯è§†åŒ–
        self.plot_feature_correlation(combined_selected_names)

        return self.X_selected, combined_selected_names

    def plot_feature_correlation(self, selected_columns):
        """ç»˜åˆ¶é€‰å®šç‰¹å¾çš„ç›¸å…³æ€§çƒ­å›¾"""
        print("Plotting feature correlation heatmap...")

        try:
            # è·å–é€‰å®šç‰¹å¾çš„æ•°æ®
            selected_data = pd.DataFrame(self.X_scaled, columns=self.feature_names)[selected_columns]

            # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            corr_matrix = selected_data.corr()

            # å‹ç¼©ç‰¹å¾åæ˜¾ç¤º
            compressed_names = [self.compress_column_name(col, 15) for col in selected_columns]
            corr_matrix.columns = compressed_names
            corr_matrix.index = compressed_names

            # è®¾ç½®å›¾å½¢å¤§å°
            plt.figure(figsize=(15, 12))

            # ç»˜åˆ¶çƒ­å›¾
            sns.heatmap(corr_matrix,
                        cmap='coolwarm',
                        center=0,
                        annot=True,
                        fmt=".2f",
                        linewidths=0.5,
                        cbar_kws={'label': 'Correlation Coefficient'},
                        annot_kws={'size': 8})

            # è°ƒæ•´æ ‡é¢˜å’Œæ ‡ç­¾
            plt.title('Selected Features Correlation Matrix', fontsize=14, pad=20)
            plt.xticks(rotation=45, ha='right', fontsize=8)
            plt.yticks(rotation=0, fontsize=8)

            # ä¿å­˜å›¾ç‰‡
            plt.tight_layout()
            plt.savefig(self.output_dir / "selected_features_correlation.png",
                        dpi=300, bbox_inches='tight')
            plt.close()

            print("Feature correlation heatmap saved successfully.")

        except Exception as e:
            print(f"Error plotting feature correlation: {e}")

    def init_models(self):
        """åˆå§‹åŒ–15ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆé’ˆå¯¹ä¸å¹³è¡¡æ•°æ®ä¼˜åŒ–ï¼‰"""
        self.models = {
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1,
                                                    class_weight='balanced'),
            'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
            'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='mlogloss', scale_pos_weight=2.16),
            'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced'),
            'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42, class_weight='balanced'),
            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),
            'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
            'Naive Bayes': GaussianNB(),
            'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),
            'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),
            'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'),
            'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=1000),
            'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),
            'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis(),
            'Bagging': BaggingClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        }

        print(f"Initialized {len(self.models)} machine learning models with class balancing")



    def evaluate_models(self):
        """è¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        print("Evaluating all models with 5-fold cross validation...")

        # 5æŠ˜äº¤å‰éªŒè¯
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

        # è·å–ç±»åˆ«æ ‡ç­¾
        class_labels = np.unique(self.y)
        class_names = [f'Class {label}' for label in class_labels]

        for name, model in tqdm(self.models.items(), desc="Training models"):
            print(f"\nTraining {name}...")

            try:
                # äº¤å‰éªŒè¯è¯„åˆ†
                cv_scores = cross_val_score(model, self.X_selected, self.y,
                                            cv=cv, scoring='accuracy', n_jobs=-1)

                # è®­ç»ƒæµ‹è¯•åˆ†å‰²
                X_train, X_test, y_train, y_test = train_test_split(
                    self.X_selected, self.y, test_size=0.2,
                    random_state=42, stratify=self.y
                )

                # è®­ç»ƒæ¨¡å‹
                model.fit(X_train, y_train)

                # é¢„æµ‹
                y_train_pred = model.predict(X_train)
                y_test_pred = model.predict(X_test)
                y_test_proba = model.predict_proba(X_test) if hasattr(model, "predict_proba") else None

                # è®¡ç®—æŒ‡æ ‡
                train_metrics = self.calculate_metrics(y_train, y_train_pred, y_test_proba, 'train')
                test_metrics = self.calculate_metrics(y_test, y_test_pred, y_test_proba, 'test')

                # è®¡ç®—æ··æ·†çŸ©é˜µ
                cm = confusion_matrix(y_test, y_test_pred, labels=class_labels)

                # å­˜å‚¨ç»“æœ
                self.results[name] = {
                    'model': model,
                    'train_metrics': train_metrics,
                    'test_metrics': test_metrics,
                    'y_test': y_test,
                    'y_test_pred': y_test_pred,
                    'y_test_proba': y_test_proba,
                    'cv_scores': cv_scores,
                    'confusion_matrix': cm
                }

                self.cv_results[name] = {
                    'CV_Mean': cv_scores.mean(),
                    'CV_Std': cv_scores.std()
                }

                print(f"  CV Accuracy: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
                print(f"  Test Accuracy: {test_metrics['Accuracy']:.4f}")

                # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
                self.plot_confusion_matrix(name, cm, class_names)

            except Exception as e:
                print(f"  Error training {name}: {e}")
                continue

    def create_ensemble_voting_classifier(self):
        """åˆ›å»ºé›†æˆæŠ•ç¥¨åˆ†ç±»å™¨"""
        print("Creating ensemble voting classifier...")

        # é‡æ–°è®­ç»ƒæ‰€æœ‰æ¨¡å‹ä»¥ç¡®ä¿ä¸€è‡´æ€§
        X_train, X_test, y_train, y_test = train_test_split(
            self.X_selected, self.y, test_size=0.2, random_state=42, stratify=self.y
        )

        # å­˜å‚¨è®­ç»ƒå¥½çš„æ¨¡å‹
        self.trained_models = {}
        self.ensemble_predictions = {}

        print("Training individual models for ensemble...")

        for name, model in tqdm(self.models.items(), desc="Training ensemble models"):
            try:
                # è®­ç»ƒæ¨¡å‹
                model_copy = clone(model)
                model_copy.fit(X_train, y_train)

                # é¢„æµ‹
                y_pred = model_copy.predict(X_test)
                y_proba = model_copy.predict_proba(X_test) if hasattr(model_copy, "predict_proba") else None

                # å­˜å‚¨ç»“æœ
                self.trained_models[name] = model_copy
                self.ensemble_predictions[name] = {
                    'y_pred': y_pred,
                    'y_proba': y_proba,
                    'accuracy': accuracy_score(y_test, y_pred)
                }

                print(f"  {name}: Accuracy = {accuracy_score(y_test, y_pred):.4f}")

            except Exception as e:
                print(f"  Error training {name}: {e}")
                continue

        # å­˜å‚¨æµ‹è¯•é›†æ•°æ®
        self.X_test_ensemble = X_test
        self.y_test_ensemble = y_test

        return len(self.trained_models)

    def perform_voting_classification(self):
        """æ‰§è¡ŒæŠ•ç¥¨åˆ†ç±»"""
        print("Performing voting classification...")

        if not hasattr(self, 'trained_models') or not self.trained_models:
            print("No trained models available. Creating ensemble first...")
            self.create_ensemble_voting_classifier()

        n_samples = len(self.X_test_ensemble)
        n_models = len(self.trained_models)
        n_classes = len(np.unique(self.y))

        # åˆå§‹åŒ–æŠ•ç¥¨çŸ©é˜µ
        self.voting_matrix = np.zeros((n_samples, n_models), dtype=int)
        self.voting_proba_matrix = np.zeros((n_samples, n_models, n_classes))
        self.model_names_list = list(self.trained_models.keys())

        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        for model_idx, (name, model) in enumerate(self.trained_models.items()):
            predictions = self.ensemble_predictions[name]
            self.voting_matrix[:, model_idx] = predictions['y_pred']

            if predictions['y_proba'] is not None:
                self.voting_proba_matrix[:, model_idx, :] = predictions['y_proba']

        # æ‰§è¡Œä¸åŒçš„æŠ•ç¥¨ç­–ç•¥
        self.voting_results = self._apply_voting_strategies()

        return self.voting_results

    def _apply_voting_strategies(self):
        """åº”ç”¨ä¸åŒçš„æŠ•ç¥¨ç­–ç•¥"""
        voting_results = {}

        # 1. ç¡¬æŠ•ç¥¨ (Hard Voting)
        hard_votes = []
        for i in range(len(self.X_test_ensemble)):
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„ç¥¨æ•°
            votes = np.bincount(self.voting_matrix[i, :], minlength=len(np.unique(self.y)))
            predicted_class = np.argmax(votes)
            hard_votes.append(predicted_class)

        hard_votes = np.array(hard_votes)

        # 2. è½¯æŠ•ç¥¨ (Soft Voting) - åŸºäºæ¦‚ç‡å¹³å‡
        soft_votes = []
        avg_probabilities = np.mean(self.voting_proba_matrix, axis=1)
        soft_votes = np.argmax(avg_probabilities, axis=1)

        # 3. åŠ æƒæŠ•ç¥¨ - åŸºäºæ¨¡å‹å‡†ç¡®ç‡
        model_weights = np.array([self.ensemble_predictions[name]['accuracy']
                                  for name in self.model_names_list])
        model_weights = model_weights / np.sum(model_weights)  # å½’ä¸€åŒ–æƒé‡

        weighted_probabilities = np.zeros((len(self.X_test_ensemble), len(np.unique(self.y))))
        for i, weight in enumerate(model_weights):
            weighted_probabilities += weight * self.voting_proba_matrix[:, i, :]

        weighted_votes = np.argmax(weighted_probabilities, axis=1)

        # 4. å¤šæ•°æŠ•ç¥¨ (Majority Voting) - éœ€è¦è¶…è¿‡åŠæ•°
        majority_votes = []
        for i in range(len(self.X_test_ensemble)):
            votes = np.bincount(self.voting_matrix[i, :], minlength=len(np.unique(self.y)))
            max_votes = np.max(votes)
            if max_votes > len(self.model_names_list) / 2:
                predicted_class = np.argmax(votes)
            else:
                predicted_class = -1  # è¡¨ç¤ºæ²¡æœ‰å¤šæ•°
            majority_votes.append(predicted_class)

        majority_votes = np.array(majority_votes)

        # è®¡ç®—å„ç§æŠ•ç¥¨ç­–ç•¥çš„æ€§èƒ½
        voting_results = {
            'hard_voting': {
                'predictions': hard_votes,
                'accuracy': accuracy_score(self.y_test_ensemble, hard_votes),
                'probabilities': None
            },
            'soft_voting': {
                'predictions': soft_votes,
                'accuracy': accuracy_score(self.y_test_ensemble, soft_votes),
                'probabilities': avg_probabilities
            },
            'weighted_voting': {
                'predictions': weighted_votes,
                'accuracy': accuracy_score(self.y_test_ensemble, weighted_votes),
                'probabilities': weighted_probabilities,
                'weights': model_weights
            },
            'majority_voting': {
                'predictions': majority_votes,
                'accuracy': accuracy_score(self.y_test_ensemble[majority_votes != -1],
                                           majority_votes[majority_votes != -1]) if np.any(majority_votes != -1) else 0,
                'probabilities': None,
                'abstention_rate': np.mean(majority_votes == -1)
            }
        }

        return voting_results

    def plot_voting_analysis(self):
        """ç»˜åˆ¶æŠ•ç¥¨åˆ†æçš„ç»¼åˆå›¾è¡¨"""
        print("Plotting voting analysis...")

        if not hasattr(self, 'voting_results'):
            print("No voting results available. Performing voting classification first...")
            self.perform_voting_classification()

        # åˆ›å»ºå¤šä¸ªå¯è§†åŒ–å›¾è¡¨
        self.plot_voting_heatmap()
        self.plot_voting_distribution()
        self.plot_model_agreement_analysis()
        self.plot_voting_performance_comparison()
        self.plot_voting_confusion_matrices()  # æ–°å¢çš„æ··æ·†çŸ©é˜µç»˜åˆ¶
        self.plot_individual_case_analysis()
        self.plot_consensus_confidence_analysis()

    def plot_voting_heatmap(self):
        """ç»˜åˆ¶æŠ•ç¥¨çƒ­å›¾"""
        print("Plotting voting heatmap...")

        fig, axes = plt.subplots(2, 2, figsize=(20, 16))

        # 1. æ¨¡å‹é¢„æµ‹çƒ­å›¾
        ax1 = axes[0, 0]
        im1 = ax1.imshow(self.voting_matrix.T, cmap='viridis', aspect='auto')
        ax1.set_title('Model Predictions Heatmap\n(Rows: Models, Columns: Test Samples)',
                      fontsize=14, weight='bold')
        ax1.set_xlabel('Test Sample Index')
        ax1.set_ylabel('Models')

        # è®¾ç½®yè½´æ ‡ç­¾
        compressed_names = [self.compress_column_name(name, 20) for name in self.model_names_list]
        ax1.set_yticks(range(len(self.model_names_list)))
        ax1.set_yticklabels(compressed_names, fontsize=8)

        # æ·»åŠ é¢œè‰²æ¡
        cbar1 = plt.colorbar(im1, ax=ax1)
        cbar1.set_label('Predicted Class', fontsize=10)

        # 2. æŠ•ç¥¨ä¸€è‡´æ€§çƒ­å›¾
        ax2 = axes[0, 1]

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŠ•ç¥¨ä¸€è‡´æ€§
        consistency_matrix = np.zeros((len(self.X_test_ensemble), len(np.unique(self.y))))
        for i in range(len(self.X_test_ensemble)):
            votes = np.bincount(self.voting_matrix[i, :], minlength=len(np.unique(self.y)))
            consistency_matrix[i, :] = votes / len(self.model_names_list)

        im2 = ax2.imshow(consistency_matrix.T, cmap='RdYlBu_r', aspect='auto', vmin=0, vmax=1)
        ax2.set_title('Voting Consistency Heatmap\n(Proportion of votes for each class)',
                      fontsize=14, weight='bold')
        ax2.set_xlabel('Test Sample Index')
        ax2.set_ylabel('Class')
        ax2.set_yticks(range(len(np.unique(self.y))))
        ax2.set_yticklabels([f'Class {i}' for i in range(len(np.unique(self.y)))])

        cbar2 = plt.colorbar(im2, ax=ax2)
        cbar2.set_label('Vote Proportion', fontsize=10)

        # 3. æ¨¡å‹é—´ç›¸å…³æ€§çƒ­å›¾
        ax3 = axes[1, 0]

        # è®¡ç®—æ¨¡å‹é¢„æµ‹çš„ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = np.corrcoef(self.voting_matrix.T)

        im3 = ax3.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)
        ax3.set_title('Model Prediction Correlation Matrix', fontsize=14, weight='bold')
        ax3.set_xlabel('Models')
        ax3.set_ylabel('Models')
        ax3.set_xticks(range(len(self.model_names_list)))
        ax3.set_yticks(range(len(self.model_names_list)))
        ax3.set_xticklabels(compressed_names, rotation=45, ha='right', fontsize=8)
        ax3.set_yticklabels(compressed_names, fontsize=8)

        # æ·»åŠ ç›¸å…³ç³»æ•°æ–‡æœ¬
        for i in range(len(self.model_names_list)):
            for j in range(len(self.model_names_list)):
                text = ax3.text(j, i, f'{correlation_matrix[i, j]:.2f}',
                                ha="center", va="center",
                                color="black" if abs(correlation_matrix[i, j]) < 0.5 else "white",
                                fontsize=6)

        cbar3 = plt.colorbar(im3, ax=ax3)
        cbar3.set_label('Correlation Coefficient', fontsize=10)

        # 4. çœŸå®æ ‡ç­¾ vs æŠ•ç¥¨ç»“æœæ¯”è¾ƒ
        ax4 = axes[1, 1]

        # åˆ›å»ºæ¯”è¾ƒçŸ©é˜µ
        comparison_data = np.column_stack([
            self.y_test_ensemble,
            self.voting_results['hard_voting']['predictions'],
            self.voting_results['soft_voting']['predictions'],
            self.voting_results['weighted_voting']['predictions']
        ])

        im4 = ax4.imshow(comparison_data.T, cmap='tab10', aspect='auto')
        ax4.set_title('True Labels vs Voting Results Comparison', fontsize=14, weight='bold')
        ax4.set_xlabel('Test Sample Index')
        ax4.set_ylabel('Prediction Source')
        ax4.set_yticks(range(4))
        ax4.set_yticklabels(['True Label', 'Hard Voting', 'Soft Voting', 'Weighted Voting'])

        cbar4 = plt.colorbar(im4, ax=ax4)
        cbar4.set_label('Class Label', fontsize=10)

        plt.tight_layout()
        plt.savefig(self.output_dir / "voting_heatmap_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_voting_distribution(self):
        """ç»˜åˆ¶æŠ•ç¥¨åˆ†å¸ƒåˆ†æ"""
        print("Plotting voting distribution analysis...")

        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        axes = axes.ravel()

        # 1. æŠ•ç¥¨ç­–ç•¥å‡†ç¡®ç‡æ¯”è¾ƒ
        ax1 = axes[0]
        strategies = ['Hard Voting', 'Soft Voting', 'Weighted Voting', 'Individual Models (Mean)']
        accuracies = [
            self.voting_results['hard_voting']['accuracy'],
            self.voting_results['soft_voting']['accuracy'],
            self.voting_results['weighted_voting']['accuracy'],
            np.mean([pred['accuracy'] for pred in self.ensemble_predictions.values()])
        ]

        bars = ax1.bar(strategies, accuracies, color=['skyblue', 'lightcoral', 'lightgreen', 'orange'])
        ax1.set_ylabel('Accuracy', fontsize=12)
        ax1.set_title('Voting Strategy Performance Comparison', fontsize=14, weight='bold')
        ax1.set_ylim([0, 1])

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars, accuracies):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                     f'{acc:.3f}', ha='center', va='bottom', fontsize=10, weight='bold')

        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)

        # 2. å•ä¸ªæ¨¡å‹å‡†ç¡®ç‡åˆ†å¸ƒ
        ax2 = axes[1]
        model_accuracies = [pred['accuracy'] for pred in self.ensemble_predictions.values()]
        ax2.hist(model_accuracies, bins=15, alpha=0.7, color='lightblue', edgecolor='black')
        ax2.axvline(np.mean(model_accuracies), color='red', linestyle='--', linewidth=2,
                    label=f'Mean: {np.mean(model_accuracies):.3f}')
        ax2.axvline(self.voting_results['hard_voting']['accuracy'], color='green', linestyle='--', linewidth=2,
                    label=f'Hard Voting: {self.voting_results["hard_voting"]["accuracy"]:.3f}')
        ax2.set_xlabel('Accuracy', fontsize=12)
        ax2.set_ylabel('Number of Models', fontsize=12)
        ax2.set_title('Individual Model Accuracy Distribution', fontsize=14, weight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. æŠ•ç¥¨ä¸€è‡´æ€§åˆ†å¸ƒ
        ax3 = axes[2]

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§æŠ•ç¥¨æ¯”ä¾‹
        max_vote_proportions = []
        for i in range(len(self.X_test_ensemble)):
            votes = np.bincount(self.voting_matrix[i, :], minlength=len(np.unique(self.y)))
            max_proportion = np.max(votes) / len(self.model_names_list)
            max_vote_proportions.append(max_proportion)

        ax3.hist(max_vote_proportions, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
        ax3.axvline(np.mean(max_vote_proportions), color='red', linestyle='--', linewidth=2,
                    label=f'Mean: {np.mean(max_vote_proportions):.3f}')
        ax3.set_xlabel('Maximum Vote Proportion', fontsize=12)
        ax3.set_ylabel('Number of Samples', fontsize=12)
        ax3.set_title('Voting Consensus Distribution', fontsize=14, weight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. æ¨¡å‹æƒé‡å¯è§†åŒ–ï¼ˆåŠ æƒæŠ•ç¥¨ï¼‰
        ax4 = axes[3]
        weights = self.voting_results['weighted_voting']['weights']
        sorted_indices = np.argsort(weights)[::-1]
        sorted_names = [self.compress_column_name(self.model_names_list[i], 15) for i in sorted_indices]
        sorted_weights = weights[sorted_indices]

        bars = ax4.barh(range(len(sorted_names)), sorted_weights, color='lightcoral')
        ax4.set_yticks(range(len(sorted_names)))
        ax4.set_yticklabels(sorted_names, fontsize=8)
        ax4.set_xlabel('Weight (Normalized Accuracy)', fontsize=12)
        ax4.set_title('Model Weights in Weighted Voting', fontsize=14, weight='bold')
        ax4.grid(True, alpha=0.3)

        # æ·»åŠ æƒé‡å€¼æ ‡ç­¾
        for i, (bar, weight) in enumerate(zip(bars, sorted_weights)):
            width = bar.get_width()
            ax4.text(width + 0.001, bar.get_y() + bar.get_height() / 2,
                     f'{weight:.3f}', ha='left', va='center', fontsize=8)

        # 5. é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ
        ax5 = axes[4]

        if self.voting_results['soft_voting']['probabilities'] is not None:
            max_probabilities = np.max(self.voting_results['soft_voting']['probabilities'], axis=1)
            ax5.hist(max_probabilities, bins=20, alpha=0.7, color='lightyellow', edgecolor='black')
            ax5.axvline(np.mean(max_probabilities), color='red', linestyle='--', linewidth=2,
                        label=f'Mean: {np.mean(max_probabilities):.3f}')
            ax5.set_xlabel('Maximum Prediction Probability', fontsize=12)
            ax5.set_ylabel('Number of Samples', fontsize=12)
            ax5.set_title('Soft Voting Confidence Distribution', fontsize=14, weight='bold')
            ax5.legend()
            ax5.grid(True, alpha=0.3)

        # 6. é”™è¯¯åˆ†æ
        ax6 = axes[5]

        # ç»Ÿè®¡æ¯ç§æŠ•ç¥¨æ–¹æ³•çš„é”™è¯¯ç±»å‹
        voting_methods = ['hard_voting', 'soft_voting', 'weighted_voting']
        error_counts = []

        for method in voting_methods:
            predictions = self.voting_results[method]['predictions']
            errors = np.sum(predictions != self.y_test_ensemble)
            error_counts.append(errors)

        method_names = ['Hard Voting', 'Soft Voting', 'Weighted Voting']
        bars = ax6.bar(method_names, error_counts, color=['skyblue', 'lightcoral', 'lightgreen'])
        ax6.set_ylabel('Number of Errors', fontsize=12)
        ax6.set_title('Error Count Comparison', fontsize=14, weight='bold')

        # æ·»åŠ é”™è¯¯ç‡æ ‡ç­¾
        for bar, errors in zip(bars, error_counts):
            height = bar.get_height()
            error_rate = errors / len(self.y_test_ensemble)
            ax6.text(bar.get_x() + bar.get_width() / 2., height + 0.5,
                     f'{errors}\n({error_rate:.2%})', ha='center', va='bottom', fontsize=10)

        ax6.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.output_dir / "voting_distribution_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_model_agreement_analysis(self):
        """ç»˜åˆ¶æ¨¡å‹ä¸€è‡´æ€§åˆ†æ"""
        print("Plotting model agreement analysis...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # 1. æ¨¡å‹ä¸€è‡´æ€§ç½‘ç»œå›¾
        ax1 = axes[0, 0]

        # è®¡ç®—æ¨¡å‹é—´çš„ä¸€è‡´æ€§ï¼ˆé¢„æµ‹ç›¸åŒçš„æ¯”ä¾‹ï¼‰
        n_models = len(self.model_names_list)
        agreement_matrix = np.zeros((n_models, n_models))

        for i in range(n_models):
            for j in range(n_models):
                agreement = np.mean(self.voting_matrix[:, i] == self.voting_matrix[:, j])
                agreement_matrix[i, j] = agreement

        # ç»˜åˆ¶ä¸€è‡´æ€§çƒ­å›¾
        im1 = ax1.imshow(agreement_matrix, cmap='YlOrRd', vmin=0, vmax=1)
        ax1.set_title('Model Agreement Matrix\n(Proportion of samples with same prediction)',
                      fontsize=12, weight='bold')

        compressed_names = [self.compress_column_name(name, 10) for name in self.model_names_list]
        ax1.set_xticks(range(n_models))
        ax1.set_yticks(range(n_models))
        ax1.set_xticklabels(compressed_names, rotation=45, ha='right', fontsize=8)
        ax1.set_yticklabels(compressed_names, fontsize=8)

        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(n_models):
            for j in range(n_models):
                text = ax1.text(j, i, f'{agreement_matrix[i, j]:.2f}',
                                ha="center", va="center",
                                color="white" if agreement_matrix[i, j] > 0.5 else "black",
                                fontsize=6)

        cbar1 = plt.colorbar(im1, ax=ax1)
        cbar1.set_label('Agreement Rate', fontsize=10)

        # 2. æŠ•ç¥¨åˆ†æ­§æ¡ˆä¾‹åˆ†æ
        ax2 = axes[0, 1]

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„åˆ†æ­§ç¨‹åº¦ï¼ˆæ ‡å‡†å·®æˆ–ç†µï¼‰
        disagreement_scores = []
        for i in range(len(self.X_test_ensemble)):
            votes = np.bincount(self.voting_matrix[i, :], minlength=len(np.unique(self.y)))
            # ä½¿ç”¨ç†µæ¥è¡¡é‡åˆ†æ­§ç¨‹åº¦
            probs = votes / np.sum(votes)
            entropy = -np.sum(probs * np.log(probs + 1e-10))
            disagreement_scores.append(entropy)

        disagreement_scores = np.array(disagreement_scores)

        # ç»˜åˆ¶åˆ†æ­§ç¨‹åº¦åˆ†å¸ƒ
        ax2.hist(disagreement_scores, bins=20, alpha=0.7, color='lightblue', edgecolor='black')
        ax2.axvline(np.mean(disagreement_scores), color='red', linestyle='--', linewidth=2,
                    label=f'Mean: {np.mean(disagreement_scores):.3f}')
        ax2.set_xlabel('Disagreement Score (Entropy)', fontsize=12)
        ax2.set_ylabel('Number of Samples', fontsize=12)
        ax2.set_title('Model Disagreement Distribution', fontsize=14, weight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. é«˜åˆ†æ­§æ¡ˆä¾‹åˆ†æ
        ax3 = axes[1, 0]

        # æ‰¾åˆ°åˆ†æ­§æœ€å¤§çš„æ¡ˆä¾‹
        high_disagreement_indices = np.argsort(disagreement_scores)[-10:]

        # ç»˜åˆ¶è¿™äº›æ¡ˆä¾‹çš„æŠ•ç¥¨æ¨¡å¼
        high_disagreement_votes = self.voting_matrix[high_disagreement_indices, :]

        im3 = ax3.imshow(high_disagreement_votes.T, cmap='viridis', aspect='auto')
        ax3.set_title('High Disagreement Cases\n(Top 10 most controversial samples)',
                      fontsize=12, weight='bold')
        ax3.set_xlabel('Sample Index (sorted by disagreement)')
        ax3.set_ylabel('Models')
        ax3.set_yticks(range(0, len(self.model_names_list), 2))
        ax3.set_yticklabels([compressed_names[i] for i in range(0, len(compressed_names), 2)], fontsize=8)

        cbar3 = plt.colorbar(im3, ax=ax3)
        cbar3.set_label('Predicted Class', fontsize=10)

        # 4. æŠ•ç¥¨ç­–ç•¥å‡†ç¡®ç‡ vs ä¸€è‡´æ€§
        ax4 = axes[1, 1]

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ä¸€è‡´æ€§å’ŒæŠ•ç¥¨ç»“æœçš„æ­£ç¡®æ€§
        consensus_levels = []
        hard_voting_correct = []

        for i in range(len(self.X_test_ensemble)):
            votes = np.bincount(self.voting_matrix[i, :], minlength=len(np.unique(self.y)))
            max_votes = np.max(votes)
            consensus = max_votes / len(self.model_names_list)
            consensus_levels.append(consensus)

            # ç¡¬æŠ•ç¥¨æ˜¯å¦æ­£ç¡®
            hard_pred = self.voting_results['hard_voting']['predictions'][i]
            is_correct = (hard_pred == self.y_test_ensemble[i])
            hard_voting_correct.append(is_correct)

        # ç»˜åˆ¶æ•£ç‚¹å›¾
        colors = ['red' if not correct else 'green' for correct in hard_voting_correct]
        ax4.scatter(consensus_levels, disagreement_scores, c=colors, alpha=0.6, s=50)
        ax4.set_xlabel('Consensus Level (Max Vote Proportion)', fontsize=12)
        ax4.set_ylabel('Disagreement Score (Entropy)', fontsize=12)
        ax4.set_title('Consensus vs Disagreement Analysis', fontsize=14, weight='bold')
        ax4.grid(True, alpha=0.3)

        # æ·»åŠ å›¾ä¾‹
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor='green', label='Correct Prediction'),
                           Patch(facecolor='red', label='Incorrect Prediction')]
        ax4.legend(handles=legend_elements)

        plt.tight_layout()
        plt.savefig(self.output_dir / "model_agreement_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_voting_performance_comparison(self):
        """ç»˜åˆ¶æŠ•ç¥¨æ€§èƒ½è¯¦ç»†æ¯”è¾ƒ"""
        print("Plotting voting performance comparison...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # 1. ROCæ›²çº¿æ¯”è¾ƒï¼ˆå¦‚æœæ˜¯äºŒåˆ†ç±»ï¼‰
        ax1 = axes[0, 0]

        if len(np.unique(self.y)) == 2:
            # ç»˜åˆ¶å„ç§æŠ•ç¥¨æ–¹æ³•çš„ROCæ›²çº¿
            from sklearn.metrics import roc_curve, auc

            methods_with_proba = ['soft_voting', 'weighted_voting']
            colors = ['blue', 'red']

            for method, color in zip(methods_with_proba, colors):
                if self.voting_results[method]['probabilities'] is not None:
                    proba = self.voting_results[method]['probabilities'][:, 1]
                    fpr, tpr, _ = roc_curve(self.y_test_ensemble, proba)
                    roc_auc = auc(fpr, tpr)

                    ax1.plot(fpr, tpr, color=color, linewidth=2,
                             label=f'{method.replace("_", " ").title()} (AUC = {roc_auc:.3f})')

            # æ·»åŠ æœ€ä½³å•ä¸€æ¨¡å‹çš„ROCæ›²çº¿
            best_model_name = max(self.ensemble_predictions.keys(),
                                  key=lambda x: self.ensemble_predictions[x]['accuracy'])
            best_model_proba = self.ensemble_predictions[best_model_name]['y_proba']
            if best_model_proba is not None:
                fpr, tpr, _ = roc_curve(self.y_test_ensemble, best_model_proba[:, 1])
                roc_auc = auc(fpr, tpr)
                ax1.plot(fpr, tpr, color='green', linewidth=2, linestyle='--',
                         label=f'Best Single Model (AUC = {roc_auc:.3f})')

            ax1.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)
            ax1.set_xlim([0.0, 1.0])
            ax1.set_ylim([0.0, 1.05])
            ax1.set_xlabel('False Positive Rate', fontsize=12)
            ax1.set_ylabel('True Positive Rate', fontsize=12)
            ax1.set_title('ROC Curves Comparison', fontsize=14, weight='bold')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
        else:
            ax1.text(0.5, 0.5, 'ROC Curve\n(Binary Classification Only)',
                     ha='center', va='center', transform=ax1.transAxes, fontsize=14)

        # 2. æ··æ·†çŸ©é˜µæ¯”è¾ƒï¼ˆä¿®æ”¹ä¸ºä¸æœºå™¨å­¦ä¹ æ¨¡å‹ä¸€è‡´çš„æ ¼å¼ï¼‰
        ax2 = axes[0, 1]

        # é€‰æ‹©æœ€ä½³æŠ•ç¥¨æ–¹æ³•
        best_voting_method = max(self.voting_results.keys(),
                                 key=lambda x: self.voting_results[x]['accuracy'])
        best_predictions = self.voting_results[best_voting_method]['predictions']

        # ç»˜åˆ¶æ··æ·†çŸ©é˜µï¼ˆä½¿ç”¨ä¸æœºå™¨å­¦ä¹ æ¨¡å‹ç›¸åŒçš„æ ¼å¼ï¼‰
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(self.y_test_ensemble, best_predictions)

        # è®¡ç®—ç™¾åˆ†æ¯”ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰
        cm_percent = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=4)

        # åˆ›å»ºçƒ­å›¾ï¼ˆæ˜¾ç¤ºç™¾åˆ†æ¯”ï¼‰
        sns.heatmap(cm_percent, annot=False, fmt='.2%', cmap='Blues',
                    xticklabels=[f'Class {i}' for i in range(len(np.unique(self.y)))],
                    yticklabels=[f'Class {i}' for i in range(len(np.unique(self.y)))],
                    cbar_kws={'format': '%.2f'}, vmin=0, vmax=1, ax=ax2)

        # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ·»åŠ æ•°é‡å’Œç™¾åˆ†æ¯”ç»„åˆæ ‡ç­¾
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                # æ•°é‡ (n=xx)
                count = cm[i, j]
                # ç™¾åˆ†æ¯” (xx%)
                percent = cm_percent[i, j] * 100
                # ç»„åˆæ–‡æœ¬
                text = f"{count}\n({percent:.1f}%)"
                # æ ¹æ®èƒŒæ™¯è‰²è‡ªåŠ¨é€‰æ‹©æ–‡å­—é¢œè‰²
                bg_color = cm_percent[i, j]
                text_color = 'white' if bg_color > 0.5 else 'black'

                ax2.text(j + 0.5, i + 0.5, text,
                         ha='center', va='center',
                         color=text_color, fontsize=10)

        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        ax2.set_title(f'Confusion Matrix - {best_voting_method.replace("_", " ").title()}\n(Count and Percentage)',
                      fontsize=12, pad=20)
        ax2.set_xlabel('Predicted Label', fontsize=10)
        ax2.set_ylabel('True Label', fontsize=10)

        # è°ƒæ•´é¢œè‰²æ¡æ ‡ç­¾ä¸ºç™¾åˆ†æ¯”å°æ•°å½¢å¼
        cbar = ax2.collections[0].colorbar
        cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])
        cbar.set_ticklabels(['0%', '20%', '40%', '60%', '80%', '100%'])
        cbar.set_label('Percentage', fontsize=10)

        # 3. è¯¦ç»†æ€§èƒ½æŒ‡æ ‡æ¯”è¾ƒ
        ax3 = axes[1, 0]

        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        from sklearn.metrics import precision_score, recall_score, f1_score

        metrics_data = []
        voting_methods = ['hard_voting', 'soft_voting', 'weighted_voting']

        for method in voting_methods:
            predictions = self.voting_results[method]['predictions']

            metrics = {
                'Method': method.replace('_', ' ').title(),
                'Accuracy': accuracy_score(self.y_test_ensemble, predictions),
                'Precision': precision_score(self.y_test_ensemble, predictions, average='weighted', zero_division=0),
                'Recall': recall_score(self.y_test_ensemble, predictions, average='weighted', zero_division=0),
                'F1-Score': f1_score(self.y_test_ensemble, predictions, average='weighted', zero_division=0)
            }
            metrics_data.append(metrics)

        # æ·»åŠ æœ€ä½³å•ä¸€æ¨¡å‹çš„æ€§èƒ½
        best_single_pred = self.ensemble_predictions[best_model_name]['y_pred']
        metrics_data.append({
            'Method': 'Best Single Model',
            'Accuracy': accuracy_score(self.y_test_ensemble, best_single_pred),
            'Precision': precision_score(self.y_test_ensemble, best_single_pred, average='weighted', zero_division=0),
            'Recall': recall_score(self.y_test_ensemble, best_single_pred, average='weighted', zero_division=0),
            'F1-Score': f1_score(self.y_test_ensemble, best_single_pred, average='weighted', zero_division=0)
        })

        # ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡æ¡å½¢å›¾
        metrics_df = pd.DataFrame(metrics_data)
        x = np.arange(len(metrics_df))
        width = 0.2

        metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange']

        for i, (metric, color) in enumerate(zip(metrics_to_plot, colors)):
            ax3.bar(x + i * width, metrics_df[metric], width, label=metric, color=color, alpha=0.8)

        ax3.set_xlabel('Methods', fontsize=12)
        ax3.set_ylabel('Score', fontsize=12)
        ax3.set_title('Performance Metrics Comparison', fontsize=14, weight='bold')
        ax3.set_xticks(x + width * 1.5)
        ax3.set_xticklabels(metrics_df['Method'], rotation=45, ha='right')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim([0, 1])

        # 4. é¢„æµ‹ç½®ä¿¡åº¦ç®±çº¿å›¾
        ax4 = axes[1, 1]

        confidence_data = []
        confidence_labels = []

        for method in ['soft_voting', 'weighted_voting']:
            if self.voting_results[method]['probabilities'] is not None:
                max_proba = np.max(self.voting_results[method]['probabilities'], axis=1)
                confidence_data.append(max_proba)
                confidence_labels.append(method.replace('_', ' ').title())

        if confidence_data:
            bp = ax4.boxplot(confidence_data, labels=confidence_labels, patch_artist=True)

            # è®¾ç½®ç®±çº¿å›¾é¢œè‰²
            colors = ['lightblue', 'lightcoral']
            for patch, color in zip(bp['boxes'], colors):
                patch.set_facecolor(color)
                patch.set_alpha(0.7)

            ax4.set_ylabel('Prediction Confidence', fontsize=12)
            ax4.set_title('Prediction Confidence Distribution', fontsize=14, weight='bold')
            ax4.grid(True, alpha=0.3)
            ax4.set_ylim([0, 1])

        plt.tight_layout()
        plt.savefig(self.output_dir / "voting_performance_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_voting_confusion_matrices(self):
        """ç»˜åˆ¶å„ç§æŠ•ç¥¨æ–¹æ³•çš„æ··æ·†çŸ©é˜µï¼ˆä¸æœºå™¨å­¦ä¹ æ¨¡å‹æ ¼å¼ä¸€è‡´ï¼‰"""
        print("Plotting voting confusion matrices...")

        voting_methods = ['hard_voting', 'soft_voting', 'weighted_voting']
        n_methods = len(voting_methods)

        fig, axes = plt.subplots(1, n_methods, figsize=(5 * n_methods, 6))
        if n_methods == 1:
            axes = [axes]

        for idx, method in enumerate(voting_methods):
            ax = axes[idx]

            predictions = self.voting_results[method]['predictions']

            # è®¡ç®—æ··æ·†çŸ©é˜µ
            from sklearn.metrics import confusion_matrix
            cm = confusion_matrix(self.y_test_ensemble, predictions)

            # è®¡ç®—ç™¾åˆ†æ¯”ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰
            cm_percent = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=4)

            # åˆ›å»ºçƒ­å›¾ï¼ˆæ˜¾ç¤ºç™¾åˆ†æ¯”ï¼‰
            class_names = [f'Class {i}' for i in range(len(np.unique(self.y)))]
            sns.heatmap(cm_percent, annot=False, fmt='.2%', cmap='Blues',
                        xticklabels=class_names, yticklabels=class_names,
                        cbar_kws={'format': '%.2f'}, vmin=0, vmax=1, ax=ax)

            # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ·»åŠ æ•°é‡å’Œç™¾åˆ†æ¯”ç»„åˆæ ‡ç­¾
            for i in range(cm.shape[0]):
                for j in range(cm.shape[1]):
                    # æ•°é‡ (n=xx)
                    count = cm[i, j]
                    # ç™¾åˆ†æ¯” (xx%)
                    percent = cm_percent[i, j] * 100
                    # ç»„åˆæ–‡æœ¬
                    text = f"{count}\n({percent:.1f}%)"
                    # æ ¹æ®èƒŒæ™¯è‰²è‡ªåŠ¨é€‰æ‹©æ–‡å­—é¢œè‰²
                    bg_color = cm_percent[i, j]
                    text_color = 'white' if bg_color > 0.5 else 'black'

                    ax.text(j + 0.5, i + 0.5, text,
                            ha='center', va='center',
                            color=text_color, fontsize=10)

            # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
            method_title = method.replace('_', ' ').title()
            accuracy = self.voting_results[method]['accuracy']
            ax.set_title(f'{method_title}\n(Count and Percentage)\nAccuracy: {accuracy:.3f}',
                         fontsize=12, pad=20)
            ax.set_xlabel('Predicted Label', fontsize=10)
            ax.set_ylabel('True Label', fontsize=10)

            # è°ƒæ•´é¢œè‰²æ¡æ ‡ç­¾ä¸ºç™¾åˆ†æ¯”å°æ•°å½¢å¼
            cbar = ax.collections[0].colorbar
            cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])
            cbar.set_ticklabels(['0%', '20%', '40%', '60%', '80%', '100%'])

        plt.suptitle('Voting Methods Confusion Matrices Comparison', fontsize=16, weight='bold')
        plt.tight_layout()
        plt.savefig(self.output_dir / "voting_confusion_matrices.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_individual_case_analysis(self):
        """ç»˜åˆ¶ä¸ªæ¡ˆåˆ†æå›¾"""
        print("Plotting individual case analysis...")

        # é€‰æ‹©ä¸€äº›æœ‰è¶£çš„æ¡ˆä¾‹è¿›è¡Œè¯¦ç»†åˆ†æ
        # 1. å…¨éƒ¨æ¨¡å‹ä¸€è‡´çš„æ¡ˆä¾‹
        # 2. é«˜åˆ†æ­§çš„æ¡ˆä¾‹
        # 3. æŠ•ç¥¨æ­£ç¡®ä½†å¤§å¤šæ•°æ¨¡å‹é”™è¯¯çš„æ¡ˆä¾‹
        # 4. æŠ•ç¥¨é”™è¯¯ä½†å¤§å¤šæ•°æ¨¡å‹æ­£ç¡®çš„æ¡ˆä¾‹

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ä¸€è‡´æ€§å’Œåˆ†æ­§
        consensus_scores = []
        disagreement_scores = []

        for i in range(len(self.X_test_ensemble)):
            votes = np.bincount(self.voting_matrix[i, :], minlength=len(np.unique(self.y)))
            max_votes = np.max(votes)
            consensus = max_votes / len(self.model_names_list)
            consensus_scores.append(consensus)

            # è®¡ç®—åˆ†æ­§ï¼ˆç†µï¼‰
            probs = votes / np.sum(votes)
            entropy = -np.sum(probs * np.log(probs + 1e-10))
            disagreement_scores.append(entropy)

        consensus_scores = np.array(consensus_scores)
        disagreement_scores = np.array(disagreement_scores)

        # 1. é«˜ä¸€è‡´æ€§æ¡ˆä¾‹ï¼ˆå‰10ä¸ªï¼‰
        ax1 = axes[0, 0]
        high_consensus_indices = np.argsort(consensus_scores)[-10:]

        case_data = []
        for idx in high_consensus_indices:
            votes = np.bincount(self.voting_matrix[idx, :], minlength=len(np.unique(self.y)))
            case_data.append(votes)

        case_data = np.array(case_data)

        im1 = ax1.imshow(case_data, cmap='YlOrRd', aspect='auto')
        ax1.set_title('High Consensus Cases\n(Vote distribution for each class)', fontsize=12, weight='bold')
        ax1.set_xlabel('Class')
        ax1.set_ylabel('Sample Index (High Consensus)')
        ax1.set_xticks(range(len(np.unique(self.y))))
        ax1.set_xticklabels([f'Class {i}' for i in range(len(np.unique(self.y)))])

        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(case_data.shape[0]):
            for j in range(case_data.shape[1]):
                text = ax1.text(j, i, f'{case_data[i, j]}',
                                ha="center", va="center",
                                color="white" if case_data[i, j] > len(self.model_names_list) / 2 else "black",
                                fontsize=10)

        cbar1 = plt.colorbar(im1, ax=ax1)
        cbar1.set_label('Number of Votes', fontsize=10)

        # 2. é«˜åˆ†æ­§æ¡ˆä¾‹ï¼ˆå‰10ä¸ªï¼‰
        ax2 = axes[0, 1]
        high_disagreement_indices = np.argsort(disagreement_scores)[-10:]

        case_data_2 = []
        for idx in high_disagreement_indices:
            votes = np.bincount(self.voting_matrix[idx, :], minlength=len(np.unique(self.y)))
            case_data_2.append(votes)

        case_data_2 = np.array(case_data_2)

        im2 = ax2.imshow(case_data_2, cmap='RdYlBu', aspect='auto')
        ax2.set_title('High Disagreement Cases\n(Vote distribution for each class)', fontsize=12, weight='bold')
        ax2.set_xlabel('Class')
        ax2.set_ylabel('Sample Index (High Disagreement)')
        ax2.set_xticks(range(len(np.unique(self.y))))
        ax2.set_xticklabels([f'Class {i}' for i in range(len(np.unique(self.y)))])

        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(case_data_2.shape[0]):
            for j in range(case_data_2.shape[1]):
                text = ax2.text(j, i, f'{case_data_2[i, j]}',
                                ha="center", va="center",
                                color="white" if case_data_2[i, j] > len(self.model_names_list) / 3 else "black",
                                fontsize=10)

        cbar2 = plt.colorbar(im2, ax=ax2)
        cbar2.set_label('Number of Votes', fontsize=10)

        # 3. æŠ•ç¥¨æ•ˆæœåˆ†æ
        ax3 = axes[1, 0]

        # åˆ†ææŠ•ç¥¨æ˜¯å¦æ”¹å–„äº†é¢„æµ‹
        hard_voting_correct = (self.voting_results['hard_voting']['predictions'] == self.y_test_ensemble)
        individual_correct_counts = np.sum(self.voting_matrix == self.y_test_ensemble[:, np.newaxis], axis=1)
        majority_threshold = len(self.model_names_list) / 2

        # å››ä¸ªè±¡é™çš„æ¡ˆä¾‹
        # 1. æŠ•ç¥¨æ­£ç¡®ï¼Œå¤šæ•°æ¨¡å‹æ­£ç¡®
        quad1 = np.sum((hard_voting_correct) & (individual_correct_counts > majority_threshold))
        # 2. æŠ•ç¥¨æ­£ç¡®ï¼Œå¤šæ•°æ¨¡å‹é”™è¯¯
        quad2 = np.sum((hard_voting_correct) & (individual_correct_counts <= majority_threshold))
        # 3. æŠ•ç¥¨é”™è¯¯ï¼Œå¤šæ•°æ¨¡å‹æ­£ç¡®
        quad3 = np.sum((~hard_voting_correct) & (individual_correct_counts > majority_threshold))
        # 4. æŠ•ç¥¨é”™è¯¯ï¼Œå¤šæ•°æ¨¡å‹é”™è¯¯
        quad4 = np.sum((~hard_voting_correct) & (individual_correct_counts <= majority_threshold))

        categories = ['Votingâœ“\nMajorityâœ“', 'Votingâœ“\nMajorityâœ—', 'Votingâœ—\nMajorityâœ“', 'Votingâœ—\nMajorityâœ—']
        counts = [quad1, quad2, quad3, quad4]
        colors = ['green', 'orange', 'red', 'darkred']

        bars = ax3.bar(categories, counts, color=colors, alpha=0.7)
        ax3.set_ylabel('Number of Cases', fontsize=12)
        ax3.set_title('Voting Effectiveness Analysis', fontsize=14, weight='bold')
        ax3.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾å’Œç™¾åˆ†æ¯”
        total_cases = len(self.y_test_ensemble)
        for bar, count in zip(bars, counts):
            height = bar.get_height()
            percentage = count / total_cases * 100
            ax3.text(bar.get_x() + bar.get_width() / 2., height + 0.5,
                     f'{count}\n({percentage:.1f}%)', ha='center', va='bottom', fontsize=10)

        # 4. é”™è¯¯æ¡ˆä¾‹è¯¦ç»†åˆ†æ
        ax4 = axes[1, 1]

        # æ‰¾åˆ°æŠ•ç¥¨é”™è¯¯çš„æ¡ˆä¾‹ï¼Œåˆ†æé”™è¯¯ç±»å‹
        wrong_cases = ~hard_voting_correct
        if np.any(wrong_cases):
            wrong_indices = np.where(wrong_cases)[0]

            # åˆ†æè¿™äº›é”™è¯¯æ¡ˆä¾‹çš„æŠ•ç¥¨æ¨¡å¼
            error_analysis = {
                'True Class': [],
                'Predicted Class': [],
                'Confidence Level': [],
                'Model Agreement': []
            }

            for idx in wrong_indices[:10]:  # åªåˆ†æå‰10ä¸ªé”™è¯¯æ¡ˆä¾‹
                true_class = self.y_test_ensemble[idx]
                pred_class = self.voting_results['hard_voting']['predictions'][idx]

                votes = np.bincount(self.voting_matrix[idx, :], minlength=len(np.unique(self.y)))
                confidence = np.max(votes) / len(self.model_names_list)
                agreement = np.sum(self.voting_matrix[idx, :] == pred_class) / len(self.model_names_list)

                error_analysis['True Class'].append(true_class)
                error_analysis['Predicted Class'].append(pred_class)
                error_analysis['Confidence Level'].append(confidence)
                error_analysis['Model Agreement'].append(agreement)

            # ç»˜åˆ¶é”™è¯¯æ¡ˆä¾‹çš„ç½®ä¿¡åº¦åˆ†å¸ƒ
            if error_analysis['Confidence Level']:
                ax4.scatter(range(len(error_analysis['Confidence Level'])),
                            error_analysis['Confidence Level'],
                            c='red', s=100, alpha=0.7, label='Error Cases')

                ax4.set_xlabel('Error Case Index', fontsize=12)
                ax4.set_ylabel('Voting Confidence', fontsize=12)
                ax4.set_title('Confidence Level of Error Cases', fontsize=14, weight='bold')
                ax4.grid(True, alpha=0.3)
                ax4.set_ylim([0, 1])

                # æ·»åŠ å¹³å‡çº¿
                avg_confidence = np.mean(error_analysis['Confidence Level'])
                ax4.axhline(y=avg_confidence, color='red', linestyle='--',
                            label=f'Avg Confidence: {avg_confidence:.3f}')
                ax4.legend()

        plt.tight_layout()
        plt.savefig(self.output_dir / "individual_case_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_consensus_confidence_analysis(self):
        """ç»˜åˆ¶å…±è¯†ç½®ä¿¡åº¦åˆ†æ"""
        print("Plotting consensus confidence analysis...")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # è®¡ç®—ç›¸å…³æŒ‡æ ‡
        consensus_scores = []
        prediction_correctness = []
        model_diversity = []

        for i in range(len(self.X_test_ensemble)):
            votes = np.bincount(self.voting_matrix[i, :], minlength=len(np.unique(self.y)))

            # å…±è¯†ç¨‹åº¦
            max_votes = np.max(votes)
            consensus = max_votes / len(self.model_names_list)
            consensus_scores.append(consensus)

            # é¢„æµ‹æ­£ç¡®æ€§
            hard_pred = self.voting_results['hard_voting']['predictions'][i]
            is_correct = (hard_pred == self.y_test_ensemble[i])
            prediction_correctness.append(is_correct)

            # æ¨¡å‹å¤šæ ·æ€§ï¼ˆåŸºäºé¢„æµ‹åˆ†å¸ƒçš„ç†µï¼‰
            vote_probs = votes / np.sum(votes)
            entropy = -np.sum(vote_probs * np.log(vote_probs + 1e-10))
            model_diversity.append(entropy)

        consensus_scores = np.array(consensus_scores)
        prediction_correctness = np.array(prediction_correctness)
        model_diversity = np.array(model_diversity)

        # 1. å…±è¯†ç¨‹åº¦ vs é¢„æµ‹å‡†ç¡®æ€§
        ax1 = axes[0, 0]

        correct_mask = prediction_correctness
        incorrect_mask = ~prediction_correctness

        ax1.scatter(consensus_scores[correct_mask], model_diversity[correct_mask],
                    c='green', alpha=0.6, s=50, label='Correct Predictions')
        ax1.scatter(consensus_scores[incorrect_mask], model_diversity[incorrect_mask],
                    c='red', alpha=0.6, s=50, label='Incorrect Predictions')

        ax1.set_xlabel('Consensus Level', fontsize=12)
        ax1.set_ylabel('Model Diversity (Entropy)', fontsize=12)
        ax1.set_title('Consensus vs Diversity Analysis', fontsize=14, weight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # æ·»åŠ è¶‹åŠ¿çº¿
        from scipy.stats import pearsonr
        correlation, p_value = pearsonr(consensus_scores, model_diversity)
        ax1.text(0.80, 0.80, f'Correlation: {correlation:.3f}\np-value: {p_value:.3f}',
                 transform=ax1.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        # 2. å…±è¯†ç¨‹åº¦åˆ†å¸ƒï¼ˆæŒ‰æ­£ç¡®æ€§åˆ†ç»„ï¼‰
        ax2 = axes[0, 1]

        consensus_correct = consensus_scores[correct_mask]
        consensus_incorrect = consensus_scores[incorrect_mask]

        ax2.hist(consensus_correct, bins=20, alpha=0.7, color='green',
                 label=f'Correct ({len(consensus_correct)} cases)', density=True)
        ax2.hist(consensus_incorrect, bins=20, alpha=0.7, color='red',
                 label=f'Incorrect ({len(consensus_incorrect)} cases)', density=True)

        ax2.set_xlabel('Consensus Level', fontsize=12)
        ax2.set_ylabel('Density', fontsize=12)
        ax2.set_title('Consensus Distribution by Prediction Correctness', fontsize=14, weight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if len(consensus_correct) > 0 and len(consensus_incorrect) > 0:
            from scipy.stats import ttest_ind
            t_stat, t_p_value = ttest_ind(consensus_correct, consensus_incorrect)
            ax2.text(0.02, 0.80, f'Mean Correct: {np.mean(consensus_correct):.3f}\n'
                                 f'Mean Incorrect: {np.mean(consensus_incorrect):.3f}\n'
                                 f't-test p-value: {t_p_value:.3f}',
                     transform=ax2.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        # 3. æŠ•ç¥¨æ•ˆç‡åˆ†æ
        ax3 = axes[1, 0]

        # æŒ‰å…±è¯†ç¨‹åº¦åˆ†ç»„ï¼Œè®¡ç®—å‡†ç¡®ç‡
        consensus_bins = np.linspace(0, 1, 11)
        bin_centers = (consensus_bins[:-1] + consensus_bins[1:]) / 2
        accuracy_by_consensus = []
        sample_counts = []

        for i in range(len(consensus_bins) - 1):
            mask = (consensus_scores >= consensus_bins[i]) & (consensus_scores < consensus_bins[i + 1])
            if i == len(consensus_bins) - 2:  # æœ€åä¸€ä¸ªbinåŒ…å«å³ç«¯ç‚¹
                mask = (consensus_scores >= consensus_bins[i]) & (consensus_scores <= consensus_bins[i + 1])

            if np.any(mask):
                accuracy = np.mean(prediction_correctness[mask])
                count = np.sum(mask)
            else:
                accuracy = 0
                count = 0

            accuracy_by_consensus.append(accuracy)
            sample_counts.append(count)

        bars = ax3.bar(bin_centers, accuracy_by_consensus, width=0.08, alpha=0.7, color='skyblue')
        ax3.set_xlabel('Consensus Level', fontsize=12)
        ax3.set_ylabel('Accuracy', fontsize=12)
        ax3.set_title('Accuracy vs Consensus Level', fontsize=14, weight='bold')
        ax3.set_ylim([0, 1])
        ax3.grid(True, alpha=0.3)

        # æ·»åŠ æ ·æœ¬æ•°é‡æ ‡ç­¾
        for bar, count in zip(bars, sample_counts):
            if count > 0:
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width() / 2., height + 0.02,
                         f'n={count}', ha='center', va='bottom', fontsize=8)

        # 4. æ¨¡å‹è´¡çŒ®åº¦åˆ†æ
        ax4 = axes[1, 1]

        # è®¡ç®—æ¯ä¸ªæ¨¡å‹å¯¹æœ€ç»ˆå†³ç­–çš„è´¡çŒ®åº¦
        model_contributions = np.zeros(len(self.model_names_list))

        for i in range(len(self.X_test_ensemble)):
            final_prediction = self.voting_results['hard_voting']['predictions'][i]
            # è®¡ç®—æ¯ä¸ªæ¨¡å‹æ˜¯å¦ä¸æœ€ç»ˆå†³ç­–ä¸€è‡´
            for j, model_pred in enumerate(self.voting_matrix[i, :]):
                if model_pred == final_prediction:
                    model_contributions[j] += 1

        # å½’ä¸€åŒ–ä¸ºç™¾åˆ†æ¯”
        model_contributions = model_contributions / len(self.X_test_ensemble) * 100

        # æŒ‰è´¡çŒ®åº¦æ’åº
        sorted_indices = np.argsort(model_contributions)[::-1]
        sorted_names = [self.compress_column_name(self.model_names_list[i], 15) for i in sorted_indices]
        sorted_contributions = model_contributions[sorted_indices]

        bars = ax4.barh(range(len(sorted_names)), sorted_contributions, color='lightcoral', alpha=0.7)
        ax4.set_yticks(range(len(sorted_names)))
        ax4.set_yticklabels(sorted_names, fontsize=8)
        ax4.set_xlabel('Contribution to Final Decision (%)', fontsize=12)
        ax4.set_title('Model Contribution Analysis', fontsize=14, weight='bold')
        ax4.grid(True, alpha=0.3)

        # æ·»åŠ è´¡çŒ®åº¦æ•°å€¼æ ‡ç­¾
        for i, (bar, contrib) in enumerate(zip(bars, sorted_contributions)):
            width = bar.get_width()
            ax4.text(width + 1, bar.get_y() + bar.get_height() / 2,
                     f'{contrib:.1f}%', ha='left', va='center', fontsize=8)

        plt.tight_layout()
        plt.savefig(self.output_dir / "consensus_confidence_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

    def save_voting_results_summary(self):
        """ä¿å­˜æŠ•ç¥¨ç»“æœç»¼åˆæ‘˜è¦"""
        print("Saving voting results summary...")

        # åˆ›å»ºç»¼åˆæ‘˜è¦Excelæ–‡ä»¶
        with pd.ExcelWriter(self.output_dir / "voting_analysis_comprehensive_summary.xlsx",
                            engine='openpyxl') as writer:

            # 1. æŠ•ç¥¨ç­–ç•¥æ€§èƒ½æ¯”è¾ƒ
            voting_performance = []
            for method, results in self.voting_results.items():
                if method != 'majority_voting':  # æ’é™¤å¯èƒ½æœ‰æŠ½è±¡é—®é¢˜çš„å¤šæ•°æŠ•ç¥¨
                    voting_performance.append({
                        'Voting_Strategy': method.replace('_', ' ').title(),
                        'Accuracy': results['accuracy'],
                        'Total_Errors': np.sum(results['predictions'] != self.y_test_ensemble),
                        'Error_Rate': np.mean(results['predictions'] != self.y_test_ensemble)
                    })

            voting_perf_df = pd.DataFrame(voting_performance)
            voting_perf_df.to_excel(writer, sheet_name='Voting_Performance', index=False)

            # 2. ä¸ªä½“æ¨¡å‹æ€§èƒ½
            individual_performance = []
            for name, pred_data in self.ensemble_predictions.items():
                individual_performance.append({
                    'Model_Name': name,
                    'Accuracy': pred_data['accuracy'],
                    'Total_Errors': np.sum(pred_data['y_pred'] != self.y_test_ensemble),
                    'Error_Rate': np.mean(pred_data['y_pred'] != self.y_test_ensemble)
                })

            individual_perf_df = pd.DataFrame(individual_performance)
            individual_perf_df = individual_perf_df.sort_values('Accuracy', ascending=False)
            individual_perf_df.to_excel(writer, sheet_name='Individual_Performance', index=False)

            # 3. è¯¦ç»†æ¡ˆä¾‹åˆ†æ
            case_analysis = []
            for i in range(len(self.X_test_ensemble)):
                votes = np.bincount(self.voting_matrix[i, :], minlength=len(np.unique(self.y)))
                max_votes = np.max(votes)
                consensus = max_votes / len(self.model_names_list)

                vote_probs = votes / np.sum(votes)
                entropy = -np.sum(vote_probs * np.log(vote_probs + 1e-10))

                case_analysis.append({
                    'Sample_Index': i,
                    'True_Label': self.y_test_ensemble[i],
                    'Hard_Voting_Pred': self.voting_results['hard_voting']['predictions'][i],
                    'Soft_Voting_Pred': self.voting_results['soft_voting']['predictions'][i],
                    'Weighted_Voting_Pred': self.voting_results['weighted_voting']['predictions'][i],
                    'Consensus_Level': consensus,
                    'Disagreement_Score': entropy,
                    'Models_Correct': np.sum(self.voting_matrix[i, :] == self.y_test_ensemble[i]),
                    'Hard_Voting_Correct': self.voting_results['hard_voting']['predictions'][i] == self.y_test_ensemble[
                        i]
                })

            case_analysis_df = pd.DataFrame(case_analysis)
            case_analysis_df.to_excel(writer, sheet_name='Case_Analysis', index=False)

            # 4. æ¨¡å‹æƒé‡å’Œè´¡çŒ®åº¦
            model_analysis = []
            for i, name in enumerate(self.model_names_list):
                # è®¡ç®—æ¨¡å‹ä¸æœ€ç»ˆå†³ç­–çš„ä¸€è‡´æ€§
                final_decisions = self.voting_results['hard_voting']['predictions']
                agreement_rate = np.mean(self.voting_matrix[:, i] == final_decisions)

                # è·å–æƒé‡ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                weight = self.voting_results['weighted_voting']['weights'][i] if 'weights' in self.voting_results[
                    'weighted_voting'] else 0

                model_analysis.append({
                    'Model_Name': name,
                    'Individual_Accuracy': self.ensemble_predictions[name]['accuracy'],
                    'Weight_in_Weighted_Voting': weight,
                    'Agreement_with_Final_Decision': agreement_rate,
                    'Contribution_Score': agreement_rate * self.ensemble_predictions[name]['accuracy']
                })

            model_analysis_df = pd.DataFrame(model_analysis)
            model_analysis_df = model_analysis_df.sort_values('Contribution_Score', ascending=False)
            model_analysis_df.to_excel(writer, sheet_name='Model_Analysis', index=False)

        print("Comprehensive voting analysis summary saved successfully!")

        # è¿”å›å…³é”®ç»Ÿè®¡ä¿¡æ¯
        best_voting_method = max(self.voting_results.keys(),
                                 key=lambda x: self.voting_results[x]['accuracy'])
        best_individual_model = max(self.ensemble_predictions.keys(),
                                    key=lambda x: self.ensemble_predictions[x]['accuracy'])

        summary_stats = {
            'best_voting_method': best_voting_method,
            'best_voting_accuracy': self.voting_results[best_voting_method]['accuracy'],
            'best_individual_model': best_individual_model,
            'best_individual_accuracy': self.ensemble_predictions[best_individual_model]['accuracy'],
            'ensemble_improvement': self.voting_results[best_voting_method]['accuracy'] -
                                    self.ensemble_predictions[best_individual_model]['accuracy'],
            'total_models_used': len(self.model_names_list),
            'test_samples': len(self.y_test_ensemble)
        }

        return summary_stats


    def plot_confusion_matrix(self, model_name, cm, classes):
        """ç»˜åˆ¶åŒæ—¶æ˜¾ç¤ºæ•°é‡å’Œç™¾åˆ†æ¯”çš„æ··æ·†çŸ©é˜µ"""
        plt.figure(figsize=(10, 8))

        # è®¡ç®—ç™¾åˆ†æ¯”ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰
        cm_percent = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=4)

        # åˆ›å»ºçƒ­å›¾ï¼ˆæ˜¾ç¤ºç™¾åˆ†æ¯”ï¼‰
        ax = sns.heatmap(cm_percent, annot=False, fmt='.2%', cmap='Blues',
                         xticklabels=classes, yticklabels=classes,
                         cbar_kws={'format': '%.2f'}, vmin=0, vmax=1)

        # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ·»åŠ æ•°é‡å’Œç™¾åˆ†æ¯”ç»„åˆæ ‡ç­¾
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                # æ•°é‡ (n=xx)
                count = cm[i, j]
                # ç™¾åˆ†æ¯” (xx%)
                percent = cm_percent[i, j] * 100
                # ç»„åˆæ–‡æœ¬
                text = f"{count}\n({percent:.1f}%)"
                # æ ¹æ®èƒŒæ™¯è‰²è‡ªåŠ¨é€‰æ‹©æ–‡å­—é¢œè‰²
                bg_color = cm_percent[i, j]
                text_color = 'white' if bg_color > 0.5 else 'black'

                ax.text(j + 0.5, i + 0.5, text,
                        ha='center', va='center',
                        color=text_color, fontsize=10)

        # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
        plt.title(f'Confusion Matrix - {model_name}\n(Count and Percentage)',
                  fontsize=12, pad=20)
        plt.xlabel('Predicted Label', fontsize=10)
        plt.ylabel('True Label', fontsize=10)

        # è°ƒæ•´é¢œè‰²æ¡æ ‡ç­¾ä¸ºç™¾åˆ†æ¯”å°æ•°å½¢å¼
        cbar = ax.collections[0].colorbar
        cbar.set_ticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])
        cbar.set_ticklabels(['0%', '20%', '40%', '60%', '80%', '100%'])

        # è°ƒæ•´åˆ»åº¦æ ‡ç­¾
        plt.xticks(rotation=45, ha='right', fontsize=8)
        plt.yticks(rotation=0, fontsize=8)

        # ä¿å­˜å›¾ç‰‡
        plt.tight_layout()
        plt.savefig(self.output_dir / f"confusion_matrix_{model_name}.png",
                    dpi=300, bbox_inches='tight')
        plt.close()

    def calculate_metrics(self, y_true, y_pred, y_proba, set_type):
        """è®¡ç®—åˆ†ç±»æŒ‡æ ‡"""
        n_classes = len(np.unique(y_true))

        metrics = {
            'Accuracy': accuracy_score(y_true, y_pred),
            'Precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),
            'Recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),
            'F1-Score': f1_score(y_true, y_pred, average='weighted', zero_division=0),
            'Specificity': self.calculate_specificity(y_true, y_pred, n_classes)
        }

        # è®¡ç®—AUC (ä»…ç”¨äºæµ‹è¯•é›†ï¼Œå› ä¸ºéœ€è¦æ¦‚ç‡)
        if set_type == 'test' and y_proba is not None:
            try:
                if n_classes == 2:
                    metrics['AUC'] = roc_auc_score(y_true, y_proba[:, 1])
                else:
                    metrics['AUC'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted')
            except:
                metrics['AUC'] = 0.0
        else:
            metrics['AUC'] = None

        return metrics

    def calculate_specificity(self, y_true, y_pred, n_classes):
        """è®¡ç®—ç‰¹å¼‚æ€§"""
        cm = confusion_matrix(y_true, y_pred)
        specificity_scores = []

        for i in range(n_classes):
            if i < cm.shape[0] and i < cm.shape[1]:
                tn = np.sum(cm) - (np.sum(cm[i, :]) + np.sum(cm[:, i]) - cm[i, i])
                fp = np.sum(cm[:, i]) - cm[i, i]
                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
                specificity_scores.append(specificity)

        return np.mean(specificity_scores) if specificity_scores else 0.0

    def calculate_net_benefit(self, y_true, y_proba, threshold):
        """è®¡ç®—å‡€æ•ˆç›Š"""
        # å¯¹äºå¤šåˆ†ç±»é—®é¢˜ï¼Œè¿™é‡Œä»¥ç¬¬ä¸€ç±»ä¸ºä¾‹
        if y_proba.ndim > 1 and y_proba.shape[1] > 1:
            # å¯¹äºå¤šåˆ†ç±»ï¼Œä½¿ç”¨æœ€å¤§æ¦‚ç‡ä½œä¸ºæ­£ç±»æ¦‚ç‡
            probabilities = np.max(y_proba, axis=1)
            # é¢„æµ‹ä¸ºæ¦‚ç‡æœ€å¤§çš„ç±»
            y_pred = np.argmax(y_proba, axis=1)
            # å°†çœŸå®æ ‡ç­¾è½¬æ¢ä¸ºäºŒåˆ†ç±»ï¼ˆæ˜¯å¦ä¸ºé¢„æµ‹çš„ç±»ï¼‰
            y_binary = (y_true == y_pred).astype(int)
        else:
            # äºŒåˆ†ç±»æƒ…å†µ
            probabilities = y_proba[:, 1] if y_proba.ndim > 1 else y_proba
            y_binary = y_true

        # æ ¹æ®é˜ˆå€¼è¿›è¡Œé¢„æµ‹
        y_pred_threshold = (probabilities >= threshold).astype(int)

        # è®¡ç®—çœŸé˜³æ€§ç‡å’Œå‡é˜³æ€§ç‡
        tp = np.sum((y_binary == 1) & (y_pred_threshold == 1))
        fp = np.sum((y_binary == 0) & (y_pred_threshold == 1))

        n_total = len(y_binary)

        # è®¡ç®—å‡€æ•ˆç›Š
        # å‡€æ•ˆç›Š = (TP/n) - (FP/n) * (pt/(1-pt))
        # å…¶ä¸­ pt æ˜¯é˜ˆå€¼æ¦‚ç‡
        if threshold == 0:
            net_benefit = tp / n_total
        elif threshold == 1:
            net_benefit = 0
        else:
            net_benefit = (tp / n_total) - (fp / n_total) * (threshold / (1 - threshold))

        return net_benefit

    def calculate_net_benefit_all(self, y_true):
        """è®¡ç®—å…¨éƒ¨æ²»ç–—ç­–ç•¥çš„å‡€æ•ˆç›Š"""
        # å‡è®¾æ‰€æœ‰æ‚£è€…éƒ½æ¥å—æ²»ç–—çš„å‡€æ•ˆç›Š
        n_total = len(y_true)
        if hasattr(y_true, 'ndim') and y_true.ndim > 0:
            n_positive = np.sum(y_true == 1) if len(np.unique(y_true)) == 2 else np.sum(y_true == np.max(y_true))
        else:
            n_positive = 1 if y_true == 1 else 0

        return n_positive / n_total

    def plot_decision_curves(self):
        """ç»˜åˆ¶å†³ç­–æ›²çº¿"""
        print("Plotting decision curves...")

        # æ£€æŸ¥æ˜¯å¦ä¸ºäºŒåˆ†ç±»é—®é¢˜
        n_classes = len(np.unique(self.y))

        if n_classes == 2:
            self._plot_binary_decision_curves()
        else:
            self._plot_multiclass_decision_curves()

    def _plot_binary_decision_curves(self):
        """ç»˜åˆ¶äºŒåˆ†ç±»å†³ç­–æ›²çº¿ï¼ˆå‰5ä¸ªæ¨¡å‹ï¼‰"""
        plt.figure(figsize=(12, 8))

        # é˜ˆå€¼èŒƒå›´
        thresholds = np.arange(0, 1, 0.01)

        # è®¡ç®—å‚è€ƒçº¿ï¼šä¸æ²»ç–—ï¼ˆå‡€æ•ˆç›Š=0ï¼‰
        treat_none = np.zeros(len(thresholds))

        # è®¡ç®—å‚è€ƒçº¿ï¼šå…¨éƒ¨æ²»ç–—
        treat_all = []
        for threshold in thresholds:
            if threshold == 0:
                # å½“é˜ˆå€¼ä¸º0æ—¶ï¼Œå…¨éƒ¨æ²»ç–—çš„å‡€æ•ˆç›Šç­‰äºæ‚£ç—…ç‡
                prevalence = np.mean(self.results[list(self.results.keys())[0]]['y_test'])
                treat_all.append(prevalence)
            else:
                # å…¨éƒ¨æ²»ç–—çš„å‡€æ•ˆç›Š
                nb_all = self.calculate_net_benefit_all(self.results[list(self.results.keys())[0]]['y_test'])
                treat_all.append(nb_all - threshold / (1 - threshold))

        # ç»˜åˆ¶å‚è€ƒçº¿
        plt.plot(thresholds, treat_none, 'k--', linewidth=2, label='Treat None')
        plt.plot(thresholds, treat_all, 'gray', linewidth=2, label='Treat All')

        # åªé€‰æ‹©å‰5ä¸ªæœ€ä½³æ¨¡å‹
        top_models = sorted(self.results.items(),
                            key=lambda x: x[1]['test_metrics']['Accuracy'],
                            reverse=True)[:5]

        # ä¸ºå‰5ä¸ªæ¨¡å‹è®¡ç®—å’Œç»˜åˆ¶å†³ç­–æ›²çº¿
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']

        for i, ((name, result), color) in enumerate(zip(top_models, colors)):
            y_test = result['y_test']
            y_proba = result['y_test_proba']

            if y_proba is None:
                continue

            net_benefits = []
            for threshold in thresholds:
                nb = self.calculate_net_benefit(y_test, y_proba, threshold)
                net_benefits.append(nb)

            plt.plot(thresholds, net_benefits, color=color, linewidth=2.5,
                     label=f'{self.compress_column_name(name, 20)}')

        plt.xlim([0, 1])
        plt.ylim([-5, 1])  # ä¿®æ”¹çºµåæ ‡èŒƒå›´
        plt.xlabel('Threshold Probability', fontsize=12)
        plt.ylabel('Net Benefit', fontsize=12)
        plt.title('Decision Curve Analysis - Top 5 Models (Binary Classification)', fontsize=14, weight='bold')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.output_dir / "decision_curves_binary_top5.png", dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_multiclass_decision_curves(self):
        """ç»˜åˆ¶å¤šåˆ†ç±»å†³ç­–æ›²çº¿ï¼ˆå‰5ä¸ªæ¨¡å‹ï¼‰"""
        plt.figure(figsize=(15, 10))

        # é˜ˆå€¼èŒƒå›´
        thresholds = np.arange(0, 1, 0.01)

        # è®¡ç®—å‚è€ƒçº¿
        treat_none = np.zeros(len(thresholds))

        # åªé€‰æ‹©å‰5ä¸ªæœ€ä½³æ¨¡å‹
        top_models = sorted(self.results.items(),
                            key=lambda x: x[1]['test_metrics']['Accuracy'],
                            reverse=True)[:5]

        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']

        for i, ((name, result), color) in enumerate(zip(top_models, colors)):
            y_test = result['y_test']
            y_proba = result['y_test_proba']

            if y_proba is None:
                continue

            net_benefits = []
            for threshold in thresholds:
                # å¯¹äºå¤šåˆ†ç±»ï¼Œè®¡ç®—åŸºäºæœ€é«˜æ¦‚ç‡çš„å‡€æ•ˆç›Š
                max_proba = np.max(y_proba, axis=1)
                predicted_class = np.argmax(y_proba, axis=1)

                # åˆ›å»ºäºŒåˆ†ç±»é—®é¢˜ï¼šé¢„æµ‹æ­£ç¡® vs é¢„æµ‹é”™è¯¯
                correct_predictions = (predicted_class == y_test).astype(int)

                # åŸºäºæœ€é«˜æ¦‚ç‡å’Œæ­£ç¡®æ€§è®¡ç®—å‡€æ•ˆç›Š
                high_confidence = (max_proba >= threshold).astype(int)

                tp = np.sum((correct_predictions == 1) & (high_confidence == 1))
                fp = np.sum((correct_predictions == 0) & (high_confidence == 1))

                n_total = len(y_test)

                if threshold == 0:
                    nb = tp / n_total
                elif threshold == 1:
                    nb = 0
                else:
                    nb = (tp / n_total) - (fp / n_total) * (threshold / (1 - threshold))

                net_benefits.append(nb)

            plt.plot(thresholds, net_benefits, color=color, linewidth=2.5,
                     label=f'{self.compress_column_name(name, 20)}')

        # ç»˜åˆ¶å‚è€ƒçº¿
        plt.plot(thresholds, treat_none, 'k--', linewidth=2, label='No Model (Treat None)')

        # è®¡ç®—å¹¶ç»˜åˆ¶"å…¨éƒ¨é¢„æµ‹"åŸºçº¿
        accuracy_baseline = []
        for threshold in thresholds:
            # åŸºäºæ•´ä½“å‡†ç¡®ç‡çš„åŸºçº¿
            overall_accuracy = np.mean([result['test_metrics']['Accuracy'] for name, result in top_models])
            if threshold == 0:
                baseline = overall_accuracy
            else:
                baseline = overall_accuracy - threshold / (1 - threshold) if threshold < 1 else 0
            accuracy_baseline.append(baseline)

        plt.plot(thresholds, accuracy_baseline, 'gray', linewidth=2, label='Baseline (Average Accuracy)')

        plt.xlim([0, 1])
        plt.ylim([-5, 1])  # ä¿®æ”¹çºµåæ ‡èŒƒå›´
        plt.xlabel('Confidence Threshold', fontsize=12)
        plt.ylabel('Net Benefit', fontsize=12)
        plt.title(
            'Decision Curve Analysis - Top 5 Models (Multi-class Classification)\n(Based on Prediction Confidence)',
            fontsize=14, weight='bold')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.output_dir / "decision_curves_multiclass_top5.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_calibration_curves(self):
        """ç»˜åˆ¶æ ¡å‡†æ›²çº¿ï¼ˆå¯é æ€§å›¾ï¼‰- æ‰€æœ‰æ¨¡å‹åœ¨ä¸€å¼ å›¾ä¸­"""
        print("Plotting calibration curves...")

        from sklearn.calibration import calibration_curve

        # æ£€æŸ¥æ˜¯å¦ä¸ºäºŒåˆ†ç±»é—®é¢˜
        n_classes = len(np.unique(self.y))

        if n_classes == 2:
            self._plot_binary_calibration_curves()
        else:
            self._plot_multiclass_calibration_curves()

    def _plot_binary_calibration_curves(self):
        """ç»˜åˆ¶äºŒåˆ†ç±»æ ¡å‡†æ›²çº¿ - å‰5ä¸ªæ¨¡å‹åœ¨ä¸€å¼ å›¾ä¸­"""
        from sklearn.calibration import calibration_curve

        plt.figure(figsize=(12, 10))

        # åªé€‰æ‹©å‰5ä¸ªæœ€ä½³æ¨¡å‹
        top_models = sorted(self.results.items(),
                            key=lambda x: x[1]['test_metrics']['Accuracy'],
                            reverse=True)[:5]

        # è®¾ç½®é¢œè‰²å’Œæ ‡è®°
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
        markers = ['o', 's', '^', 'D', 'v']

        # å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„æ ¡å‡†æŒ‡æ ‡
        calibration_metrics = []

        for i, ((name, result), color, marker) in enumerate(zip(top_models, colors, markers)):
            y_test = result['y_test']
            y_proba = result['y_test_proba']

            if y_proba is None:
                continue

            # äºŒåˆ†ç±»æ ¡å‡†æ›²çº¿
            prob_positive = y_proba[:, 1]

            try:
                fraction_of_positives, mean_predicted_value = calibration_curve(
                    y_test, prob_positive, n_bins=10, strategy='uniform'
                )

                # ç»˜åˆ¶æ ¡å‡†æ›²çº¿
                plt.plot(mean_predicted_value, fraction_of_positives,
                         color=color, marker=marker, linewidth=2.5, markersize=8,
                         label=f'{self.compress_column_name(name, 20)}',
                         markerfacecolor='white', markeredgecolor=color, markeredgewidth=2)

                # è®¡ç®—æ ¡å‡†æŒ‡æ ‡ï¼ˆBrier Scoreå’Œå¯é æ€§ï¼‰
                from sklearn.metrics import brier_score_loss
                brier_score = brier_score_loss(y_test, prob_positive)

                # è®¡ç®—æ ¡å‡†è¯¯å·®ï¼ˆExpected Calibration Errorï¼‰
                ece = np.mean(np.abs(fraction_of_positives - mean_predicted_value))

                calibration_metrics.append({
                    'Model': name,
                    'Brier_Score': brier_score,
                    'ECE': ece
                })

            except Exception as e:
                print(f"Error calculating calibration for {name}: {e}")
                continue

        # ç»˜åˆ¶å®Œç¾æ ¡å‡†çº¿
        plt.plot([0, 1], [0, 1], 'k--', linewidth=3, alpha=0.8, label='Perfect Calibration')

        # è®¾ç½®å›¾å½¢å±æ€§
        plt.xlim([0, 1])
        plt.ylim([0, 1])
        plt.xlabel('Mean Predicted Probability', fontsize=14)
        plt.ylabel('Fraction of Positives', fontsize=14)
        plt.title('Calibration Curves - Top 5 Models (Binary Classification)', fontsize=16, weight='bold')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12)
        plt.grid(True, alpha=0.3)

        # æ·»åŠ è¯´æ˜æ–‡æœ¬
        plt.text(0.02, 0.98, 'Perfect calibration: predicted probabilities match observed frequencies',
                 transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',
                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

        plt.tight_layout()
        plt.savefig(self.output_dir / "calibration_curves_top5_combined.png", dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜æ ¡å‡†æŒ‡æ ‡
        if calibration_metrics:
            calibration_df = pd.DataFrame(calibration_metrics)
            calibration_df = calibration_df.sort_values('Brier_Score')
            calibration_df.to_excel(self.output_dir / "calibration_metrics_top5.xlsx", index=False)
            print("Top 5 models calibration metrics saved to calibration_metrics_top5.xlsx")

    def _plot_multiclass_calibration_curves(self):
        """ç»˜åˆ¶å¤šåˆ†ç±»æ ¡å‡†æ›²çº¿ - å‰5ä¸ªæ¨¡å‹åœ¨ä¸€å¼ å›¾ä¸­"""
        from sklearn.calibration import calibration_curve

        n_classes = len(np.unique(self.y))

        # åªé€‰æ‹©å‰5ä¸ªæœ€ä½³æ¨¡å‹
        top_models = sorted(self.results.items(),
                            key=lambda x: x[1]['test_metrics']['Accuracy'],
                            reverse=True)[:5]

        # åˆ›å»ºå­å›¾ï¼šæ¯ä¸ªç±»åˆ«ä¸€ä¸ªå­å›¾
        fig, axes = plt.subplots(1, min(n_classes, 4), figsize=(5 * min(n_classes, 4), 8))
        if n_classes == 1:
            axes = [axes]
        elif n_classes > 4:
            axes = axes[:4] if hasattr(axes, '__len__') else [axes]
            n_classes = min(n_classes, 4)

        # è®¾ç½®é¢œè‰²å’Œæ ‡è®°
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
        markers = ['o', 's', '^', 'D', 'v']

        # å­˜å‚¨æ ¡å‡†æŒ‡æ ‡
        calibration_metrics = []

        for class_idx in range(n_classes):
            ax = axes[class_idx] if n_classes > 1 else axes[0]

            class_metrics = []

            for i, ((name, result), color, marker) in enumerate(zip(top_models, colors, markers)):
                y_test = result['y_test']
                y_proba = result['y_test_proba']

                if y_proba is None or class_idx >= y_proba.shape[1]:
                    continue

                # å°†å¤šåˆ†ç±»è½¬æ¢ä¸ºäºŒåˆ†ç±»ï¼ˆå½“å‰ç±» vs å…¶ä»–ç±»ï¼‰
                y_binary = (y_test == class_idx).astype(int)
                prob_class = y_proba[:, class_idx]

                try:
                    fraction_of_positives, mean_predicted_value = calibration_curve(
                        y_binary, prob_class, n_bins=10, strategy='uniform'
                    )

                    # ç»˜åˆ¶æ ¡å‡†æ›²çº¿
                    ax.plot(mean_predicted_value, fraction_of_positives,
                            color=color, marker=marker, linewidth=2.5, markersize=8,
                            label=f'{self.compress_column_name(name, 15)}',
                            markerfacecolor='white', markeredgecolor=color, markeredgewidth=2)

                    # è®¡ç®—æ ¡å‡†æŒ‡æ ‡
                    from sklearn.metrics import brier_score_loss
                    brier_score = brier_score_loss(y_binary, prob_class)
                    ece = np.mean(np.abs(fraction_of_positives - mean_predicted_value))

                    class_metrics.append({
                        'Model': name,
                        'Class': class_idx,
                        'Brier_Score': brier_score,
                        'ECE': ece
                    })

                except Exception as e:
                    print(f"Error calculating calibration for {name}, class {class_idx}: {e}")
                    continue

            # ç»˜åˆ¶å®Œç¾æ ¡å‡†çº¿
            ax.plot([0, 1], [0, 1], 'k--', linewidth=3, alpha=0.8, label='Perfect')

            # è®¾ç½®å­å›¾å±æ€§
            ax.set_xlim([0, 1])
            ax.set_ylim([0, 1])
            ax.set_xlabel('Mean Predicted Probability', fontsize=12)
            ax.set_ylabel('Fraction of Positives', fontsize=12)
            ax.set_title(f'Class {class_idx} Calibration', fontsize=14, weight='bold')
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)

            calibration_metrics.extend(class_metrics)

        # è®¾ç½®æ€»æ ‡é¢˜
        fig.suptitle('Calibration Curves - Top 5 Models (Multi-class Classification)',
                     fontsize=16, weight='bold', y=1.02)

        plt.tight_layout()
        plt.savefig(self.output_dir / "calibration_curves_top5_combined.png", dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜æ ¡å‡†æŒ‡æ ‡
        if calibration_metrics:
            calibration_df = pd.DataFrame(calibration_metrics)
            calibration_df = calibration_df.sort_values(['Class', 'Brier_Score'])
            calibration_df.to_excel(self.output_dir / "calibration_metrics_top5.xlsx", index=False)
            print("Top 5 models calibration metrics saved to calibration_metrics_top5.xlsx")



    def save_decision_curve_metrics(self):
        """ä¿å­˜å†³ç­–æ›²çº¿ç›¸å…³æŒ‡æ ‡"""
        print("Calculating and saving decision curve metrics...")

        thresholds = np.arange(0.1, 0.9, 0.1)  # å¸¸ç”¨çš„é˜ˆå€¼ç‚¹
        dca_results = []

        for name, result in self.results.items():
            y_test = result['y_test']
            y_proba = result['y_test_proba']

            if y_proba is None:
                continue

            model_dca = {'Model': name}

            for threshold in thresholds:
                net_benefit = self.calculate_net_benefit(y_test, y_proba, threshold)
                model_dca[f'NetBenefit_Threshold_{threshold:.1f}'] = net_benefit

            # è®¡ç®—å†³ç­–æ›²çº¿ä¸‹é¢ç§¯ï¼ˆç±»ä¼¼AUCçš„æ¦‚å¿µï¼‰
            all_thresholds = np.arange(0, 1, 0.01)
            net_benefits = [self.calculate_net_benefit(y_test, y_proba, t) for t in all_thresholds]

            # ä½¿ç”¨æ¢¯å½¢ç§¯åˆ†è®¡ç®—é¢ç§¯
            dca_auc = np.trapz(net_benefits, all_thresholds)
            model_dca['DCA_AUC'] = dca_auc

            # è®¡ç®—æœ€å¤§å‡€æ•ˆç›ŠåŠå…¶å¯¹åº”çš„é˜ˆå€¼
            max_nb_idx = np.argmax(net_benefits)
            model_dca['Max_Net_Benefit'] = net_benefits[max_nb_idx]
            model_dca['Optimal_Threshold'] = all_thresholds[max_nb_idx]

            dca_results.append(model_dca)

        # ä¿å­˜åˆ°Excel
        dca_df = pd.DataFrame(dca_results)
        dca_df = dca_df.sort_values('DCA_AUC', ascending=False)

        dca_df.to_excel(self.output_dir / "decision_curve_metrics.xlsx", index=False)

        print("Decision curve metrics saved successfully")
        return dca_df


    def plot_violin_analysis(self):
        """ç»˜åˆ¶å°æç´å›¾åˆ†æ"""
        print("Generating violin plot analysis...")

        # 1. ç‰¹å¾åˆ†å¸ƒå°æç´å›¾
        self.plot_feature_distribution_violins()

        # 2. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå°æç´å›¾
        self.plot_prediction_probability_violins()

        # 3. æ¨¡å‹ç½®ä¿¡åº¦åˆ†å¸ƒå°æç´å›¾
        self.plot_model_confidence_violins()

        # 4. é‡è¦ç‰¹å¾è¯¦ç»†åˆ†æå°æç´å›¾
        self.plot_important_features_violins()

    def plot_feature_distribution_violins(self):
        """ç»˜åˆ¶é‡è¦ç‰¹å¾åœ¨ä¸åŒç±»åˆ«é—´çš„åˆ†å¸ƒå°æç´å›¾"""
        print("Plotting feature distribution violin plots...")

        # é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼ˆå‰12ä¸ªï¼‰
        if hasattr(self, 'selected_features'):
            # ä½¿ç”¨éšæœºæ£®æ—è·å–ç‰¹å¾é‡è¦æ€§
            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
            rf_model.fit(self.X_selected, self.y)

            feature_importance = rf_model.feature_importances_
            top_indices = np.argsort(feature_importance)[-12:][::-1]
            top_features = [self.selected_features[i] for i in top_indices]
            top_feature_data = self.X_selected[:, top_indices]
        else:
            print("No selected features available")
            return

        # åˆ›å»ºæ•°æ®æ¡†
        feature_df = pd.DataFrame(top_feature_data, columns=[
            self.compress_column_name(name, 20) for name in top_features
        ])
        feature_df['Label'] = self.y
        feature_df['Class'] = [f'Class {label}' for label in self.y]

        # ç»˜åˆ¶å°æç´å›¾
        n_features = len(top_features)
        n_cols = 4
        n_rows = (n_features + n_cols - 1) // n_cols

        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))
        axes = axes.ravel() if n_rows > 1 else [axes] if n_cols == 1 else axes

        # è®¾ç½®é¢œè‰²
        colors = ['lightblue', 'lightcoral', 'lightgreen', 'lightyellow', 'lightpink']

        for i, (feature, compressed_name) in enumerate(zip(top_features, feature_df.columns[:-2])):
            if i >= len(axes):
                break

            ax = axes[i]

            # ç»˜åˆ¶å°æç´å›¾
            violin_parts = ax.violinplot([feature_df[feature_df['Label'] == label][compressed_name].values
                                          for label in np.unique(self.y)],
                                         positions=range(len(np.unique(self.y))),
                                         showmeans=True, showmedians=True, showextrema=True)

            # è®¾ç½®é¢œè‰²
            for pc, color in zip(violin_parts['bodies'], colors[:len(np.unique(self.y))]):
                pc.set_facecolor(color)
                pc.set_alpha(0.7)

            # è®¾ç½®å…¶ä»–å…ƒç´ é¢œè‰²
            violin_parts['cmeans'].set_color('red')
            violin_parts['cmedians'].set_color('black')
            violin_parts['cmaxes'].set_color('gray')
            violin_parts['cmins'].set_color('gray')
            violin_parts['cbars'].set_color('gray')

            # æ·»åŠ æ•£ç‚¹å›¾æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒ
            for j, label in enumerate(np.unique(self.y)):
                data = feature_df[feature_df['Label'] == label][compressed_name].values
                # æ·»åŠ ä¸€äº›éšæœºæŠ–åŠ¨ä»¥é¿å…é‡å 
                x_jitter = np.random.normal(j, 0.05, len(data))
                ax.scatter(x_jitter, data, alpha=0.4, s=20, color='darkblue')

            # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
            ax.set_xlabel('Class', fontsize=10)
            ax.set_ylabel('Feature Value', fontsize=10)
            ax.set_title(f'{compressed_name}', fontsize=11, weight='bold')
            ax.set_xticks(range(len(np.unique(self.y))))
            ax.set_xticklabels([f'Class {label}' for label in np.unique(self.y)])
            ax.grid(True, alpha=0.3)

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            class_means = [feature_df[feature_df['Label'] == label][compressed_name].mean()
                           for label in np.unique(self.y)]
            stats_text = '\n'.join([f'Class {label} Î¼={mean:.3f}'
                                    for label, mean in zip(np.unique(self.y), class_means)])
            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=8,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        # éšè—å¤šä½™çš„å­å›¾
        for i in range(n_features, len(axes)):
            axes[i].set_visible(False)

        plt.suptitle('Feature Distribution Analysis - Violin Plots\n(Top Important Features by Class)',
                     fontsize=16, weight='bold')
        plt.tight_layout()
        plt.savefig(self.output_dir / "feature_distribution_violins.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_prediction_probability_violins(self):
        """ç»˜åˆ¶å„æ¨¡å‹é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒçš„å°æç´å›¾"""
        print("Plotting prediction probability violin plots...")

        # åªé€‰æ‹©å‰8ä¸ªæœ€ä½³æ¨¡å‹
        top_models = sorted(self.results.items(),
                            key=lambda x: x[1]['test_metrics']['Accuracy'],
                            reverse=True)[:8]

        # æ£€æŸ¥æ˜¯å¦ä¸ºäºŒåˆ†ç±»
        n_classes = len(np.unique(self.y))

        if n_classes == 2:
            self._plot_binary_probability_violins(top_models)
        else:
            self._plot_multiclass_probability_violins(top_models)

    def _plot_binary_probability_violins(self, top_models):
        """ç»˜åˆ¶äºŒåˆ†ç±»é¢„æµ‹æ¦‚ç‡å°æç´å›¾"""
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.ravel()

        for idx, (name, result) in enumerate(top_models):
            ax = axes[idx]

            y_test = result['y_test']
            y_proba = result['y_test_proba']

            if y_proba is None:
                ax.text(0.5, 0.5, 'No probability\navailable',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title(name)
                continue

            # è·å–æ­£ç±»æ¦‚ç‡
            prob_positive = y_proba[:, 1]

            # æŒ‰çœŸå®æ ‡ç­¾åˆ†ç»„
            prob_class_0 = prob_positive[y_test == 0]
            prob_class_1 = prob_positive[y_test == 1]

            # ç»˜åˆ¶å°æç´å›¾
            violin_data = [prob_class_0, prob_class_1]
            violin_parts = ax.violinplot(violin_data, positions=[0, 1],
                                         showmeans=True, showmedians=True, showextrema=True)

            # è®¾ç½®é¢œè‰²
            colors = ['lightcoral', 'lightblue']
            for pc, color in zip(violin_parts['bodies'], colors):
                pc.set_facecolor(color)
                pc.set_alpha(0.7)

            # è®¾ç½®å…¶ä»–å…ƒç´ 
            violin_parts['cmeans'].set_color('red')
            violin_parts['cmedians'].set_color('black')
            violin_parts['cmaxes'].set_color('gray')
            violin_parts['cmins'].set_color('gray')
            violin_parts['cbars'].set_color('gray')

            # æ·»åŠ æ•£ç‚¹
            for i, (data, color) in enumerate(zip(violin_data, colors)):
                x_jitter = np.random.normal(i, 0.05, len(data))
                ax.scatter(x_jitter, data, alpha=0.4, s=15, color='darkred' if i == 0 else 'darkblue')

            # æ·»åŠ é˜ˆå€¼çº¿
            ax.axhline(y=0.5, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Threshold=0.5')
            ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.0), fontsize=8, ncol=1, fancybox=True, shadow=True)
            # è®¾ç½®æ ‡ç­¾
            ax.set_xlabel('True Class', fontsize=10)
            ax.set_ylabel('Predicted Probability', fontsize=10)
            ax.set_title(f'{self.compress_column_name(name, 25)}', fontsize=11, weight='bold')
            ax.set_xticks([0, 1])
            ax.set_xticklabels(['Class 0', 'Class 1'])
            ax.set_ylim([0, 1])
            ax.grid(True, alpha=0.3)


            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            auc_score = result['test_metrics'].get('AUC', 0)
            accuracy = result['test_metrics']['Accuracy']
            stats_text = f'AUC: {auc_score:.3f}\nAcc: {accuracy:.3f}'
            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        plt.suptitle('Prediction Probability Distribution - Violin Plots\n(Binary Classification)',
                     fontsize=16, weight='bold')
        plt.tight_layout()
        plt.savefig(self.output_dir / "prediction_probability_violins.png", dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_multiclass_probability_violins(self, top_models):
        """ç»˜åˆ¶å¤šåˆ†ç±»é¢„æµ‹æ¦‚ç‡å°æç´å›¾"""
        n_classes = len(np.unique(self.y))

        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.ravel()

        for idx, (name, result) in enumerate(top_models):
            ax = axes[idx]

            y_test = result['y_test']
            y_proba = result['y_test_proba']

            if y_proba is None:
                ax.text(0.5, 0.5, 'No probability\navailable',
                        ha='center', va='center', transform=ax.transAxes)
                ax.set_title(name)
                continue

            # è·å–æœ€å¤§æ¦‚ç‡ï¼ˆç½®ä¿¡åº¦ï¼‰
            max_proba = np.max(y_proba, axis=1)
            predicted_class = np.argmax(y_proba, axis=1)

            # æŒ‰é¢„æµ‹ç±»åˆ«åˆ†ç»„
            violin_data = []
            colors = plt.cm.Set3(np.linspace(0, 1, n_classes))

            for class_label in range(n_classes):
                class_probabilities = max_proba[predicted_class == class_label]
                if len(class_probabilities) > 0:
                    violin_data.append(class_probabilities)
                else:
                    violin_data.append([0])  # ç©ºæ•°æ®çš„å ä½ç¬¦

            # ç»˜åˆ¶å°æç´å›¾
            if any(len(data) > 1 for data in violin_data):
                violin_parts = ax.violinplot([data for data in violin_data if len(data) > 1],
                                             positions=range(len(violin_data)),
                                             showmeans=True, showmedians=True, showextrema=True)

                # è®¾ç½®é¢œè‰²
                for pc, color in zip(violin_parts['bodies'], colors):
                    pc.set_facecolor(color)
                    pc.set_alpha(0.7)

                # è®¾ç½®å…¶ä»–å…ƒç´ 
                violin_parts['cmeans'].set_color('red')
                violin_parts['cmedians'].set_color('black')
                violin_parts['cmaxes'].set_color('gray')
                violin_parts['cmins'].set_color('gray')
                violin_parts['cbars'].set_color('gray')

            # æ·»åŠ æ•£ç‚¹
            for i, (data, color) in enumerate(zip(violin_data, colors)):
                if len(data) > 1:
                    x_jitter = np.random.normal(i, 0.05, len(data))
                    ax.scatter(x_jitter, data, alpha=0.4, s=15, color=color)

            # è®¾ç½®æ ‡ç­¾
            ax.set_xlabel('Predicted Class', fontsize=10)
            ax.set_ylabel('Maximum Probability (Confidence)', fontsize=10)
            ax.set_title(f'{self.compress_column_name(name, 25)}', fontsize=11, weight='bold')
            ax.set_xticks(range(n_classes))
            ax.set_xticklabels([f'Class {i}' for i in range(n_classes)])
            ax.set_ylim([0, 1])
            ax.grid(True, alpha=0.3)

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            accuracy = result['test_metrics']['Accuracy']
            mean_confidence = np.mean(max_proba)
            stats_text = f'Acc: {accuracy:.3f}\nMean Conf: {mean_confidence:.3f}'
            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,
                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        plt.suptitle('Prediction Confidence Distribution - Violin Plots\n(Multi-class Classification)',
                     fontsize=16, weight='bold')
        plt.tight_layout()
        plt.savefig(self.output_dir / "prediction_confidence_violins.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_model_confidence_violins(self):
        """ç»˜åˆ¶æ¨¡å‹ç½®ä¿¡åº¦æ¯”è¾ƒå°æç´å›¾"""
        print("Plotting model confidence comparison violin plots...")

        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„ç½®ä¿¡åº¦æ•°æ®
        model_confidences = {}
        model_accuracies = {}

        for name, result in self.results.items():
            y_proba = result['y_test_proba']
            if y_proba is not None:
                # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆæœ€å¤§æ¦‚ç‡ï¼‰
                confidence = np.max(y_proba, axis=1)
                model_confidences[self.compress_column_name(name, 15)] = confidence
                model_accuracies[name] = result['test_metrics']['Accuracy']

        if not model_confidences:
            print("No probability predictions available for confidence analysis")
            return

        # æŒ‰å‡†ç¡®ç‡æ’åº
        sorted_models = sorted(model_confidences.items(),
                               key=lambda x: model_accuracies.get(x[0], 0), reverse=True)

        # ç»˜åˆ¶ç½®ä¿¡åº¦æ¯”è¾ƒå›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))

        # 1. æ‰€æœ‰æ¨¡å‹ç½®ä¿¡åº¦åˆ†å¸ƒæ¯”è¾ƒ
        model_names = [item[0] for item in sorted_models]
        confidence_data = [item[1] for item in sorted_models]

        violin_parts = ax1.violinplot(confidence_data, positions=range(len(model_names)),
                                      showmeans=True, showmedians=True, showextrema=True)

        # è®¾ç½®é¢œè‰²æ¸å˜
        colors = plt.cm.viridis(np.linspace(0, 1, len(model_names)))
        for pc, color in zip(violin_parts['bodies'], colors):
            pc.set_facecolor(color)
            pc.set_alpha(0.7)

        # è®¾ç½®å…¶ä»–å…ƒç´ 
        violin_parts['cmeans'].set_color('red')
        violin_parts['cmedians'].set_color('white')
        violin_parts['cmaxes'].set_color('gray')
        violin_parts['cmins'].set_color('gray')
        violin_parts['cbars'].set_color('gray')

        ax1.set_xlabel('Models (sorted by accuracy)', fontsize=12)
        ax1.set_ylabel('Prediction Confidence', fontsize=12)
        ax1.set_title('Model Confidence Distribution Comparison', fontsize=14, weight='bold')
        ax1.set_xticks(range(len(model_names)))
        ax1.set_xticklabels(model_names, rotation=45, ha='right')
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim([0, 1])

        # 2. ç½®ä¿¡åº¦ vs å‡†ç¡®ç‡æ•£ç‚¹å›¾
        mean_confidences = [np.mean(data) for data in confidence_data]
        accuracies = [model_accuracies.get(name, 0) for name in model_names]

        scatter = ax2.scatter(mean_confidences, accuracies, c=range(len(model_names)),
                              cmap='viridis', s=100, alpha=0.7, edgecolors='black')

        # æ·»åŠ æ¨¡å‹åç§°æ ‡ç­¾
        for i, name in enumerate(model_names):
            ax2.annotate(name, (mean_confidences[i], accuracies[i]),
                         xytext=(5, 5), textcoords='offset points', fontsize=8)

        # æ·»åŠ è¶‹åŠ¿çº¿
        z = np.polyfit(mean_confidences, accuracies, 1)
        p = np.poly1d(z)
        ax2.plot(mean_confidences, p(mean_confidences), "r--", alpha=0.8, linewidth=2)

        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(mean_confidences, accuracies)[0, 1]

        ax2.set_xlabel('Mean Prediction Confidence', fontsize=12)
        ax2.set_ylabel('Test Accuracy', fontsize=12)
        ax2.set_title(f'Confidence vs Accuracy\n(Correlation: {correlation:.3f})', fontsize=14, weight='bold')
        ax2.grid(True, alpha=0.3)

        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(scatter, ax=ax2)
        cbar.set_label('Model Rank (by accuracy)', fontsize=10)

        plt.tight_layout()
        plt.savefig(self.output_dir / "model_confidence_violins.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_important_features_violins(self):
        """ç»˜åˆ¶æœ€é‡è¦ç‰¹å¾çš„è¯¦ç»†å°æç´å›¾åˆ†æ"""
        print("Plotting detailed important features violin analysis...")

        # è·å–æœ€é‡è¦çš„6ä¸ªç‰¹å¾
        if hasattr(self, 'selected_features'):
            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
            rf_model.fit(self.X_selected, self.y)

            feature_importance = rf_model.feature_importances_
            top_indices = np.argsort(feature_importance)[-6:][::-1]
            top_features = [self.selected_features[i] for i in top_indices]
            top_feature_data = self.X_selected[:, top_indices]
        else:
            print("No selected features available")
            return

        # åˆ›å»ºè¯¦ç»†çš„å°æç´å›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()

        # è®¡ç®—ç»Ÿè®¡æ£€éªŒï¼ˆANOVAæˆ–t-testï¼‰
        from scipy import stats

        for i, (feature_name, feature_idx) in enumerate(zip(top_features, range(len(top_features)))):
            ax = axes[i]
            feature_data = top_feature_data[:, feature_idx]

            # æŒ‰ç±»åˆ«åˆ†ç»„æ•°æ®
            groups = [feature_data[self.y == label] for label in np.unique(self.y)]

            # ç»˜åˆ¶å¢å¼ºçš„å°æç´å›¾
            violin_parts = ax.violinplot(groups, positions=range(len(np.unique(self.y))),
                                         showmeans=True, showmedians=True, showextrema=True)

            # è®¾ç½®é¢œè‰²
            colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC']
            for pc, color in zip(violin_parts['bodies'], colors[:len(np.unique(self.y))]):
                pc.set_facecolor(color)
                pc.set_alpha(0.8)

            # è®¾ç½®å°æç´å›¾å…ƒç´ æ ·å¼
            violin_parts['cmeans'].set_color('red')
            violin_parts['cmeans'].set_linewidth(2)
            violin_parts['cmedians'].set_color('black')
            violin_parts['cmedians'].set_linewidth(2)
            violin_parts['cmaxes'].set_color('gray')
            violin_parts['cmins'].set_color('gray')
            violin_parts['cbars'].set_color('gray')

            # æ·»åŠ ç®±çº¿å›¾å…ƒç´ 
            bp = ax.boxplot(groups, positions=range(len(np.unique(self.y))),
                            widths=0.3, patch_artist=False,
                            boxprops=dict(color='black', linewidth=1),
                            whiskerprops=dict(color='black', linewidth=1),
                            capprops=dict(color='black', linewidth=1),
                            medianprops=dict(color='red', linewidth=2),
                            flierprops=dict(marker='o', markerfacecolor='red', markersize=5, alpha=0.5))

            # æ·»åŠ æ•°æ®ç‚¹ï¼ˆå¸¦æŠ–åŠ¨ï¼‰
            for j, group in enumerate(groups):
                x_jitter = np.random.normal(j, 0.04, len(group))
                ax.scatter(x_jitter, group, alpha=0.6, s=20,
                           color=colors[j % len(colors)], edgecolors='black', linewidth=0.5)

            # ç»Ÿè®¡æ£€éªŒ
            if len(np.unique(self.y)) == 2:
                # tæ£€éªŒ
                statistic, p_value = stats.ttest_ind(groups[0], groups[1])
                test_name = "t-test"
            else:
                # ANOVA
                statistic, p_value = stats.f_oneway(*groups)
                test_name = "ANOVA"

            # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
            compressed_name = self.compress_column_name(feature_name, 25)
            ax.set_xlabel('Class', fontsize=11)
            ax.set_ylabel('Feature Value', fontsize=11)
            ax.set_title(f'{compressed_name}\n{test_name} p-value: {p_value:.2e}',
                         fontsize=12, weight='bold')
            ax.set_xticks(range(len(np.unique(self.y))))
            ax.set_xticklabels([f'Class {label}' for label in np.unique(self.y)])
            ax.grid(True, alpha=0.3)

            # æ·»åŠ ç»Ÿè®¡æ‘˜è¦
            stats_lines = []
            for j, (label, group) in enumerate(zip(np.unique(self.y), groups)):
                mean_val = np.mean(group)
                std_val = np.std(group)
                stats_lines.append(f'Class {label}: Î¼={mean_val:.3f}, Ïƒ={std_val:.3f}')

            effect_size = abs(statistic) if len(np.unique(self.y)) == 2 else statistic
            stats_lines.append(f'Effect size: {effect_size:.3f}')

            stats_text = '\n'.join(stats_lines)
            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, fontsize=9,
                    verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='gray'))

            # # æ·»åŠ æ˜¾è‘—æ€§æ ‡è®°
            # if p_value < 0.001:
            #     sig_text = "***"
            # elif p_value < 0.01:
            #     sig_text = "**"
            # elif p_value < 0.05:
            #     sig_text = "*"
            # else:
            #     sig_text = "ns"
            #
            # ax.text(0.98, 0.98, sig_text, transform=ax.transAxes, fontsize=16,
            #         verticalalignment='top', horizontalalignment='right',
            #         weight='bold', color='red' if p_value < 0.05 else 'gray')

        plt.suptitle(
            'Detailed Analysis of Most Important Features\n(Violin + Box + Scatter Plots with Statistical Tests)',
            fontsize=16, weight='bold')
        plt.tight_layout()
        plt.savefig(self.output_dir / "important_features_detailed_violins.png", dpi=300, bbox_inches='tight')
        plt.close()

    def save_violin_analysis_summary(self):
        """ä¿å­˜å°æç´å›¾åˆ†ææ‘˜è¦"""
        print("Saving violin analysis summary...")

        summary_data = []

        # ç‰¹å¾åˆ†å¸ƒåˆ†ææ‘˜è¦
        if hasattr(self, 'selected_features'):
            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
            rf_model.fit(self.X_selected, self.y)

            feature_importance = rf_model.feature_importances_
            top_indices = np.argsort(feature_importance)[-10:][::-1]

            from scipy import stats

            for i, feature_idx in enumerate(top_indices):
                feature_name = self.selected_features[feature_idx]
                feature_data = self.X_selected[:, feature_idx]

                # æŒ‰ç±»åˆ«åˆ†ç»„
                groups = [feature_data[self.y == label] for label in np.unique(self.y)]

                # ç»Ÿè®¡æ£€éªŒ
                if len(np.unique(self.y)) == 2:
                    statistic, p_value = stats.ttest_ind(groups[0], groups[1])
                    test_type = "t-test"
                else:
                    statistic, p_value = stats.f_oneway(*groups)
                    test_type = "ANOVA"

                # è®¡ç®—æ•ˆåº”é‡
                class_means = [np.mean(group) for group in groups]
                class_stds = [np.std(group) for group in groups]

                summary_data.append({
                    'Feature': feature_name,
                    'Importance_Rank': i + 1,
                    'Feature_Importance': feature_importance[feature_idx],
                    'Test_Type': test_type,
                    'Test_Statistic': statistic,
                    'P_Value': p_value,
                    'Significant': p_value < 0.05,
                    'Class_0_Mean': class_means[0] if len(class_means) > 0 else None,
                    'Class_0_Std': class_stds[0] if len(class_stds) > 0 else None,
                    'Class_1_Mean': class_means[1] if len(class_means) > 1 else None,
                    'Class_1_Std': class_stds[1] if len(class_stds) > 1 else None,
                })

        # ä¿å­˜æ‘˜è¦
        if summary_data:
            summary_df = pd.DataFrame(summary_data)
            summary_df = summary_df.sort_values('Feature_Importance', ascending=False)
            summary_df.to_excel(self.output_dir / "violin_analysis_summary.xlsx", index=False)
            print("Violin analysis summary saved to violin_analysis_summary.xlsx")

        return summary_data

    def plot_roc_curves(self):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        print("Plotting ROC curves...")

        n_classes = len(np.unique(self.y))

        # ä¸ºå¤šåˆ†ç±»é—®é¢˜ç»˜åˆ¶ROCæ›²çº¿
        if n_classes > 2:
            self.plot_multiclass_roc()
        else:
            self.plot_binary_roc()

    def plot_multiclass_roc(self):
        """ç»˜åˆ¶å¤šåˆ†ç±»ROCæ›²çº¿"""
        n_classes = len(np.unique(self.y))
        class_names = [f'Class {i}' for i in range(n_classes)]

        # è®¾ç½®é¢œè‰²
        colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()

        for idx, (name, result) in enumerate(self.results.items()):
            if idx >= 6:  # åªæ˜¾ç¤ºå‰6ä¸ªæ¨¡å‹
                break

            ax = axes[idx]

            y_test = result['y_test']
            y_proba = result['y_test_proba']

            # äºŒå€¼åŒ–æ ‡ç­¾
            y_test_bin = np.zeros((len(y_test), n_classes))
            for i, label in enumerate(y_test):
                if label < n_classes:
                    y_test_bin[i, label] = 1

            # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ROC
            fpr = dict()
            tpr = dict()
            roc_auc = dict()

            for i in range(n_classes):
                if i < y_proba.shape[1]:
                    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
                    roc_auc[i] = auc(fpr[i], tpr[i])

            # ç»˜åˆ¶ROCæ›²çº¿
            for i, color in zip(range(n_classes), colors):
                if i in fpr:
                    ax.plot(fpr[i], tpr[i], color=color, lw=2,
                            label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')

            ax.plot([0, 1], [0, 1], 'k--', lw=2)
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(f'ROC Curve - {name}')
            ax.legend(loc="lower right", fontsize=8)
            ax.grid(True, alpha=0.3)

        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(len(self.results), len(axes)):
            axes[idx].set_visible(False)

        plt.tight_layout()
        plt.savefig(self.output_dir / "roc_curves_multiclass.png", dpi=300, bbox_inches='tight')
        plt.close()

        # ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„å¹³å‡ROC
        self.plot_average_roc_comparison()

    def plot_binary_roc(self):
        """ç»˜åˆ¶äºŒåˆ†ç±»ROCæ›²çº¿"""
        plt.figure(figsize=(12, 10))

        colors = plt.cm.Set3(np.linspace(0, 1, len(self.results)))

        for (name, result), color in zip(self.results.items(), colors):
            y_test = result['y_test']
            y_proba = result['y_test_proba']

            fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])
            roc_auc = auc(fpr, tpr)

            plt.plot(fpr, tpr, color=color, lw=2,
                     label=f'{name} (AUC = {roc_auc:.3f})')

        plt.plot([0, 1], [0, 1], 'k--', lw=2)
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title('ROC Curves Comparison - All Models', fontsize=14, weight='bold')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.output_dir / "roc_curves_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

    def plot_average_roc_comparison(self):
        """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„ROCæ›²çº¿æ¯”è¾ƒ"""
        plt.figure(figsize=(12, 10))

        colors = plt.cm.tab20(np.linspace(0, 1, len(self.results)))

        for (name, result), color in zip(self.results.items(), colors):
            y_test = result['y_test']
            y_proba = result['y_test_proba']

            # è®¡ç®—macro-average ROC
            n_classes = len(np.unique(y_test))
            y_test_bin = np.zeros((len(y_test), n_classes))
            for i, label in enumerate(y_test):
                if label < n_classes:
                    y_test_bin[i, label] = 1

            # è®¡ç®—macro-average
            all_fpr = np.unique(np.concatenate([
                roc_curve(y_test_bin[:, i], y_proba[:, i])[0]
                for i in range(min(n_classes, y_proba.shape[1]))
            ]))

            mean_tpr = np.zeros_like(all_fpr)
            for i in range(min(n_classes, y_proba.shape[1])):
                fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
                mean_tpr += np.interp(all_fpr, fpr, tpr)

            mean_tpr /= min(n_classes, y_proba.shape[1])
            macro_auc = auc(all_fpr, mean_tpr)

            plt.plot(all_fpr, mean_tpr, color=color, lw=2,
                     label=f'{name} (Macro-AUC = {macro_auc:.3f})')

        plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title('Macro-Average ROC Curves Comparison', fontsize=14, weight='bold')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(self.output_dir / "macro_roc_curves_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

    def save_performance_metrics(self):
        """ä¿å­˜æ€§èƒ½æŒ‡æ ‡åˆ°Excel"""
        print("Saving performance metrics...")

        # è®­ç»ƒé›†æ€§èƒ½
        train_metrics_df = pd.DataFrame({
            name: result['train_metrics']
            for name, result in self.results.items()
        }).T

        # æµ‹è¯•é›†æ€§èƒ½
        test_metrics_df = pd.DataFrame({
            name: result['test_metrics']
            for name, result in self.results.items()
        }).T

        # äº¤å‰éªŒè¯ç»“æœ
        cv_metrics_df = pd.DataFrame(self.cv_results).T

        # ä¿å­˜åˆ°Excel
        with pd.ExcelWriter(self.output_dir / "model_performance_metrics.xlsx",
                            engine='openpyxl') as writer:
            train_metrics_df.to_excel(writer, sheet_name='Training_Set', index=True)
            test_metrics_df.to_excel(writer, sheet_name='Test_Set', index=True)
            cv_metrics_df.to_excel(writer, sheet_name='Cross_Validation', index=True)

        print("Performance metrics saved successfully")

        # è¿”å›æœ€ä½³æ¨¡å‹
        best_model_name = test_metrics_df['Accuracy'].idxmax()
        print(f"Best performing model: {best_model_name}")
        print(f"Best test accuracy: {test_metrics_df.loc[best_model_name, 'Accuracy']:.4f}")

        return best_model_name

    def plot_performance_comparison(self):
        """ç»˜åˆ¶æ€§èƒ½æ¯”è¾ƒå›¾"""
        print("Plotting performance comparison...")

        # å‡†å¤‡æ•°æ®
        metrics_data = []
        for name, result in self.results.items():
            test_metrics = result['test_metrics']
            metrics_data.append({
                'Model': name,
                'Accuracy': test_metrics['Accuracy'],
                'Precision': test_metrics['Precision'],
                'Recall': test_metrics['Recall'],
                'F1-Score': test_metrics['F1-Score'],
                'Specificity': test_metrics['Specificity']
            })

        metrics_df = pd.DataFrame(metrics_data)

        # ç»˜åˆ¶æ¡å½¢å›¾
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.ravel()

        metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']

        for i, metric in enumerate(metrics_to_plot):
            ax = axes[i]

            # æ’åº
            sorted_data = metrics_df.sort_values(metric, ascending=True)

            bars = ax.barh(range(len(sorted_data)), sorted_data[metric])
            ax.set_yticks(range(len(sorted_data)))
            ax.set_yticklabels([self.compress_column_name(name, 15) for name in sorted_data['Model']])
            ax.set_xlabel(metric)
            ax.set_title(f'{metric} Comparison')
            ax.grid(True, alpha=0.3)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, bar in enumerate(bars):
                width = bar.get_width()
                ax.text(width + 0.01, bar.get_y() + bar.get_height() / 2,
                        f'{width:.3f}', ha='left', va='center', fontsize=8)

        # éšè—æœ€åä¸€ä¸ªå­å›¾
        axes[-1].set_visible(False)

        plt.tight_layout()
        plt.savefig(self.output_dir / "performance_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

    def shap_analysis(self, best_model_name):
        """SHAPå¯è§£é‡Šæ€§åˆ†æ"""
        print(f"Performing SHAP analysis for {best_model_name}...")

        try:
            best_model = self.results[best_model_name]['model']

            # å‡†å¤‡æ•°æ®æ ·æœ¬ï¼ˆä½¿ç”¨è¾ƒå°çš„æ ·æœ¬ä»¥åŠ é€Ÿè®¡ç®—ï¼‰
            sample_size = min(100, len(self.X_selected))
            sample_indices = np.random.choice(len(self.X_selected), sample_size, replace=False)
            X_sample = self.X_selected[sample_indices]

            # å‹ç¼©ç‰¹å¾å
            compressed_feature_names = [
                self.compress_column_name(name, 15) for name in self.selected_features
            ]

            # é€‰æ‹©åˆé€‚çš„SHAP explainer
            if hasattr(best_model, 'predict_proba'):
                try:
                    # å°è¯•ä½¿ç”¨TreeExplainerï¼ˆé€‚ç”¨äºæ ‘æ¨¡å‹ï¼‰
                    if any(model_type in best_model_name.lower()
                           for model_type in ['forest', 'tree', 'boost', 'xgb']):
                        explainer = shap.TreeExplainer(best_model)
                        shap_values = explainer.shap_values(X_sample)
                    else:
                        # ä½¿ç”¨KernelExplainerï¼ˆé€šç”¨ä½†è¾ƒæ…¢ï¼‰
                        background = shap.sample(self.X_selected, 50)
                        explainer = shap.KernelExplainer(best_model.predict_proba, background)
                        shap_values = explainer.shap_values(X_sample)

                except Exception as e:
                    print(f"TreeExplainer failed, using KernelExplainer: {e}")
                    background = shap.sample(self.X_selected, 50)
                    explainer = shap.KernelExplainer(best_model.predict_proba, background)
                    shap_values = explainer.shap_values(X_sample)
            else:
                print("Model does not support probability prediction, skipping SHAP analysis")
                return

            # ç»˜åˆ¶SHAPå›¾
            self.plot_shap_analysis(shap_values, X_sample, compressed_feature_names, best_model_name, explainer)

        except Exception as e:
            print(f"Error in SHAP analysis: {e}")
            print("SHAP analysis requires additional setup. Please install shap package properly.")

    def plot_shap_analysis(self, shap_values, X_sample, feature_names, model_name, explainer):
        """ç»˜åˆ¶SHAPåˆ†æå›¾"""
        try:
            # å¦‚æœæ˜¯å¤šåˆ†ç±»ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç±»åˆ«çš„SHAPå€¼
            if isinstance(shap_values, list):
                shap_values_plot = shap_values[0]
                class_name = "Class 0"
            else:
                shap_values_plot = shap_values
                class_name = "Target"

            # 1. Summary plot
            plt.figure(figsize=(12, 8))
            shap.summary_plot(shap_values_plot, X_sample, feature_names=feature_names,
                              show=False, max_display=20)
            plt.title(f'SHAP Summary Plot - {model_name} ({class_name})', fontsize=14, weight='bold')
            plt.tight_layout()
            plt.savefig(self.output_dir / f"shap_summary_{model_name.replace(' ', '_')}.png",
                        dpi=300, bbox_inches='tight')
            plt.close()

            # 2. Bar plot
            plt.figure(figsize=(10, 8))
            shap.summary_plot(shap_values_plot, X_sample, feature_names=feature_names,
                              plot_type="bar", show=False, max_display=20)
            plt.title(f'SHAP Feature Importance - {model_name} ({class_name})', fontsize=14, weight='bold')
            plt.tight_layout()
            plt.savefig(self.output_dir / f"shap_importance_{model_name.replace(' ', '_')}.png",
                        dpi=300, bbox_inches='tight')
            plt.close()

            # 3. Waterfall plot for first sample
            if len(X_sample) > 0:
                plt.figure(figsize=(10, 8))
                shap.waterfall_plot(
                    shap.Explanation(values=shap_values_plot[0],
                                     base_values=explainer.expected_value[0] if isinstance(explainer.expected_value,
                                                                                           np.ndarray) else explainer.expected_value,
                                     feature_names=feature_names),
                    show=False, max_display=15
                )
                plt.title(f'SHAP Waterfall Plot - {model_name} (Sample 1, {class_name})', fontsize=14, weight='bold')
                plt.tight_layout()
                plt.savefig(self.output_dir / f"shap_waterfall_{model_name.replace(' ', '_')}.png",
                            dpi=300, bbox_inches='tight')
                plt.close()

            print(f"SHAP analysis plots saved for {model_name}")

        except Exception as e:
            print(f"Error plotting SHAP analysis: {e}")

    def plot_edge_features_effect_comparison(self):
        """ç»˜åˆ¶è¾¹ç¼˜ç‰¹å¾æ·»åŠ æ•ˆæœå¯¹æ¯”å›¾ - å‰5ä¸ªæ¨¡å‹çš„ä¸åŒé¢œè‰²æŸ±çŠ¶å›¾"""
        print("Plotting edge features addition effect comparison...")

        # åªé€‰æ‹©å‰5ä¸ªæœ€ä½³æ¨¡å‹
        top_models = sorted(self.results.items(),
                            key=lambda x: x[1]['test_metrics']['Accuracy'],
                            reverse=True)[:5]

        # è®¾ç½®é¢œè‰²
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']

        # æ¨¡æ‹Ÿæ²¡æœ‰è¾¹ç¼˜ç‰¹å¾çš„æ€§èƒ½ï¼ˆä»…ä½¿ç”¨è¶…å£°ç‰¹å¾ï¼‰
        us_only_performance = {}

        # æå–è¶…å£°ç‰¹å¾ç´¢å¼•
        us_indices = [i for i, col in enumerate(self.feature_names) if col in self.discrete_cols]
        X_us_only = self.X_scaled[:, us_indices]

        # å¯¹å‰5ä¸ªæ¨¡å‹è®¡ç®—ä»…ä½¿ç”¨è¶…å£°ç‰¹å¾çš„æ€§èƒ½
        for name, result in top_models:
            try:
                model = clone(self.models[name])
                X_train, X_test, y_train, y_test = train_test_split(
                    X_us_only, self.y, test_size=0.2, random_state=42, stratify=self.y
                )
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                us_only_acc = accuracy_score(y_test, y_pred)
                us_only_performance[name] = us_only_acc
            except:
                # å¦‚æœæŸäº›æ¨¡å‹ä¸æ”¯æŒï¼Œä½¿ç”¨é»˜è®¤å€¼
                us_only_performance[name] = result['test_metrics']['Accuracy'] - 0.05

        # å‡†å¤‡æ•°æ®
        model_names = [name for name, _ in top_models]
        compressed_names = [self.compress_column_name(name, 20) for name in model_names]

        us_only_accs = [us_only_performance[name] for name in model_names]
        full_feature_accs = [result['test_metrics']['Accuracy'] for _, result in top_models]
        improvements = [full - us for full, us in zip(full_feature_accs, us_only_accs)]

        # åˆ›å»ºå›¾å½¢
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))

        # 1. æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
        x = np.arange(len(model_names))
        width = 0.35

        bars1 = ax1.bar(x - width / 2, us_only_accs, width,
                        label='Ultrasound Features Only',
                        color='lightgray', alpha=0.8, edgecolor='black')
        bars2 = ax1.bar(x + width / 2, full_feature_accs, width,
                        label='Ultrasound + Edge Features',
                        color=colors, alpha=0.8, edgecolor='black')

        ax1.set_xlabel('Models', fontsize=12)
        ax1.set_ylabel('Accuracy', fontsize=12)
        ax1.set_title('Performance Comparison: With vs Without Edge Features', fontsize=14, weight='bold')
        ax1.set_xticks(x)
        ax1.set_xticklabels(compressed_names, rotation=45, ha='right')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim([0, 1])

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars1:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                     f'{height:.3f}', ha='center', va='bottom', fontsize=9)

        for bar in bars2:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                     f'{height:.3f}', ha='center', va='bottom', fontsize=9)

        # 2. è¾¹ç¼˜ç‰¹å¾æ•ˆæœæ”¹å–„å›¾
        bars3 = ax2.bar(x, improvements, color=colors, alpha=0.8, edgecolor='black')

        ax2.set_xlabel('Models', fontsize=12)
        ax2.set_ylabel('Accuracy Improvement', fontsize=12)
        ax2.set_title('Edge Features Addition Effect', fontsize=14, weight='bold')
        ax2.set_xticks(x)
        ax2.set_xticklabels(compressed_names, rotation=45, ha='right')
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)

        # æ·»åŠ æ”¹å–„æ•°å€¼æ ‡ç­¾
        for i, (bar, improvement) in enumerate(zip(bars3, improvements)):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width() / 2., height + 0.002 if height >= 0 else height - 0.005,
                     f'{improvement:+.3f}\n({improvement * 100:+.1f}%)',
                     ha='center', va='bottom' if height >= 0 else 'top',
                     fontsize=9, weight='bold',
                     color='green' if improvement > 0 else 'red')

        # æ·»åŠ å¹³å‡æ”¹å–„çº¿
        avg_improvement = np.mean(improvements)
        ax2.axhline(y=avg_improvement, color='red', linestyle='--', linewidth=2, alpha=0.7,
                    label=f'Average: {avg_improvement:+.3f}')
        ax2.legend()

        plt.tight_layout()
        plt.savefig(self.output_dir / "edge_features_effect_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

        # ä¿å­˜è¯¦ç»†æ•°æ®
        comparison_data = pd.DataFrame({
            'Model': model_names,
            'Ultrasound_Only_Accuracy': us_only_accs,
            'Full_Features_Accuracy': full_feature_accs,
            'Improvement': improvements,
            'Improvement_Percentage': [imp * 100 for imp in improvements]
        })
        comparison_data.to_excel(self.output_dir / "edge_features_effect_analysis.xlsx", index=False)

        print("Edge features effect comparison plot and analysis saved successfully!")

    def plot_improved_data_balancing_analysis(self):
        """æ”¹è¿›çš„æ•°æ®å¹³è¡¡æ•ˆæœåˆ†æå›¾"""
        print("Plotting improved data balancing analysis...")
        import numpy as np
        if not hasattr(self, 'balancing_info'):
            print("No balancing information available")
            return

        fig = plt.figure(figsize=(20, 12))
        gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)

        original_dist = self.balancing_info['original_distribution']
        balanced_dist = self.balancing_info['balanced_distribution']
        classes = list(original_dist.keys())

        # 1. é¥¼å›¾å¯¹æ¯” (ä¸Šæ’å·¦ä¾§ä¸¤ä¸ª)
        ax1 = fig.add_subplot(gs[0, 0])
        colors_pie = ['lightcoral', 'lightblue', 'lightgreen', 'lightyellow']
        wedges1, texts1, autotexts1 = ax1.pie([original_dist[c] for c in classes],
                                              labels=[f'Class {c}' for c in classes],
                                              colors=colors_pie[:len(classes)],
                                              autopct='%1.1f%%', startangle=90)
        ax1.set_title('Original Distribution', fontsize=14, weight='bold')

        ax2 = fig.add_subplot(gs[0, 1])
        wedges2, texts2, autotexts2 = ax2.pie([balanced_dist[c] for c in classes],
                                              labels=[f'Class {c}' for c in classes],
                                              colors=colors_pie[:len(classes)],
                                              autopct='%1.1f%%', startangle=90)
        ax2.set_title('Balanced Distribution', fontsize=14, weight='bold')

        # 2. ç€‘å¸ƒå›¾æ˜¾ç¤ºæ ·æœ¬æ•°å˜åŒ– (ä¸Šæ’å³ä¾§ä¸¤ä¸ª)
        ax3 = fig.add_subplot(gs[0, 2:])

        # åˆ›å»ºç€‘å¸ƒå›¾æ•°æ®
        original_counts = [original_dist[c] for c in classes]
        balanced_counts = [balanced_dist[c] for c in classes]
        changes = [balanced_counts[i] - original_counts[i] for i in range(len(classes))]

        # ç»˜åˆ¶ç€‘å¸ƒå›¾
        x_pos = np.arange(len(classes))
        bottom_original = np.zeros(len(classes))
        bottom_change = original_counts.copy()

        # åŸå§‹æ•°æ®
        bars_orig = ax3.bar(x_pos - 0.2, original_counts, 0.4,
                            label='Original', color='lightcoral', alpha=0.7)

        # å˜åŒ–é‡
        for i, change in enumerate(changes):
            if change > 0:  # å¢åŠ 
                ax3.bar(x_pos[i] + 0.2, change, 0.4, bottom=original_counts[i],
                        color='lightgreen', alpha=0.7, label='Added' if i == 0 else "")
                ax3.bar(x_pos[i] + 0.2, original_counts[i], 0.4,
                        color='lightcoral', alpha=0.7)
            else:  # å‡å°‘
                ax3.bar(x_pos[i] + 0.2, balanced_counts[i], 0.4,
                        color='lightcoral', alpha=0.7)
                ax3.bar(x_pos[i] + 0.2, abs(change), 0.4, bottom=balanced_counts[i],
                        color='orange', alpha=0.7, label='Removed' if i == 0 else "")

        ax3.set_xlabel('Class', fontsize=12)
        ax3.set_ylabel('Number of Samples', fontsize=12)
        ax3.set_title('Sample Count Changes (Waterfall View)', fontsize=14, weight='bold')
        ax3.set_xticks(x_pos)
        ax3.set_xticklabels([f'Class {c}' for c in classes])
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # æ·»åŠ å˜åŒ–æ•°å€¼
        for i, change in enumerate(changes):
            ax3.text(i + 0.2, max(original_counts[i], balanced_counts[i]) + max(balanced_counts) * 0.02,
                     f'{change:+d}', ha='center', va='bottom', fontsize=10, weight='bold',
                     color='green' if change > 0 else 'red')

        # 3. ä¸å¹³è¡¡æŒ‡æ ‡å¯¹æ¯” (ä¸­æ’å·¦ä¾§)
        ax4 = fig.add_subplot(gs[1, 0])

        # è®¡ç®—å„ç§ä¸å¹³è¡¡æŒ‡æ ‡
        from collections import Counter
        import numpy as np

        def gini_index(distribution):
            """è®¡ç®—åŸºå°¼ç³»æ•°"""
            values = list(distribution.values())
            total = sum(values)
            if total == 0:
                return 0
            proportions = [v / total for v in values]
            return 1 - sum(p ** 2 for p in proportions)

        def shannon_entropy(distribution):
            """è®¡ç®—é¦™å†œç†µ"""
            values = list(distribution.values())
            total = sum(values)
            if total == 0:
                return 0
            proportions = [v / total for v in values if v > 0]
            return -sum(p * np.log2(p) for p in proportions)

        def imbalance_ratio(distribution):
            """è®¡ç®—ä¸å¹³è¡¡æ¯”ç‡"""
            values = list(distribution.values())
            if len(values) < 2:
                return 1
            return min(values) / max(values)

        # è®¡ç®—æŒ‡æ ‡
        metrics = ['Gini Index', 'Shannon Entropy', 'Imbalance Ratio']
        original_metrics = [
            gini_index(original_dist),
            shannon_entropy(original_dist),
            imbalance_ratio(original_dist)
        ]
        balanced_metrics = [
            gini_index(balanced_dist),
            shannon_entropy(balanced_dist),
            imbalance_ratio(balanced_dist)
        ]

        x_metrics = np.arange(len(metrics))
        width = 0.35

        bars1 = ax4.bar(x_metrics - width / 2, original_metrics, width,
                        label='Original', color='lightcoral', alpha=0.8)
        bars2 = ax4.bar(x_metrics + width / 2, balanced_metrics, width,
                        label='Balanced', color='lightblue', alpha=0.8)

        ax4.set_xlabel('Metrics', fontsize=12)
        ax4.set_ylabel('Value', fontsize=12)
        ax4.set_title('Imbalance Metrics Comparison', fontsize=14, weight='bold')
        ax4.set_xticks(x_metrics)
        ax4.set_xticklabels(metrics, rotation=45, ha='right')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width() / 2., height + max(original_metrics + balanced_metrics) * 0.01,
                         f'{height:.3f}', ha='center', va='bottom', fontsize=9)

        # 4. ç±»åˆ«æ ·æœ¬æ•°åˆ†å¸ƒç›´æ–¹å›¾ (ä¸­æ’ä¸­é—´)
        ax5 = fig.add_subplot(gs[1, 1])

        all_counts = list(original_dist.values()) + list(balanced_dist.values())
        bins = np.linspace(0, max(all_counts), 20)

        ax5.hist(list(original_dist.values()), bins=bins, alpha=0.7,
                 label='Original', color='lightcoral', edgecolor='black')
        ax5.hist(list(balanced_dist.values()), bins=bins, alpha=0.7,
                 label='Balanced', color='lightblue', edgecolor='black')

        ax5.set_xlabel('Sample Count', fontsize=12)
        ax5.set_ylabel('Frequency', fontsize=12)
        ax5.set_title('Sample Count Distribution', fontsize=14, weight='bold')
        ax5.legend()
        ax5.grid(True, alpha=0.3)

        # 5. ç­–ç•¥æ•ˆæœé›·è¾¾å›¾ (ä¸­æ’å³ä¾§)
        ax6 = fig.add_subplot(gs[1, 2:], projection='polar')

        # é›·è¾¾å›¾æ•°æ®
        categories = ['Balance\nImprovement', 'Data\nEfficiency', 'Diversity\nPreservation', 'Computational\nCost']

        # æ ¹æ®ç­–ç•¥è®¡ç®—å¾—åˆ† (0-1)
        strategy = self.balancing_info['strategy']
        if 'smote' in strategy.lower():
            scores = [0.9, 0.8, 0.9, 0.7]  # SMOTEè¯„åˆ†
        elif 'adasyn' in strategy.lower():
            scores = [0.9, 0.8, 0.95, 0.6]  # ADASYNè¯„åˆ†
        elif 'random' in strategy.lower():
            scores = [0.8, 0.9, 0.6, 0.9]  # Randomè¯„åˆ†
        else:
            scores = [0.7, 0.7, 0.7, 0.8]  # é»˜è®¤è¯„åˆ†

        # è§’åº¦
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        scores += scores[:1]  # é—­åˆ
        angles += angles[:1]

        ax6.plot(angles, scores, color='blue', linewidth=2, label=strategy.upper())
        ax6.fill(angles, scores, color='blue', alpha=0.25)
        ax6.set_xticks(angles[:-1])
        ax6.set_xticklabels(categories)
        ax6.set_ylim(0, 1)
        ax6.set_title('Strategy Performance Radar', fontsize=14, weight='bold', pad=20)
        ax6.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))

        # 6. è¯¦ç»†ä¿¡æ¯è¡¨æ ¼ (ä¸‹æ’)
        ax7 = fig.add_subplot(gs[2, :])
        ax7.axis('off')

        # åˆ›å»ºä¿¡æ¯è¡¨æ ¼
        info_data = [
            ['Metric', 'Original', 'Balanced', 'Change'],
            ['Total Samples', f"{self.balancing_info['original_samples']:,}",
             f"{self.balancing_info['balanced_samples']:,}",
             f"{self.balancing_info['balanced_samples'] - self.balancing_info['original_samples']:+,}"],
            ['Minority Class', f"{min(original_dist.values()):,}",
             f"{min(balanced_dist.values()):,}",
             f"{min(balanced_dist.values()) - min(original_dist.values()):+,}"],
            ['Majority Class', f"{max(original_dist.values()):,}",
             f"{max(balanced_dist.values()):,}",
             f"{max(balanced_dist.values()) - max(original_dist.values()):+,}"],
            ['Imbalance Ratio', f"{imbalance_ratio(original_dist):.3f}",
             f"{imbalance_ratio(balanced_dist):.3f}",
             f"{imbalance_ratio(balanced_dist) - imbalance_ratio(original_dist):+.3f}"],
            ['Gini Index', f"{gini_index(original_dist):.3f}",
             f"{gini_index(balanced_dist):.3f}",
             f"{gini_index(balanced_dist) - gini_index(original_dist):+.3f}"]
        ]

        # ç»˜åˆ¶è¡¨æ ¼
        table = ax7.table(cellText=info_data[1:], colLabels=info_data[0],
                          cellLoc='center', loc='center',
                          colColours=['lightgray'] * 4)
        table.auto_set_font_size(False)
        table.set_fontsize(11)
        table.scale(1.2, 2)

        # è®¾ç½®è¡¨æ ¼æ ·å¼
        for i in range(1, len(info_data)):
            for j in range(len(info_data[0])):
                cell = table[(i, j)]
                if j == 3:  # Changeåˆ—
                    value = info_data[i][j]
                    if '+' in value:
                        cell.set_facecolor('lightgreen')
                    elif '-' in value:
                        cell.set_facecolor('lightcoral')

        plt.suptitle(f'Comprehensive Data Balancing Analysis - {self.balancing_info["strategy"].upper()}',
                     fontsize=18, weight='bold', y=0.98)

        plt.savefig(self.output_dir / "improved_data_balancing_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()

        print("Improved data balancing analysis plot saved successfully!")

    def generate_rfe_optimization_summary(self):
        """ç”ŸæˆRFEä¼˜åŒ–ç»“æœæ‘˜è¦"""
        print("Generating RFE optimization summary...")

        if not hasattr(self, 'accuracy_result_table'):
            print("No RFE optimization results available")
            return

        df = self.accuracy_result_table.copy()

        summary_file = self.output_dir / "rfe_optimization_summary.txt"

        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("=== RFEç‰¹å¾é€‰æ‹©ä¼˜åŒ–æ‘˜è¦æŠ¥å‘Š ===\n\n")

            # åŸºæœ¬ç»Ÿè®¡
            f.write("1. ä¼˜åŒ–æœç´¢ç»Ÿè®¡\n")
            f.write(f"   æ€»è®¡ç®—ç»„åˆæ•°: {len(df)}\n")
            f.write(
                f"   æœç´¢èŒƒå›´: Edgeç‰¹å¾1-{df['Edge Features'].max()}, è¶…å£°ç‰¹å¾1-{df['Ultrasound Features'].max()}\n")
            f.write(f"   æœ€é«˜å‡†ç¡®æ€§: {df['Accuracy'].max():.4f}\n")
            f.write(f"   æœ€ä½å‡†ç¡®æ€§: {df['Accuracy'].min():.4f}\n")
            f.write(f"   å¹³å‡å‡†ç¡®æ€§: {df['Accuracy'].mean():.4f} Â± {df['Accuracy'].std():.4f}\n\n")

            # æœ€ä½³ç»„åˆ
            best_row = df.loc[df['Accuracy'].idxmax()]
            f.write("2. æœ€ä½³ç‰¹å¾ç»„åˆ\n")
            f.write(f"   è¾¹ç¼˜ç‰¹å¾æ•°: {best_row['Edge Features']}\n")
            f.write(f"   è¶…å£°ç‰¹å¾æ•°: {best_row['Ultrasound Features']}\n")
            f.write(f"   æ€»ç‰¹å¾æ•°: {best_row['Edge Features'] + best_row['Ultrasound Features']}\n")
            f.write(f"   å‡†ç¡®æ€§: {best_row['Accuracy']:.4f}\n")
            f.write(f"   æ ‡å‡†å·®: {best_row['Accuracy_Std']:.4f}\n\n")

            # å‰5åç»„åˆ
            f.write("3. å‰5åç‰¹å¾ç»„åˆ\n")
            top5 = df.nlargest(5, 'Accuracy')
            for i, row in enumerate(top5.itertuples(), 1):
                f.write(f"   #{i}: Edge={row._2}, US={row._3}, Acc={row._4:.4f}\n")

            # æ•ˆç‡åˆ†æ
            df['Efficiency'] = df['Accuracy'] / (df['Edge Features'] + df['Ultrasound Features'])
            best_eff_row = df.loc[df['Efficiency'].idxmax()]
            f.write(f"\n4. æœ€é«˜æ•ˆç‡ç»„åˆ\n")
            f.write(f"   è¾¹ç¼˜ç‰¹å¾æ•°: {best_eff_row['Edge Features']}\n")
            f.write(f"   è¶…å£°ç‰¹å¾æ•°: {best_eff_row['Ultrasound Features']}\n")
            f.write(f"   æ•ˆç‡å€¼: {best_eff_row['Efficiency']:.4f}\n")
            f.write(f"   å‡†ç¡®æ€§: {best_eff_row['Accuracy']:.4f}\n")

        print(f"RFE optimization summary saved to: {summary_file}")

    def generate_classification_report(self):
        """ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š"""
        print("Generating comprehensive classification report...")

        report_file = self.output_dir / "classification_analysis_report.txt"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=== Ultrasoundå½±åƒç»„å­¦æ•°æ®æœºå™¨å­¦ä¹ åˆ†ç±»åˆ†ææŠ¥å‘Š ===\n\n")

            # æ•°æ®æ¦‚å†µ
            f.write("1. æ•°æ®é›†æ¦‚å†µ\n")
            f.write(f"   æ€»æ ·æœ¬æ•°: {len(self.y)}\n")
            f.write(f"   ç‰¹å¾é€‰æ‹©å‰: {len(self.feature_names)} ä¸ªç‰¹å¾\n")
            f.write(f"   ç‰¹å¾é€‰æ‹©å: {len(self.selected_features)} ä¸ªç‰¹å¾\n")
            f.write(f"   ç±»åˆ«åˆ†å¸ƒ: {dict(zip(np.unique(self.y), np.bincount(self.y)))}\n\n")

            # æ¨¡å‹æ€§èƒ½æ±‡æ€»
            f.write("2. æ¨¡å‹æ€§èƒ½æ±‡æ€» (æŒ‰æµ‹è¯•å‡†ç¡®ç‡æ’åº)\n")
            model_performance = sorted(
                [(name, result['test_metrics']['Accuracy'], result['test_metrics']['F1-Score'])
                 for name, result in self.results.items()],
                key=lambda x: x[1], reverse=True
            )

            for i, (name, acc, f1) in enumerate(model_performance[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                f.write(f"   {i + 1}. {name}: å‡†ç¡®ç‡={acc:.4f}, F1åˆ†æ•°={f1:.4f}\n")

            # æœ€ä½³æ¨¡å‹è¯¦ç»†ä¿¡æ¯
            best_model = model_performance[0][0]
            f.write(f"\n3. æœ€ä½³æ¨¡å‹: {best_model}\n")
            best_result = self.results[best_model]

            for metric, value in best_result['test_metrics'].items():
                if value is not None:
                    f.write(f"   {metric}: {value:.4f}\n")

            # è¾“å‡ºæ–‡ä»¶è¯´æ˜
            f.write("\n4. è¾“å‡ºæ–‡ä»¶è¯´æ˜\n")
            output_files = [
                "feature_selection_results.xlsx: ç‰¹å¾é€‰æ‹©è¯¦ç»†ç»“æœ",
                "model_performance_metrics.xlsx: æ‰€æœ‰æ¨¡å‹æ€§èƒ½æŒ‡æ ‡",
                "roc_curves_*.png: ROCæ›²çº¿å¯è§†åŒ–",
                "decision_curves_*.png: å†³ç­–æ›²çº¿åˆ†æ",
                "voting_analysis_*.png: é›†æˆæŠ•ç¥¨åˆ†æ",
                "violin_*.png: å°æç´å›¾åˆ†æ"
            ]
            for file_desc in output_files:
                f.write(f"   - {file_desc}\n")

        print(f"åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    def update_imaging_thresholds(self, new_thresholds):
        """æ›´æ–°å½±åƒå­¦æŒ‡æ ‡çš„è¯Šæ–­é˜ˆå€¼

        Parameters:
        -----------
        new_thresholds : dict
            æ–°çš„é˜ˆå€¼è®¾ç½®ï¼Œæ ¼å¼å¦‚ï¼š
            {
                'C-TIRADS': 1,
                'ACR-TIRADS': 2,
                'Kwak TI-RADS': 1,
                'ATA': 3
            }
        """
        for indicator, threshold in new_thresholds.items():
            if indicator in self.imaging_thresholds:
                self.imaging_thresholds[indicator]['benign_max'] = threshold
                print(f"Updated {indicator} threshold to {threshold}")
            else:
                print(f"Warning: {indicator} not found in imaging thresholds")

        # é‡æ–°åˆ†æ
        print("Re-analyzing with new thresholds...")
        return self.analyze_imaging_indicators_performance()

    def run_analysis(self):
        """è¿è¡Œå®Œæ•´çš„åˆ†ç±»åˆ†ææµç¨‹"""
        print("=== Starting Breast Cancer MRI Radiomics Classification Analysis ===\n")

        try:
            # 1. æ•°æ®é¢„å¤„ç†ï¼ˆåŒ…å«æ•°æ®å¹³è¡¡ï¼‰
            X, y = self.preprocess_data()

            # 1.5 ç»˜åˆ¶æ•°æ®å¹³è¡¡åˆ†æå›¾
            self.plot_improved_data_balancing_analysis()
            # 2. ç‰¹å¾é€‰æ‹©
            self.dual_rfe_feature_selection(n_edge_features=4, n_ultrasound_features=11)

            # # 2. RFEç‰¹å¾é€‰æ‹©ä¼˜åŒ–ï¼ˆä»¥å‡†ç¡®æ€§ä¸ºæ ‡å‡†ï¼‰
            # print("\n=== Starting RFE Feature Selection Optimization ===")
            # best_model, best_features, best_accuracy = self.search_best_rfe_combination_by_accuracy(
            #     max_edge_features=20, max_us_features=11
            # )
            #
            # # 2.5 ç»˜åˆ¶RFEä¼˜åŒ–è¯¦ç»†åˆ†æ
            # self.plot_rfe_accuracy_detailed_analysis()
            # print(f"RFE Optimization completed. Best accuracy: {best_accuracy:.4f}")
            # print(f"Best feature combination: {len(best_features)} features")

            # 3. æ¨¡å‹è¯„ä¼°
            self.evaluate_models()
            if not hasattr(self, 'cv_results') or not self.cv_results:
                raise ValueError("Model evaluation failed - cv_result not available")

            # æ–°å¢ï¼šå½±åƒå­¦æŒ‡æ ‡è¯Šæ–­æ•ˆèƒ½åˆ†æ
            print("\n=== Starting Traditional Imaging Indicators Analysis ===")
            imaging_results = self.analyze_imaging_indicators_performance()

            # # æ–°å¢ï¼šè¾¹ç¼˜ç‰¹å¾ä¸“ç”¨æ¨¡å‹åˆ†æ
            # print("\n=== Starting Edge Features Only Analysis ===")
            # edge_only_results = self.evaluate_edge_features_only_performance()

            # æ–°å¢ï¼šç»˜åˆ¶è¯Šæ–­æ•ˆèƒ½å¯¹æ¯”å›¾
            print("\n=== Plotting Diagnostic Performance Comparison ===")
            self.plot_diagnostic_performance_comparison()
            self.plot_confusion_matrices_comparison()

            # æ–°å¢ï¼šä¿å­˜è¯Šæ–­å¯¹æ¯”ç»“æœ
            comparison_data = self.save_diagnostic_comparison_results()

            # 4. ç»˜åˆ¶ROCæ›²çº¿
            self.plot_roc_curves()

            # 5. ä¿å­˜æ€§èƒ½æŒ‡æ ‡
            best_model_name = self.save_performance_metrics()

            # 6. æ€§èƒ½æ¯”è¾ƒå›¾
            self.plot_performance_comparison()

            # 6.5 æ–°å¢ï¼šè¾¹ç¼˜ç‰¹å¾æ•ˆæœå¯¹æ¯”å›¾
            self.plot_edge_features_effect_comparison()

            # 7. å†³ç­–æ›²çº¿åˆ†æ
            self.plot_decision_curves()
            dca_metrics = self.save_decision_curve_metrics()

            # 8. æ ¡å‡†æ›²çº¿åˆ†æ
            self.plot_calibration_curves()


            # 9. å°æç´å›¾åˆ†æ
            self.plot_violin_analysis()
            violin_summary = self.save_violin_analysis_summary()

            # 10. é›†æˆæŠ•ç¥¨åˆ†ç±»åˆ†æ
            print("\n=== Starting Ensemble Voting Analysis ===")
            n_trained_models = self.create_ensemble_voting_classifier()
            print(f"Successfully trained {n_trained_models} models for ensemble voting")

            voting_results = self.perform_voting_classification()
            print("Voting classification completed")

            # ç»˜åˆ¶æŠ•ç¥¨åˆ†æå›¾è¡¨
            self.plot_voting_analysis()

            # ä¿å­˜æŠ•ç¥¨ç»“æœæ‘˜è¦
            voting_summary = self.save_voting_results_summary()

            # 11. SHAPåˆ†æ
            self.shap_analysis(best_model_name)

            # 12. ç”ŸæˆæŠ¥å‘Š
            self.generate_classification_report()

            print(f"\n=== Analysis completed! All results saved to {self.output_dir} ===")

            return {
                'best_model': best_model_name,
                'best_accuracy': self.results[best_model_name]['test_metrics']['Accuracy'],
                'selected_features': self.selected_features,
                'n_models_evaluated': len(self.results),
                'dca_metrics': dca_metrics,
                'violin_summary': violin_summary,
                'voting_summary': voting_summary,
                'imaging_results': imaging_results,  # æ–°å¢
                # 'edge_only_results': edge_only_results,  # æ–°å¢
                'diagnostic_comparison': comparison_data  # æ–°å¢
            }

        except Exception as e:
            print(f"Error during analysis: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆè¯·ä¿®æ”¹ä¸ºæ‚¨çš„å®é™…æ–‡ä»¶è·¯å¾„ï¼‰
    data_path = r"C:\Users\Hasee\Desktop\nodule_data\test_new.xlsx"  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„Excelæ–‡ä»¶è·¯å¾„

    # å¯é€‰æ‹©çš„å¹³è¡¡ç­–ç•¥
    balance_strategies = ['smote', 'adasyn', 'borderline', 'smote_tomek', 'smote_enn']

    # æ¨èç­–ç•¥ï¼ˆæ‚¨å¯ä»¥ä¿®æ”¹ï¼‰
    recommended_strategy = 'smote'  # å¯¹äºæ‚¨çš„æ•°æ®æ¨èä½¿ç”¨SMOTE

    try:
        # åˆ›å»ºåˆ†ç±»å™¨å¹¶è¿è¡Œåˆ†æ
        classifier = BreastCancerClassifier(data_path, balance_strategy=recommended_strategy)
        results = classifier.run_analysis()

        # æ‰“å°åˆ†æç»“æœæ‘˜è¦
        print("\n=== Analysis Results Summary ===")
        print(f"Data balancing strategy: {classifier.balance_strategy}")
        if hasattr(classifier, 'balancing_info'):
            print(f"Original samples: {classifier.balancing_info['original_samples']}")
            print(f"Balanced samples: {classifier.balancing_info['balanced_samples']}")
            print(f"Original distribution: {dict(classifier.balancing_info['original_distribution'])}")
            print(f"Balanced distribution: {dict(classifier.balancing_info['balanced_distribution'])}")

        print(f"Best performing model: {results['best_model']}")
        print(f"Best test accuracy: {results['best_accuracy']:.4f}")
        print(f"Number of selected features: {len(results['selected_features'])}")
        print(f"Total models evaluated: {results['n_models_evaluated']}")

        print("\nTop 5 selected features:")
        for i, feature in enumerate(results['selected_features'][:5]):
            print(f"  {i + 1}. {feature}")

        # å†³ç­–æ›²çº¿åˆ†ææ‘˜è¦
        if 'dca_metrics' in results and not results['dca_metrics'].empty:
            print("\nDecision Curve Analysis Summary:")
            top_dca_model = results['dca_metrics'].iloc[0]
            print(f"  Best DCA model: {top_dca_model['Model']}")
            print(f"  DCA AUC: {top_dca_model['DCA_AUC']:.4f}")
            print(f"  Max Net Benefit: {top_dca_model['Max_Net_Benefit']:.4f}")
            print(f"  Optimal Threshold: {top_dca_model['Optimal_Threshold']:.3f}")

    except FileNotFoundError:
        print(f"Error: Data file '{data_path}' not found.")
        print("Please ensure the file path is correct and the file exists.")
    except Exception as e:
        print(f"Error during analysis: {e}")



if __name__ == "__main__":
    main()